[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP: From Simple to Spectacular",
    "section": "",
    "text": "Preface\nI didn’t originally set out to write a book. This was supposed to be a series of blog posts that explored machine learning models and natural language processing. The idea was simple, to explain machine learning by incrementally changing a model using the same data set. I wanted to do away with math equations and proofs and focus on intuitive understanding of machine learning with programming examples from scratch. Inspiration came from the fastai course which takes a similar approach. My motivation was to better understand the algorithms I use in practice while developing the learning materials I wish I had. Practical examples with real data that explains algorithms with code step by step. I hope you find this useful.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is NLP?\nNatural language processing (or NLP) is a subfield of artificial intelligence focused on applying learning algorithms to natural language. There’s many ways to use such algorithms, for understanding language, text generation, and speech recognition to name a few. In a nutshell, an NLP model accepts a string of text, infers some understanding of it, and outputs it’s understanding as numbers. The process of creating such a model is what this book focuses on.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-this-book-is",
    "href": "intro.html#what-this-book-is",
    "title": "Introduction",
    "section": "What this book is",
    "text": "What this book is\nAs somebody who’s a programmer and not a mathematician, I always struggle with blog posts about machine learning. They throw some math equations at you and show you how to “implement” the algorithm in two lines of code using a framework, but rarely do I see implementations from scratch. This book aims to bridge that gap. The content will be programming first with little to no math or theory. Any “mathy” stuff will be picked apart with coding examples and visualizations. Algorithms will be implemented in pure python as much as possible. Frameworks will be introduced incrementally where it makes sense.\nThe gimmick behind this book is to walk through implementations of different models using one dataset. Each chapter incrementally changing our models while introducing new concepts. Where possible, each chapter will start with the quickest way to get the model up and running with scikit-learn, pytorch, or some other framework, followed by implementations from scratch. With that in mind, I recommend starting at the beginning and finishing at the end, but feel free to jump around if that makes more sense for you.\nThis book is written entirely in jupyter notebooks so you can run the code. Each chapter lives in it’s own directory. A conda environment file is provided at the root of the project with everything you need to run each chapter in this book.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#asking-for-help",
    "href": "intro.html#asking-for-help",
    "title": "Introduction",
    "section": "Asking for help",
    "text": "Asking for help\nI assume you have some grasp of Python and how to use external packages. This book may be difficult otherwise. Code blocks will include comments where necessary to help walk you through what’s going on. I will try to comment on what different functions do, but if I miss something or you want a deeper explanation then reach for your search engine. If I think something needs a little explanation but isn’t essential to the post I’ll note it in the margin. Check out the right margin to see what I mean.Hi I’m a note in the right margin, ok you can go back to the main text now.\nHowever, if you have questions about the concepts I go over or find my explanations confusing, don’t be afraid to drop me an email at smitchell556[at]gmail.com (smitchell556 at gmail dot com). If you’re confused by what I write, somebody else probably is to and I can update the post to be more clear.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#fundamental-ai-packages-well-learn-about",
    "href": "intro.html#fundamental-ai-packages-well-learn-about",
    "title": "Introduction",
    "section": "Fundamental AI packages we’ll learn about",
    "text": "Fundamental AI packages we’ll learn about\nSeparately, there’s some machine learning packages we will use throughout the book.\n\nscikit-learn\npytorch\ntransformers\n\nThese libraries are fundamental for machine learning and deep learning. You don’t need to know anything about them because we will cover them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#things-you-dont-need-but-should-know",
    "href": "intro.html#things-you-dont-need-but-should-know",
    "title": "Introduction",
    "section": "Things you don’t need but should know",
    "text": "Things you don’t need but should know\nWhile you don’t need to know anything about conda or jupyter to read this book, I highly recommend using them if you’re serious about machine learning.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter/01_data/data.html",
    "href": "chapter/01_data/data.html",
    "title": "1  Machine learning needs data",
    "section": "",
    "text": "1.1 The dataset\nWithout data, machine learning is nothing. After all what will it learn if it has nothing to learn from. Think about everything you’ve learned throughout your life. All the books you’ve read, the movies you’ve watched, the experiences you’ve lived. Everything you’ve touched, tasted, heard, felt, smelled. It all comes together to shape who you are and what you know. Our brains adapt and change to all of this input. Our brains learn. Take away all of the memories and experiences and what are we left with? Thoughts maybe, but of what? Without the context of our life there isn’t much to think about. Machine learning models work like our brain, but simpler. They take data and try to make sense of it, or learn from it. The more data, the easier it is to learn from, at least in theory.\nOur models will be using datasets of movie reviews from IMDB (Maas et al. 2011). The original datasets can be found at https://ai.stanford.edu/~amaas/data/sentiment.\nI’ve taken the liberty of further cleaning the data and making it accessible through a Python API so we can get right to work on machine learning and NLP.\nThis book provides a conda environment file (see the introduction) with everything you need to run the code. If you want to access the dataset without setting up the conda environment, you can get access to it through pip. If you’re just following along through the book website then there’s nothing you need to do.\nLet’s get a feel for the datasets before we move on to machine learning. There are two movie review datasets available. One for classification and the other for unsupervised learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine learning needs data</span>"
    ]
  },
  {
    "objectID": "chapter/01_data/data.html#the-dataset",
    "href": "chapter/01_data/data.html#the-dataset",
    "title": "1  Machine learning needs data",
    "section": "",
    "text": "Note\n\n\n\nOne of the most important chapters in this book is about cleaning data. It’s a bonus chapter at the end. It walks through the steps and analysis I performed to clean and prepare the dataset for this book. At this point it’s not important. The focus should be on building models as fast as possible and iterating on them. By the end of this book, you should feel comfortable with machine learning, at which point understanding where data comes from and how it’s prepared becomes useful. After all we want to apply te things learned here to the real world, and that starts with real world data.\nIt is still a bonus chapter and not required for understanding machine learning, but I found some suprising things when I cleaned this dataset. If you make it to the end, you really should read it.\n\n\n\n\n$ pip install git+https://github.com/spenceforce/NLP-Simple-to-Spectacular",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine learning needs data</span>"
    ]
  },
  {
    "objectID": "chapter/01_data/data.html#classification-dataset",
    "href": "chapter/01_data/data.html#classification-dataset",
    "title": "1  Machine learning needs data",
    "section": "1.2 Classification dataset",
    "text": "1.2 Classification dataset\nThis dataset contains movie reviews and labels indicating if the review is positive (label 1) or negative (label 0). It is intended for benchmarking sentiment classification tasks. Sentiment classification is about predicting the feeling a text conveys. Like emotions such as happy, sad, or angry. In this case it’s predicting whether a review says if a movie is good or bad.\nThis dataset is split into a set for training and a set for testing. We can access both the train and test sets with get_train_test_data, which returns a DataFrame object for each set. The DataFrame class is a staple of pandas. Dataframes are tables and they are not unique to pandas, but pandas is the de facto Python library for working with dataframes. You can think of dataframes as the programmatic version of an excel spreadsheet.\n\nfrom nlpbook import get_train_test_data\n\ntrain_df, test_df = get_train_test_data()\n\ntrain_df and test_df have the same format, so we’ll just inspect train_df. We can see how many rows are in the dataframe and information about the columns with DataFrame.info().\n\ntrain_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 24904 entries, 0 to 12499\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        24904 non-null  int64 \n 1   movie_id  24904 non-null  object\n 2   rating    24904 non-null  int64 \n 3   review    24904 non-null  object\n 4   label     24904 non-null  int64 \ndtypes: int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDataFrame.info() says there are 24,904 rows. There are five columns, three of which have type int64 and two with type object; the object types are strings in our dataframe.pandas assigns the type object to non-numeric values.\nEach row is for one review. A brief rundown of what the columns are:\n\nid: The review ID. For each label, this is a unique identifier for the review.\nmovie_id: The movie ID. Uniqe identifier for the movie the review is about.\nrating: A score from 1-10 that the reviewer gave the movie.\nreview: This is the review. Pretty self-explanatory.\nlabel: A 0 or 1 value indicating if the review is negative or positive, respectively.\n\nThe columns we’re interested in are review, label, and to a lesser extend, rating. review will be the input to all models as this is the natural language we are trying to process. label and rating are what we’re trying to predict! We will mainly be predicting label, but we will also use rating to show how to scale binary classification (predicting two labels) to multiple labels (predicting three or more labels).\nLet’s inspect a few reviews with DataFrame.head().\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nreview\nlabel\n\n\n\n\n0\n7275\ntt0082799\n1\n\"National Lampoon Goes to the Movies\" (1981) i...\n0\n\n\n1\n1438\ntt0397501\n4\nWell! What can one say? Firstly, this adaptati...\n0\n\n\n2\n9137\ntt0364986\n1\nWhat can I say, this is a piece of brilliant f...\n0\n\n\n3\n173\ntt0283974\n3\nA decent sequel, but does not pack the punch o...\n0\n\n\n4\n8290\ntt0314630\n2\nAlan Rudolph is a so-so director, without that...\n0\n\n\n\n\n\n\n\nThe review column looks like natural language and the label and rating columns have numeric values just like DataFrame.info() said.\nWe can also see how this dataset is split by label.\n\ntrain_df.value_counts(\"label\")\n\nlabel\n1    12472\n0    12432\nName: count, dtype: int64\n\n\nThere are 12,472 positive labels and 12,432 negative labels. That’s almost a 50/50 split.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine learning needs data</span>"
    ]
  },
  {
    "objectID": "chapter/01_data/data.html#unsupervised-dataset",
    "href": "chapter/01_data/data.html#unsupervised-dataset",
    "title": "1  Machine learning needs data",
    "section": "1.3 Unsupervised dataset",
    "text": "1.3 Unsupervised dataset\nThe train/test sets above have labels, 0 or 1, which allows them to be used in a supervised learning fashion. In supervised learning we have real outputs, the review labels in this case, to compare to our machine learning model outputs. We can supervise the models learning by comparing it’s outputs to the labels and let the model know how it’s doing.\nUnsupervised learning is just input data. There’s no label to use as a comparator. Instead the model must learn from the data without knowing whether it is right or wrong. This kind of learning is less about predicting a specific property and more about learning general properties of the data.\nThis dataset is available through get_unsup_data. Let’s inspect it with DataFrame.info().\n\nfrom nlpbook import get_unsup_data\n\nunsup_df = get_unsup_data()\nunsup_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 49507 entries, 0 to 49999\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        49507 non-null  int64 \n 1   movie_id  49507 non-null  object\n 2   review    49507 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 1.5+ MB\n\n\nAs you can see, there is no label or rating columns. It’s just reviews and nothing else.\nThe next chapter will focus on building our first model. It will be simple and not very good.\n\n\n\n\nMaas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. “Learning Word Vectors for Sentiment Analysis.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142–50. Portland, Oregon, USA: Association for Computational Linguistics. http://www.aclweb.org/anthology/P11-1015.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine learning needs data</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html",
    "href": "chapter/02_baseline/baseline_classifier.html",
    "title": "2  Baseline: gotta start somewhere",
    "section": "",
    "text": "2.1 What is machine learning?\nWe’ll start simple. So simple you may wonder if it’s AI at all. When building models from scratch, I like to start with the easiest thing possible and iterate. Before we write any code, let’s define what machine learning and machine learning models are.\nMachine learning is a field of artificial intelligence focused on algorithms that 1) learn from data and 2) generalize to unseen data. There are three main components to machine learning, data, models, and algorithms. Machine learning models are the things that do the learning and the algorithms direct the models learning. The lines blur at times between models and algorithms and some algorithms only work for some models. At their core, models are functions. They take an input, process them in some way, and return an output. Models have learnable parameters and machine learning algorithms focus on adjusting these parameters to make the model produce better outputs.\nConceptually, models are simple. Take the equation for a line, f(x) = m*x + b. This is a model where m and b are constant values indicating the slope and y-intercept of the line. x is an input value and the output is f(x) = y. If we know m and b we can compute y for any x. If we don’t know m and b, this is where machine learning comes in. Assuming we have a bunch of (x, y) points, a machine learning algorithm can guess what good values for m and b are based on those points. Then we freeze those values and the model can predict y for any x.\nModels can be giant equations and it’s easy to get lost in the details, but remember it’s still just an equation! The learning algorithm will find the right constant terms for us. It’s our job to set up the problem for the algorithm and model and then get out of the way so the machine can learn.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#what-is-machine-learning",
    "href": "chapter/02_baseline/baseline_classifier.html#what-is-machine-learning",
    "title": "2  Baseline: gotta start somewhere",
    "section": "",
    "text": "Note\n\n\n\nIn machine learning, the constant terms of the equation are called parameters or weights. During training they are not constant as the learning algorithm is trying to find their optimal values, but once training is done these values become constant and you’re left with a normal equation.\n\n\n\n2.1.1 The learning process\nAt it’s core the learning process iterates through prediction, comparison, and tweaking the model parameters. The training data is used in this process. We initialize the model, then use it as is to predict the outputs on the training data. These outputs are compared to the actual outputs, or targets, of the training data. If we are happy with the comparison training is done. Otherwise the model parameters are adjusted and the process begins again. This whole process is outlined below.\n\n\n\n\n\nflowchart TB\n  B[Predict output for train data] --&gt; C[Compare predictions to targets]\n  C --&gt; D{Predictions good enough?}\n  D -- No --&gt; E[Update model weights]\n  D -- Yes --&gt; F[Freeze model for production]\n  E --&gt; B",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#the-simplest-of-models",
    "href": "chapter/02_baseline/baseline_classifier.html#the-simplest-of-models",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.2 The simplest of models",
    "text": "2.2 The simplest of models\nTurning our attention back to our model, the simplest thing we can do is predict the same thing for every input. And if we’re going to predict the same thing for every input, it should be the most common thing.\nThis may seem silly, but it gives us a measuring stick to compare to other models. If we use the latest and greatest techniques in deep learning, it should outperform this model. The only way we’ll know it outperforms it is by building the model and testing it. If it doesn’t outperform this model, that raises cause for concern and the results should be investigated. So we start simple and incrementally improve until we are satisfied.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#easy-button",
    "href": "chapter/02_baseline/baseline_classifier.html#easy-button",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.3 Easy button",
    "text": "2.3 Easy button\nThroughout this book the first model in each chapter will be an existing implementation, where possible, that we can get up and running immediately. Then we’ll implement the model ourselves to get a deeper understanding of what’s going on.\n\n2.3.1 scikit-learn\nscikit-learn is a widely used machine learning library. It comes fully loaded with all kinds of models and tools that let machine learning practitioners build models in just a few lines of code. And it turns out scikit-learn provides the simple model we want already as sklearn.dummy.DummyClassifier. This model is also referred to as a baseline model. Let’s train it.\n\nfrom sklearn.dummy import DummyClassifier\n\nfrom nlpbook import get_train_test_data\n\ntrain_df, test_df = get_train_test_data()\nX, y = train_df[[\"review\"]], train_df[\"label\"]\ncls = DummyClassifier().fit(X, y)\ncls.score(X, y)\n\n0.500803083841953\n\n\nIn four lines of code, we load the dataset, train the model, and evaluate it. Don’t worry if it doesn’t make sense, we’ll go through it line by line.\n\n2.3.1.1 Load the data\nThe first two lines load and prepare the data for training.\n\ntrain_df, test_df = get_train_test_data()\nX, y = train_df[[\"review\"]], train_df[\"label\"]\n\nThis book’s companion Python package, nlpbook, comes with a helper function get_train_test_data which cleans up the data and returns it. Then the inputs and outputs for training are separated into their own variables, X and y, respectively. What do these inputs and outputs look like? From the last chapter we know the review column contains the review, this is our input. And the label column is a binary value, 0 or 1, indicating if the review is negative or positive. There is something interesting going on with X though, why do we use double square brackets when accessing the review column? The double square bracket notation tells pandas to return a DataFrame instead of a Series, which begs another question, why a DataFrame instead of a Series?\nA DataFrame is basically a matrix, while a Series is an array. In machine learning, inputs can have more than one feature and many algorithms and libraries expect inputs to be in the form of a matrix where the rows of the matrix are data points and the columns are features of each data point.DataFrame.shape shows the dimensions of the matrix as (rows, columns).\n\nX.shape\n\n(24904, 1)\n\n\nIn our case, we have 24,904 data points and each data point has one feature. That feature is the review. In practice, you’ll use a matrix as input, even if you only have one feature.\nMoving on to the output, why isn’t y a matrix? Well it could be depending on your use case, but most of the time you will be predicting one value, so standard practice is to use an array. Each value in y corresponds to the output of each row in X. So, the first value in y is the output for the first row in X, the second value in y is the output for the second row in X, and so on.\n\n\n2.3.1.2 Train the model\nThe line initializes the model and train it.\n\ncls = DummyClassifier().fit(X, y)\n\nWith scikit-learn you’ll see a few methods over and over again, fit is one of those methods. fit just means train the model. We are “fitting” the model to the data. scikit-learn models returns self from fit, so we can initialize the model and train it in one line.\n\n\n2.3.1.3 Evaluate the model\nFinally the model is evaluated with score.\n\ncls.score(X, y)\n\n0.500803083841953\n\n\nWhat is this score though? It’s a metric that evaluates the performance of the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#metrics-metrics-metrics",
    "href": "chapter/02_baseline/baseline_classifier.html#metrics-metrics-metrics",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.4 Metrics, metrics, metrics",
    "text": "2.4 Metrics, metrics, metrics\nMetrics give a sense of how well your model is doing. They allow you to measure and compare models. There is no one size fits all metric, and you’ll usually want to look at multiple metrics when evaluating models. I recommend starting with the end when picking metrics. Why are you building this model in the first place? What is it you want the model to do? Then define your metrics with that objective in mind.\n\n2.4.1 Accuracy\nWe are trying to predict the sentiment of movie reviews. We don’t care about positive vs negative, we just care that we get it right. It’s a simple question, “How many predictions are correct?” This is accuracy. It’s measured as a ratio, the number of correct predictions over all predictions.\n\nimport numpy as np\n\n\ndef accuracy(y, y_pred):\n    y, y_pred = np.array(y), np.array(y_pred)\n    return np.sum(y == y_pred) / len(y)\n\n\naccuracy([0, 1, 1], [1, 1, 1])\n\nnp.float64(0.6666666666666666)\n\n\nIn the simple example above there are two lists where the difference between them is at the first element. Two out of three are the same and that’s exactly what accuracy tells us.\nIn practice, we don’t have to write our own accuracy function because scikit-learn offers one.\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy([0, 1, 1], [1, 1, 1]) == accuracy_score([0, 1, 1], [1, 1, 1])\n\nnp.True_\n\n\nBack to our classifier! When we ran cls.score(X, y), the model returned the accuracy. We can check this by computing the accuracy ourselves.\n\npredictions = cls.predict(train_df)\naccuracy_score(y, predictions), cls.score(X, y)\n\n(0.500803083841953, 0.500803083841953)\n\n\nOur accuracy is 0.5 out of 1.0, or 50% , which means we got half of the predictions correct. This makes sense since our dataset is half positive and half negative labels, so if we predict 0 or 1 for everything we should get half of them correct.The accuracy score is on a 0-1 scale where 1 means 100% accuracy\n\n2.4.1.1 Imbalanced datasets and accuracy\nMetrics are not foolproof. What is “good” for one dataset may not be for another. This is why we start with the simplest possible model. Because it allows us to establish minimum benchmarking values to compare against. Let’s run through an example to show how accuracy changes with the ratio of positive to negative labels in the dataset using this simple model.I love seaborn for rapid data visualization. It can’t do everything perfectly, but it does the common things beautifully.\n\n\n\n\n\n\n\n\n\nThe accuracy of our model is 50% when we have a 50/50 ratio of positive to negative labels. As the ratio moves to either extreme however (all ones or all zeros), the accuracy goes up until it reaches 100% when all the labels in the dataset are the same.\nIntuitively a higher accuracy is better, but it might not be high because your model works well. It could be that your dataset is imbalanced and your model has figured that out and predicts everything to be the majority class. In that scenario, you end up with a fancy model you put all this effort into just to get the same result as the simple model we just made. How do you combat this? By doing what we’ve done here and start with a simple model. Then as you develop more complex models, they can be compared to this simple model to see if they make improvements!\n\n\n\n2.4.2 Other metrics\nAccuracy is not the only thing we can measure, but for simplicities sake it will be the only thing we measure in this book. Other common metrics we could look at are F1-score and Matthews Correlation Coefficient (MCC), but there’s plenty more.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#train-vs.-test-datasets",
    "href": "chapter/02_baseline/baseline_classifier.html#train-vs.-test-datasets",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.5 Train vs. test datasets",
    "text": "2.5 Train vs. test datasets\nWe’ve trained a model and evaluated it’s accuracy so now we can move on to making a better model. WRONG, WE HAVEN’T EVALUATED THE MODEL!!! We checked the accuracy on the training data which isn’t a true evaluation of the model. We want to know how well our model works on data it hasn’t seen yet, which is why we have a test set. The test set is used exclusively to benchmark the performance of the model and is not used in the training process. This comes back to making models that generalize to unseen data. Models generally perform better on data they’re trained on because that data is what the model is optimized to predict for. If we use that same data to test the model, then we’ll likely be overconfident in our models predictions. When we deploy the model and it is used on new data it hasn’t seen we’ll be in for a rude awakening. So we separate our dataset into training and testing datasets. In practice care should go into how these datasets are curated, but we won’t worry about that here since I’ve already cleaned our datasets.I highly recommend this blog post on the dangers of blindly splitting your data. It is about validation sets, but the same concepts apply to test sets.\nSo let’s evaluate our model for real.\n\nX_test, y_test = test_df[[\"review\"]], test_df[\"label\"]\ncls.score(X_test, y_test)\n\n0.5011190233977619\n\n\nThe accuracy is still 0.5 which makes sense. The test dataset is curated the same way as the train dataset and has a roughly 50/50 split of positive to negative labels. While the accuracy didn’t change between the train and test datasets, we have a more unbiased evaluation of our model and this is the accuracy we will aim to beat next.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#preprocessing",
    "href": "chapter/02_baseline/baseline_classifier.html#preprocessing",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.6 Preprocessing",
    "text": "2.6 Preprocessing\nWhen we prepared our training data, we ran this code:\n\nX, y = train_df[[\"review\"]], train_df[\"label\"]\n\nAll we did was pull out the relevant columns from the dataframe, which is as simple as it gets. Preprocessing can get very complicated and we’ll explore how preprocessing impacts model performance in future chapters. One important aspect of preprocessing is the test dataset needs to be preprocessed the same way as the train dataset, because the model expects the data it makes predictions about to have the same form as the data it was trained on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#rolling-our-own",
    "href": "chapter/02_baseline/baseline_classifier.html#rolling-our-own",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.7 Rolling our own",
    "text": "2.7 Rolling our own\nNow that we’ve trained a model, let’s make it ourselves. It’s as simple as you can get, majority rules.Model implementations will include gratuitous comments explaining the code. This is a more streamlined approach that allows for covering the abstract concepts in the chapter text while giving details on implementation alongside the implementation itself.\n\nclass BaselineClassifier:\n    def fit(self, X, y):\n        \"\"\"Train the model with inputs `X` on labels `y`.\"\"\"\n        # Get the unique labels and their counts.\n        labels, counts = np.unique(y, return_counts=True)\n        # Keep the most common label for prediction.\n        self.prediction = labels[np.argmax(counts)]\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Return the most common label as the prediction for every\n        # input.\n        return np.full(len(X), self.prediction)\n\n\nbc = BaselineClassifier()\nbc.fit(X, y)\naccuracy_score(y_test, bc.predict(X_test))\n\n0.5011190233977619\n\n\nBeautiful, we got the same score. But wait, there’s more!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#sec-scikit-learnifying-our-model",
    "href": "chapter/02_baseline/baseline_classifier.html#sec-scikit-learnifying-our-model",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.8 scikit-learnifying our model",
    "text": "2.8 scikit-learnifying our model\nscikit-learn provides a framework to integrate your own machine learning models to leverage all the bells and whistles provided by their API. One such benefit is benchmarking. scikit-learn models come with a score method, and the default score method for classifiers is accuracy. By wrapping our model in scikit-learns framework, we get accuracy scores (and a bunch of other stuff) for free.\n\n2.8.1 scikit-learn best practices\nThere’s a pretty lengthy document on developing scikit-learn models, but most of the advice on that page can be boiled down to a few points.\n\n__init__ is for setting attributes, not computation.\nEvery parameter for __init__ should have a corresponding attribute.\nComputation during training that needs to persist across method calls should be assigned to attributes with an underscore suffix.\nThe fit method is for training and the predict method is for prediction.\nfit should return self.\nBaseEstimator should be to the right of scikit-learn mixins.\nMust accept N data points at once.\n\nA note on that last point, accepting multiple data points at once comes with two benefits.\n\nMultiple data points can be predicted at once.\nOperating on multiple data points at once allows for leveraging highly optimized libraries, like numpy, for improved speed.\n\nWith a few tweaks, our BaselineClassifier becomes a scikit-learn model.\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n\nclass BaselineClassifier(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        \"\"\"Train the model with inputs `X` on labels `y`.\"\"\"\n        # Get the unique labels and their counts.\n        labels, counts = np.unique(y, return_counts=True)\n        # Keep the most common label for prediction.\n        # Note we changed the `prediction` attribute to include a\n        # trailing suffix because it results from a computation\n        # that persists across method calls.\n        self.prediction_ = labels[np.argmax(counts)]\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Return the most common label as the prediction for every\n        # input.\n        return np.full(len(X), self.prediction_)\n\n\ncls = BaselineClassifier().fit(X, y)\ncls.score(X_test, y_test)\n\n0.5011190233977619\n\n\nAnd with that we get accuracy for free. Thanks scikit-learn.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/02_baseline/baseline_classifier.html#multiclass-classification",
    "href": "chapter/02_baseline/baseline_classifier.html#multiclass-classification",
    "title": "2  Baseline: gotta start somewhere",
    "section": "2.9 Multiclass classification",
    "text": "2.9 Multiclass classification\nUp to this point we’ve dealt with binary classification, the label is either 0 or 1, but classification can extend to 3 or more labels. Our dataset includes ratings which we can use as multiclass labels. Fortunately, we’ve written BaselineClassifier to work on an arbitrary number of labels, so we should get multiclass classification for free. Let’s see what happens when we use the rating column instead of the label column.\n\ncls = BaselineClassifier().fit(X, train_df[\"rating\"])\ncls.score(X_test, test_df[\"rating\"])\n\n0.1997151576805697\n\n\nAt first glance this may seem low, but if you take a second to think about it, it actually makes sense. As the number of unique labels goes up, the frequency of the majority label is more likely to go down. Let’s look at the breakdown of ratings frequency.\n\ntrain_df.groupby(\"rating\").size() / len(train_df)\n\nrating\n1     0.203501\n2     0.091230\n3     0.096852\n4     0.107613\n7     0.100104\n8     0.120704\n9     0.090548\n10    0.189447\ndtype: float64\n\n\nHere we see the most frequent label shows up 20% of the time, which is inline with our accuracy on the test set. Good thing we have a baseline model to orient us as we think about accuracy on binary and multiclass classification problems!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Baseline: gotta start somewhere</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html",
    "href": "chapter/03_oner/oner.html",
    "title": "3  OneR",
    "section": "",
    "text": "3.1 The OneR algorithm\nMoving beyond the simplest model, the next step is a deep neural network. Just kidding, the next step is something slightly more complex but still very simple. OneR (Holte 1993) is such a model.\nConceptually, it works on the sampe principle as the baseline model, but on feature categories instead of on the whole dataset. For each category in the feature, predict the most frequent label. To implement this, we iterate over each unique category and train a baseline classifier for just that category. When predicting on new data, we look up the baseline classifier for that category and return it’s prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#the-oner-algorithm",
    "href": "chapter/03_oner/oner.html#the-oner-algorithm",
    "title": "3  OneR",
    "section": "",
    "text": "3.1.1 Categorical features\nA categorical feature is an unordered set of values. Movie genres are a good example, horror, comedy, action, etc. There is no order to a movie genre, but each value represents a type of movie. We can treat our movie reviews the same way. We’ll represent each unique review as a category.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#rolling-our-own",
    "href": "chapter/03_oner/oner.html#rolling-our-own",
    "title": "3  OneR",
    "section": "3.2 Rolling our own",
    "text": "3.2 Rolling our own\nThere is no easy button for this model, so we’ll go straight to the implementation.\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.utils.validation import validate_data\n\n\nclass OneR(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        \"\"\"Find the most predictive rule.\"\"\"\n        # Sanity check on `X` and `y`.\n        X, y = validate_data(self, X, y)\n        predictors = {}\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points and labels in this category.\n            _X = X[is_category]\n            _y = y[is_category]\n            # Train a baseline classifier on the category.\n            predictors[category] = DummyClassifier().fit(_X, _y)\n        self.predictors_ = predictors\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Sanity check on `X`.\n        # `reset` should be `True` in `fit` and `False` everywhere else.\n        X = validate_data(self, X, reset=False)\n        # Create an empty array that will hold the predictions.\n        rv = np.zeros(X.shape[0])\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points in this category.\n            _X = X[is_category]\n            # Predict the label for all datapoints in `_X`.\n            predictions = self.predictors_[category].predict(_X)\n            # Assign the prediction for this category to\n            # the corresponding indices in `rv`.\n            rv[is_category] = predictions\n        return rv\n\nThere’s a lot going on here. We’ve added a scikit-learn validation function, validate_data. Since we’re building a sklearn classifier we might as well take advantage of the validation functions the framework provides. While it’s not necessary to use this function, I highly recommend it for a few reasons.\n\nIt checks our inputs and outputs have the right shape, a matrix and array, respectively.\nWe should always be operating on these shapes and raising an error otherwise. The input, X, passed to predict also needs to be the same shape as the input passed to fit. If it doesn’t have the same number of columns this will raise and error.\nIt converts the type to a numpy or scipy array.\nThis gives consistency. Arguments to fit and predict could by numpy arrays, scipy arrays, pandas dataframes, pandas series, lists, or any number of container types. By converting them to numpy/scipy arrays we are guaranteed a predictible API for X and y.\n\nLet’s try it out!\n\nfrom nlpbook import get_train_test_data\n\ntrain_df, test_df = get_train_test_data()\nfeatures = [\"review\"]\nlabel = \"label\"\n\nX, y = train_df[features], train_df[label]\nX\n\noner = OneR().fit(X, y)\noner.score(X, y)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/tmp/ipykernel_9161/3341900254.py in ?()\n      6 \n      7 X, y = train_df[features], train_df[label]\n      8 X\n      9 \n---&gt; 10 oner = OneR().fit(X, y)\n     11 oner.score(X, y)\n\n/tmp/ipykernel_9161/258435010.py in ?(self, X, y)\n      8     def fit(self, X, y):\n      9         \"\"\"Find the most predictive rule.\"\"\"\n     10         # Sanity check on `X` and `y`.\n---&gt; 11         X, y = validate_data(self, X, y)\n     12         predictors = {}\n     13         # Get the unique categories from the first column.\n     14         categories = np.unique(X[:, 0])\n\n~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/utils/validation.py in ?(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n   2957             if \"estimator\" not in check_y_params:\n   2958                 check_y_params = {**default_check_params, **check_y_params}\n   2959             y = check_array(y, input_name=\"y\", **check_y_params)\n   2960         else:\n-&gt; 2961             X, y = check_X_y(X, y, **check_params)\n   2962         out = X, y\n   2963 \n   2964     if not no_val_X and check_params.get(\"ensure_2d\", True):\n\n~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/utils/validation.py in ?(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1366         )\n   1367 \n   1368     ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n   1369 \n-&gt; 1370     X = check_array(\n   1371         X,\n   1372         accept_sparse=accept_sparse,\n   1373         accept_large_sparse=accept_large_sparse,\n\n~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/utils/validation.py in ?(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n   1052                         )\n   1053                     array = xp.astype(array, dtype, copy=False)\n   1054                 else:\n   1055                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n-&gt; 1056             except ComplexWarning as complex_warning:\n   1057                 raise ValueError(\n   1058                     \"Complex data not supported\\n{}\\n\".format(array)\n   1059                 ) from complex_warning\n\n~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/utils/_array_api.py in ?(array, dtype, order, copy, xp, device)\n    828         # Use NumPy API to support order\n    829         if copy is True:\n    830             array = numpy.array(array, order=order, dtype=dtype)\n    831         else:\n--&gt; 832             array = numpy.asarray(array, order=order, dtype=dtype)\n    833 \n    834         # At this point array is a NumPy ndarray. We convert it to an array\n    835         # container that is consistent with the input's namespace.\n\n~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/pandas/core/generic.py in ?(self, dtype, copy)\n   2149     def __array__(\n   2150         self, dtype: npt.DTypeLike | None = None, copy: bool_t | None = None\n   2151     ) -&gt; np.ndarray:\n   2152         values = self._values\n-&gt; 2153         arr = np.asarray(values, dtype=dtype)\n   2154         if (\n   2155             astype_is_view(values.dtype, arr.dtype)\n   2156             and using_copy_on_write()\n\nValueError: could not convert string to float: '\"National Lampoon Goes to the Movies\" (1981) is absolutely the worst movie ever made, surpassing even the witless \"Plan 9 from Outer Space.\" The Lampoon film unreels in three separate and unconnected vignettes, each featuring different performers. The only common thread is the total lack of any redeeming qualities.&lt;br /&gt;&lt;br /&gt;Well, maybe there is one. Another reviewer on this site has said that the fleeting nude shots are nice, and he\\'s right. Misses Ganzel and Dusenberry flash their assets prettily, in part one and part two, respectively. But their glamorous displays are, alas, wasted. The directors seem to have forgotten that even T&A needs a credible story to surround it, and there\\'s none in sight.&lt;br /&gt;&lt;br /&gt;The third segment, starring Robby Benson and Richard Widmark, is the most disgusting of the three, and an unfortunate choice as the windup of this film. Benson plays an eager-beaver young policeman, brightly reporting for his first day of duty, ready to rid the streets of evil. He is paired with an old, cynical cop played by Widmark, and when these oil-and-water partners set out on their first patrol together, we sense a possible redemption of the film\\'s earlier failures. Maybe, just maybe, the cynical old-timer will be reformed by his new partner\\'s stalwart sense of duty and loyalty. Maybe all will end happily after all. But alas, this movie heads straight for the toilet, with no redemption, no happy ending, no coherent story of any kind.&lt;br /&gt;&lt;br /&gt;Before \"National Lampoon Goes to the Movies,\" I thought I had already seen the worst schlock that Hollywood could possibly turn out. Unfortunately, I hadn\\'t seen the half of it.&lt;br /&gt;&lt;br /&gt;'\n\n\n\n*Surprised pikachu face* An error!? validate_data didn’t like our inputs. Turns out validate_data also trys to convert X and y to numeric values and fails hard. But why?\nWell this is because machine learning works on numbers, not text. We’ve written our algorithm to work on numbers or text, but in general the str type isn’t usable by most machine learning algorithms and validate_data enforces that up front. Fortunately it’s not hard to convert text categories to numbers, we just assign a unique number to each category. sklearn even has a class to do just that.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#transformers",
    "href": "chapter/03_oner/oner.html#transformers",
    "title": "3  OneR",
    "section": "3.3 Transformers",
    "text": "3.3 Transformers\nWhat we’ve worked with so far in scikit-learn are estimators. These are machine learning models. scikit-learn has another common type of class called transformers, which transform data as the name implies. These classes are commonly used for preprocessing tasks. They do not have a predict method, but instead have a transform method. We “train” them with fit just like we would a model, but this is for consistent preprocessing and not prediction.\n\n3.3.1 Transforming text to numbers\nOrdinalEncoder is the sklearn class for transforming categories to numbers.\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder()\nencoder.fit(X)\nencoder.transform(X)\n\narray([[  300.],\n       [22675.],\n       [22956.],\n       ...,\n       [16257.],\n       [ 4455.],\n       [ 2065.]])\n\n\nTurns out transforming data is so common, there’s a short hand method for fitting and transforming in one go, fit_transform.\n\nencoder = OrdinalEncoder()\nencoder.fit_transform(X)\n\narray([[  300.],\n       [22675.],\n       [22956.],\n       ...,\n       [16257.],\n       [ 4455.],\n       [ 2065.]])\n\n\nLet’s give our model another try.\n\nX_ordinal = encoder.fit_transform(X)\noner = OneR().fit(X_ordinal, y)\noner.score(X_ordinal, y)\n\n1.0\n\n\nThat’s 100% accurate! Well it’s 100% accurate on our training data, which we don’t really care about. How does it perform on our test data?\n\n\n3.3.2 Do unto the test data as you have done unto the train data\nBefore we can score the model on the test data, we need to apply the same transforms. Again, sklearn has some niceties that make it easy to enforce the same process on all data.\n\n3.3.2.1 Pipelines\nPipelines offer convenient ways to string preprocessing and models together into one object that can be trained end to end and then make predictions on different data following the same process. The input to pipelines is a sequence of transformers with an optional predictor as the last element. Once the pipeline is set up it has the same methods for training and predicting as a regular sklearn model.\n\nfrom sklearn.pipeline import Pipeline\n\n# Layout the sequence of operations. The first element transforms\n# the categories to numbers. The second element is our model. Each\n# element is a tuple where the first element of the tuple is the\n# name of the step and the second element is the transformer or model.\nsteps = [\n    (\"categorical_transform\", OrdinalEncoder()),\n    (\"model\", OneR()),\n]\npipeline = Pipeline(steps)\n# Now train the model with the original data as input.\n# No need to fit the encoder ourselves!\npipeline.fit(X, y)\n\nNow let’s try predicting on one review. Our pipeline will handle the preprocessing and prediction for us.\n\nX_test, y_test = test_df[features], test_df[label]\npipeline.predict(X_test.head(1))\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[7], line 2\n      1 X_test, y_test = test_df[features], test_df[label]\n----&gt; 2 pipeline.predict(X_test.head(1))\n\nFile ~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/pipeline.py:785, in Pipeline.predict(self, X, **params)\n    783 if not _routing_enabled():\n    784     for _, name, transform in self._iter(with_final=False):\n--&gt; 785         Xt = transform.transform(Xt)\n    786     return self.steps[-1][1].predict(Xt, **params)\n    788 # metadata routing enabled\n\nFile ~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319, in _wrap_method_output.&lt;locals&gt;.wrapped(self, X, *args, **kwargs)\n    317 @wraps(f)\n    318 def wrapped(self, X, *args, **kwargs):\n--&gt; 319     data_to_wrap = f(self, X, *args, **kwargs)\n    320     if isinstance(data_to_wrap, tuple):\n    321         # only wrap the first output for cross decomposition\n    322         return_tuple = (\n    323             _wrap_data_with_container(method, data_to_wrap[0], X, self),\n    324             *data_to_wrap[1:],\n    325         )\n\nFile ~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:1597, in OrdinalEncoder.transform(self, X)\n   1583 \"\"\"\n   1584 Transform X to ordinal codes.\n   1585 \n   (...)\n   1594     Transformed input.\n   1595 \"\"\"\n   1596 check_is_fitted(self, \"categories_\")\n-&gt; 1597 X_int, X_mask = self._transform(\n   1598     X,\n   1599     handle_unknown=self.handle_unknown,\n   1600     ensure_all_finite=\"allow-nan\",\n   1601     ignore_category_indices=self._missing_indices,\n   1602 )\n   1603 X_trans = X_int.astype(self.dtype, copy=False)\n   1605 for cat_idx, missing_idx in self._missing_indices.items():\n\nFile ~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:218, in _BaseEncoder._transform(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\n    213 if handle_unknown == \"error\":\n    214     msg = (\n    215         \"Found unknown categories {0} in column {1}\"\n    216         \" during transform\".format(diff, i)\n    217     )\n--&gt; 218     raise ValueError(msg)\n    219 else:\n    220     if warn_on_unknown:\n\nValueError: Found unknown categories [\"1st watched 2/9/2008, 4 out of 10(Dir-J.S. Cardone): Sexual political thriller that doesn't really succeed in any of these areas very well except early on where there are some interesting soft-core scenes. The movie starts off portraying a couple exploring their sexual fantasies amidst their work environments or wherever and whatever suits their fancy. The couple takes an excursion to a retreat and bathhouse where they run into a woman that's willing to be a part of a three-some and fulfill some of their fantasies. At this point, we only know that this couple is well off but we don't know until they return that the fiancé is part of a well-to-do political family. The man hopes to be on the rise to the point of possibly getting a congressional seat after the marriage. They then receive a package in the mail from an anonymous source with explicit pictures of their encounter at the bath house and their qwest begins as to how and why they were filmed, who sent the package, what they want, and how to clear their names before any of this gets out. This qwest becomes an obsession that leads them deeper into seedier worlds and takes a lot of their time, to the point where their friends & family wonder what they're doing all day and why they look rundown all the time. This movie is interesting at times but drifts into ridiculousness as they personally seek out the problem instead of getting the police involved early on because of their pride. This mistake, of course, keeps the movie going. The performances are fine despite the no-name cast but the lunacy of the situation overrides and the movie starts to become ho-hum about ½ the way through. And of course, they throw in a twist at the end that defies and challenges everything that happened prior(as is the norm these days when they don't know what else to do to spice up the movie). This doesn't help this movie one bit, though.\"] in column 0 during transform\n\n\n\nMore errors, this is becoming a theme for this chapter. The error says the transform method found an unknown category. It turns out there are reviews in the test set that are different from the train set.\nSo when OrdinalEncoder tries to transform unseen reviews in the test set it doesn’t know what category to assign them since it was fitted on the train set. The sklearn developers have thought of a way to handle this though by assigning all unknown categories to one unknown value.\n\nencoder = OrdinalEncoder(\n    handle_unknown=\"use_encoded_value\", unknown_value=-1\n)\nencoder.fit(X)\nencoder.transform(X_test.head(1))\n\narray([[-1.]])\n\n\nVoila, it’s fixed. But this raises another problem. Our encoder can handle unknown categories, what about our model? It can’t, so we need to provide a fallback prediction for unknown categories. The simple solution is to use the baseline classifier trained on all the train data to predict unknown categories.\n\nclass OneR(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        \"\"\"Find the most predictive rule.\"\"\"\n        # Sanity check on `X` and `y`.\n        X, y = validate_data(self, X, y)\n        predictors = {}\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points and labels in this category.\n            _X = X[is_category]\n            _y = y[is_category]\n            # Train a baseline classifier on the category.\n            predictors[category] = DummyClassifier().fit(_X, _y)\n        self.predictors_ = predictors\n        # Create a fallback predictor for unknown categories.\n        self.unknown_predictor_ = DummyClassifier().fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Sanity check on `X`.\n        # `reset` should be `True` in `fit` and `False` everywhere else.\n        X = validate_data(self, X, reset=False)\n        # Create an empty array that will hold the predictions.\n        rv = np.zeros(X.shape[0])\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points in this category.\n            _X = X[is_category]\n            # Predict the label for all datapoints in `_X`.\n            try:\n                predictions = self.predictors_[category].predict(_X)\n            except KeyError:\n                # Fallback to the predictor for unknown categories.\n                predictions = self.unknown_predictor_.predict(_X)\n            # Assign the prediction for this category to\n            # the corresponding indices in `rv`.\n            rv[is_category] = predictions\n        return rv\n\nNow we’ll recreate the pipeline with our new model.\n\nsteps = [\n    (\n        \"categorical_transform\",\n        OrdinalEncoder(\n            handle_unknown=\"use_encoded_value\", unknown_value=-1\n        ),\n    ),\n    (\"model\", OneR()),\n]\npipeline = Pipeline(steps)\npipeline.fit(X, y)\npipeline.predict(X_test.head(1))\n\narray([1.])\n\n\nOur model predicted the first review in the test set has a positive sentiment, which means it’s predicting something, so it must be working!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#evaluating-our-model",
    "href": "chapter/03_oner/oner.html#evaluating-our-model",
    "title": "3  OneR",
    "section": "3.4 Evaluating our model",
    "text": "3.4 Evaluating our model\nOur preprocessing is locked in and our model is trained. Now we can score our model on the test set.\n\npipeline.score(X_test, y_test)\n\n0.5011190233977619\n\n\nThat’s a lot worse than the 100% accuracy we got on the train data. How does this compare to the baseline model?The nlpbook package contains accessors for our model results. This way we don’t need to retrain each model we want to compare. Just pass in a list of model names.\n\nfrom nlpbook import get_results\n\nget_results([\"Baseline\"])\n\n\n\n\n\n\n\n\nAccuracy\n\n\nModel\n\n\n\n\n\nBaseline\n0.501119\n\n\n\n\n\n\n\nDamn, that’s the same accuracy as the baseline model. Why is that?\nTurns out all the reviews in the test set are different from the reviews in the train set, so this model devolves to the baseline model, giving us the same result.\n\nset(train_df[\"review\"]) & set(test_df[\"review\"])\n\nset()\n\n\nIt’s unfortunate we didn’t see an improvement in performance, but because we are comparing to a baseline, we know where we stand. In the next chapter we’ll mostly ignore the OneR algorithm and focus on improving performance by creating a richer representation of the input data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#rolling-your-own-transformer",
    "href": "chapter/03_oner/oner.html#rolling-your-own-transformer",
    "title": "3  OneR",
    "section": "3.5 Rolling your own transformer",
    "text": "3.5 Rolling your own transformer\nMuch like our model, we can write transformers that slot into the sklearn API, all we need to do is implement fit and transform methods. These follow all the same guidelines outlined in Section 2.8. Let’s make our own OrdinalEncoder called CategoricalEncoder.The transform method heavily uses broadcasting, which is one way numpy speeds up operations on arrays.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CategoricalEncoder(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        \"\"\"Generate numeric categories from `X`.\n\n        Note: All `fit` methods must accept a `y` argument whether\n              they use them or not. Transfomers typically ignore\n              this argument whether it's passed in or not.\n        \"\"\"\n        # Validate the data as before. `skip_check_array=True`\n        # tells `validate_data` not to convert `X` to a numeric array\n        # type. This is important since we have to deal with numeric\n        # or text types.\n        X = validate_data(self, X, skip_check_array=True)\n        try:\n            # Since `validate_data` did not convert `X` to a numeric\n            # array, we need to convert it to a matrix if it's still\n            # a `DataFrame`.\n            X = X.to_numpy()\n        except AttributeError:\n            # This is not a `DataFrame`. Assume it's a `numpy` or\n            # `scipy` array.\n            pass\n\n        categories = []\n        # Iterate over each column in `X`.\n        for column in X.T:\n            # Get all unique values in the column.\n            values = np.unique(column)\n            # Store the unique values as the ith element in the array.\n            categories.append(values)\n        # Save the categories on the transformer.\n        self.categories_ = categories\n        return self\n\n    def transform(self, X):\n        \"\"\"Return the categorical values.\"\"\"\n        X = validate_data(self, X, skip_check_array=True, reset=False)\n        try:\n            X = X.to_numpy()\n        except AttributeError:\n            pass\n\n        # Create an array with the same shape as `X` to store the\n        # categorical values. An unknown category, `-1` is used as\n        # the default value.\n        rv = np.full(X.shape, -1)\n        # Iterate over each column in `X`.\n        for i, x in enumerate(X.T):\n            # Grab the categories for the ith column of `X`.\n            categories = self.categories_[i]\n            # Reshape the column to be a Nx1 matrix. Using a matrix\n            # instead of an array allows us to leverage `numpy`\n            # broadcasting.\n            x = x.reshape(-1, 1)\n            # Create boolean matrix where `True` values indicate the\n            # index of `categories` that equals `x` for each row.\n            is_category = x == categories\n            # Find the indices of `x` that contain known categories.\n            # This tells us which rows have known categories.\n            known_category = is_category.any(axis=1)\n            # Get the index of the `True` value in each row. This\n            # is the numeric value for the category.\n            category_value = np.where(is_category)[1]\n            # Assign the category index to the appropriate rows.\n            rv[known_category, i] = category_value\n        return rv\n\nOur very first transformer! Let’s run it.\n\ncat_encoder = CategoricalEncoder()\ncat_encoder.fit_transform(X)\n\narray([[  300],\n       [22675],\n       [22956],\n       ...,\n       [16257],\n       [ 4455],\n       [ 2065]])\n\n\nBeautiful, we’ve successfully recreated OrdinalEncoder. And we should get -1 for the test data.\n\ncat_encoder.transform(X_test)\n\narray([[-1],\n       [-1],\n       [-1],\n       ...,\n       [-1],\n       [-1],\n       [-1]])\n\n\nLet’s use it in a pipeline!\n\nsteps = [\n    (\"categorical_transform\", CategoricalEncoder()),\n    (\"model\", OneR()),\n]\npipeline = Pipeline(steps)\npipeline.fit(X, y)\npipeline.score(X_test, y_test)\n\n0.5011190233977619\n\n\nHeck yeah it worked.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/03_oner/oner.html#multiclass-classification",
    "href": "chapter/03_oner/oner.html#multiclass-classification",
    "title": "3  OneR",
    "section": "3.6 Multiclass classification",
    "text": "3.6 Multiclass classification\nWe know every review in the test data will have an unknown category, so the OneR model will fallback to the baseline classifier for every review meaning we’ll get the same result we did in the last chapter.\n\n\n\n\nHolte, Robert C. 1993. “Very Simple Classification Rules Perform Well on Most Commonly Used Datasets.” Machine Learning 11 (1): 63–90. https://doi.org/10.1023/A:1022631118932.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OneR</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html",
    "href": "chapter/04_oner/oner.html",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "",
    "text": "4.1 Feature extraction\nLast chapter we implemented the OneR (Holte 1993) algorithm. In this chapter we’ll improve the model without improving the model. Sounds strange, but bear with me. This chapter is all about changing the input to the model.\nRight now we just pass in the review as input. It’s a string and every review is unique. The review is treated as a category where each unique review is it’s own category. Before training, each unique review is assigned a number to represent it.\nWith this setup, the only way to compare two reviews is to see if they are the same review or not. But two pieces of text contain much richer information that can be used to compare them. Letters make up words, words make up sentences, sentences make up paragraphs, and so on. Language also has syntax and semantics. As machine learning practitioners we actually don’t care about syntax and semantics. We expect our models will learn the concepts they need as they train on the target task. We do care about how the text is represented as this is what our models learn from. Our model currently learns from a single number that represents a review. Let’s change that.\nFeature extraction is a way to convert data into a format that is useable by machine learning models. Data that is numeric, like scientific measurements, are machine learning friendly by default, but text and image data is not. These types of data need to be converted to numbers in a way that preserves their information. Last chapter we converted our text data to numbers, but we didn’t preserve any of the information in the reviews and ended up with a model that performed the same as the baseline.\nWe’ll try again, but while preserving some of the information in the text. As always, let’s start simple.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#feature-extraction",
    "href": "chapter/04_oner/oner.html#feature-extraction",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "",
    "text": "4.1.1 Bag of characters\nWe have reviews. These are just sentences, which are just words, which are just characters. So reviews are characters. We can count the number of each character in a review and represent the review as character counts. This is representation is called a bag of characters and it can be extended to words, sentences, or whatever you want really. And of course sklearn has a way to do this.CountVectotrizer returns a sparse matrix which are memory efficient for matrices that contain mostly 0s.\n\nfrom nlpbook import get_train_test_data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntrain_df, test_df = get_train_test_data()\n\n# Create the bag of characters feature extraction transformer.\n# The `CountVectorizer` class bags input text. We set\n# `analyzer=\"char\"` so that `CountVectorizer` counts characters\n# instead of words and `lowercase=False` to prevent upper case\n# letters from being converted to lowercase.\nvectorizer = CountVectorizer(analyzer=\"char\", lowercase=False)\n\n# Fit the bag of characters transformer on our reviews.\n# Notice we do not pass a matrix into the fit method, but an array.\n# Feature extraction should be performed on a per column basis so\n# we need to pass in the column we want feature extraction performed\n# on.\nvectorizer.fit(train_df[\"review\"])\n\n# Transform the first row to a bag of characters.\n# Convert the sparse matrix to a numpy array to see the counts.\nvectorizer.transform(train_df[\"review\"].head(1)).toarray()\n\narray([[  0,   0,   0, 275,   0,   6,   0,   0,   0,   1,   5,   1,   1,\n          0,   0,  25,   4,  16,   8,   0,   2,   0,   0,   0,   0,   0,\n          0,   1,   2,   0,   0,   8,   0,   8,   0,   0,   2,   5,   0,\n          1,   0,   0,   3,   2,   3,   0,   0,   3,   5,   2,   1,   1,\n          0,   2,   1,   5,   1,   0,   3,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,  99,  24,  23,  50, 166,  31,  21,  64,  84,   1,\n          5,  61,  27,  89,  92,  30,   1,  94,  90, 122,  29,  13,  17,\n          0,  35,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0]])\n\n\nWe can check what character each array element is counting with the vocabulary_ dict.\n\nlist(vectorizer.vocabulary_.items())[:5]\n\n[('\"', 5), ('N', 49), ('a', 68), ('t', 87), ('i', 76)]\n\n\nSo index 5 of the above array is the counts for the character \" and index 49 is the counts for N.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#oner-revisited",
    "href": "chapter/04_oner/oner.html#oner-revisited",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "4.2 OneR revisited",
    "text": "4.2 OneR revisited\nNow we can just pass this as input to our OneR model right? Not quite, our model isn’t capable of handling this input yet. Here’s the implementation from last chapter.\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.utils.validation import validate_data\n\n\nclass OneR(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        \"\"\"Find the most predictive rule.\"\"\"\n        # Sanity check on `X` and `y`.\n        X, y = validate_data(self, X, y)\n        predictors = {}\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points and labels in this category.\n            _X = X[is_category]\n            _y = y[is_category]\n            # Train a baseline classifier on the category.\n            predictors[category] = DummyClassifier().fit(_X, _y)\n        self.predictors_ = predictors\n        # Create a fallback predictor for unknown categories.\n        self.unknown_predictor_ = DummyClassifier().fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Sanity check on `X`.\n        # `reset` should be `True` in `fit` and `False` everywhere else.\n        X = validate_data(self, X, reset=False)\n        # Create an empty array that will hold the predictions.\n        rv = np.zeros(X.shape[0])\n        # Get the unique categories from the first column.\n        categories = np.unique(X[:, 0])\n        for category in categories:\n            # Create a conditional array where each index\n            # is a boolean indicating if that index in the\n            # first column of `X` is the category we're iterating\n            # over.\n            is_category = X[:, 0] == category\n            # Grab all data points in this category.\n            _X = X[is_category]\n            # Predict the label for all datapoints in `_X`.\n            try:\n                predictions = self.predictors_[category].predict(_X)\n            except KeyError:\n                # Fallback to the predictor for unknown categories.\n                predictions = self.unknown_predictor_.predict(_X)\n            # Assign the prediction for this category to\n            # the corresponding indices in `rv`.\n            rv[is_category] = predictions\n        return rv\n\nNotice the model only looks at the first column. If we use it now it will use the counts in the first column of our bag of characters matrix for training and prediction which isn’t what we want.\nPreviously, I said the algorithm works by predicting the most frequent label for each category, but in reality it only does this for one feature, hence the name OneR which stands for One Rule. The model finds the feature whose categories are most predictive of the training data, then predicts labels based on just that feature. When we first implemented this model we only had one feature so it didn’t matter, but now we have many, so we need to tweak the implementation to find the best feature when given any number of features.\n\nimport scipy.sparse\n\n# Rename the `OneR` class to `Rule`.\nRule = OneR\n\n\n# Create a new `OneR` class which finds the best `Rule` in the\n# dataset.\nclass OneR(ClassifierMixin, BaseEstimator):\n    def fit(self, X, y):\n        \"\"\"Find the best rule in the dataset.\"\"\"\n        # Sanity check on `X` and `y`.\n        X, y = validate_data(self, X, y, accept_sparse=True)\n\n        col_idx = score = rule = None\n        # Iterate over the indices for each column in X.\n        for i in range(X.shape[1]):\n            # Create a new matrix containing just the ith column.\n            _X = X[:, [i]]\n            # Convert sparse matrix to `numpy` array.\n            # `Rule` works on numpy arrays, so we should use consistent\n            # array types.\n            if scipy.sparse.issparse(_X):\n                _X = _X.toarray()\n\n            # Create a rule for the ith column.\n            rule_i = Rule().fit(_X, y)\n            # Score the ith columns accuracy.\n            score_i = rule_i.score(_X, y)\n\n            # Keep the rule for the ith column if it has the highest\n            # accuracy so far.\n            if score is None or score_i &gt; score:\n                rule = rule_i\n                score = score_i\n                col_idx = i\n\n        self.rule_ = rule\n        self.i_ = col_idx\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the labels for inputs `X`.\"\"\"\n        # Sanity check on `X`.\n        X = validate_data(self, X, reset=False, accept_sparse=True)\n        _X = X[:, [self.i_]]\n        # Convert sparse matrix to `numpy` array.\n        # `Rule` works on numpy arrays, so we should use consistent\n        # array types.\n        if scipy.sparse.issparse(_X):\n            _X = _X.toarray()\n\n        # Return predictions for the rule.\n        return self.rule_.predict(_X)\n\nAnd now we can train the model! ColumnTransformer is a handy utility for applying transforms to specific columns of a dataframe.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Grab `X` and `y`.\nfeatures = [\"review\"]\nlabels = \"label\"\nX, y = train_df[features], train_df[labels]\n\n# Create the bag of characters transform.\nbag_of_chars = CountVectorizer(analyzer=\"char\", lowercase=False)\n\n# Wrap the bag of characters transform in a `ColumnTransformer`.\n# This class lets us perform transforms on specific columns of a\n# `DataFrame` instead of on the entire `DataFrame`. This allows\n# us to use `CountVectorizer` on just the \"review\" column.\n# Check the docs link in the margin.\ncolumn_transform = ColumnTransformer(\n    [(\"bag_of_chars\", bag_of_chars, \"review\")]\n)\n\noner = OneR()\n\n# Create our pipeline.\npipeline = Pipeline([(\"bag_chars\", column_transform), (\"oner\", oner)])\n\n# Train it!\npipeline.fit(X, y)\n\nNow after all that work, how’d we do?\n\nX_test, y_test = test_df[features], test_df[labels]\npipeline.score(X_test, y_test)\n\n0.5812817904374364\n\n\n58% accuracy, that’s a big improvement over the 50% accuracy of our baseline and original OneR models! All we did was change the input to the OneR model and we gained an 8% bump in accuracy.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#machine-learning-is-an-end-to-end-process",
    "href": "chapter/04_oner/oner.html#machine-learning-is-an-end-to-end-process",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "4.3 Machine learning is an end to end process",
    "text": "4.3 Machine learning is an end to end process\nWe barely touched the model and ended up with an 8% improvement just by improving the preprocessing step. There are multiple steps to building a model, from collecting data through model training and they are all important for the final outcome. I probably sound like a broken record at this point, but this is why we started with a baseline model. The process of building a model is really one of experimentation. You make a change here then test it. Now a change there and test it. It’s tweak after tweak, but you need to know how those tweaks affect the performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#models-as-analysis-tools",
    "href": "chapter/04_oner/oner.html#models-as-analysis-tools",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "4.4 Models as analysis tools",
    "text": "4.4 Models as analysis tools\nNow that we’ve trained a model that actually has some predictive power, we can start to use it for interesting things. Often we use models to predict outcomes, but we can also go in the other direction and use them to learn about our data. OneR found some character that is predictive for our data. What is that character?\n\n# Get the index of the rule.\nbag_idx = oner.i_\n\n# Grab the vocabulary from the fitted bag of characters.\nvocab = column_transform.named_transformers_.bag_of_chars.vocabulary_\n\n# Find the character associated with the index.\n[char for char, char_idx in vocab.items() if char_idx == bag_idx]\n\n['?']\n\n\nInteresting, question marks are the most important character for determining labels. My hypothesis is more question marks means a bad review.\n\nimport seaborn as sns\n\ndata = X_test.copy()\ndata[\"?\"] = column_transform.transform(X_test)[:, bag_idx].toarray()\ndata[\"label\"] = y_test\nsns.countplot(data, x=\"?\", hue=\"label\")\n\n\n\n\n\n\n\n\nAh, so if there are no question marks, the review is more likely to be positive. Inversely we can say if the review contains a question, it’s likely the review is negative.\nInstead of analyzing the data ourselves to find which characters are associated with positive or negative reviews, our model did the analysis for us. We let the model learn about the data, then in turn we leveraged what the model learned to gain an insight into the data. As we make better models and richer representations of the input text, the amount of information we’ll learn about the data will increase as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#rolling-our-own-feature-extractor",
    "href": "chapter/04_oner/oner.html#rolling-our-own-feature-extractor",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "4.5 Rolling our own feature extractor",
    "text": "4.5 Rolling our own feature extractor\nWe’ve already handled the OneR model, now let’s make a bag of characters transformer.\n\nfrom sklearn.base import TransformerMixin\n\n\nclass BagOfChars(TransformerMixin, BaseEstimator):\n    \"\"\"Bag of characters feature extractor.\"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Fit on all characters in the array `X`.\n\n        Note: `X` should be a 1d array.\n        \"\"\"\n        # We run our own validation check since `validate_data`\n        # expects a 2d numberic array.\n        # We want a 1d text array so we'll check its shape here.\n        # While iterating over the array values we'll check\n        # they are text while trying to extract characters.\n        assert len(X.shape) == 1\n\n        vocabulary_ = {}\n        # Iterate over each string in the array.\n        for x in X:\n            # Check it's a string!\n            assert isinstance(x, str)\n            # Get the unique characters in the string.\n            chars = np.unique(list(x))\n            # Add each character to the vocabulary if it isn't\n            # there already.\n            for char in chars:\n                if char not in vocabulary_:\n                    vocabulary_[char] = len(vocabulary_)\n        self.vocabulary_ = vocabulary_\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform `X` to a count matrix.\n\n        Note: `X` should be a 1d array.\n        \"\"\"\n        # Run our own checks.\n        assert len(X.shape) == 1\n        # Check we fit the instance.\n        assert hasattr(self, \"vocabulary_\")\n\n        # Create a matrix to hold the counts.\n        rv = np.zeros((X.shape[0], len(self.vocabulary_)))\n        # Iterate over each string in the array.\n        for i, x in enumerate(X):\n            # Check it's a string!\n            assert isinstance(x, str)\n            # Get the unique characters in the string and their\n            # counts.\n            chars, counts = np.unique(list(x), return_counts=True)\n            # Add each character count to the count matrix\n            # for the specific row.\n            for char, count in zip(chars, counts):\n                # Make sure the character is part of the vocabulary,\n                # otherwise ignore it.\n                if char in self.vocabulary_:\n                    rv[i, self.vocabulary_[char]] = count\n        # Return the count matrix.\n        return rv\n\n\n# Let's plug it into the pipeline and see how it goes.\ncolumn_transform = ColumnTransformer(\n    [(\"bag_of_chars\", BagOfChars(), \"review\")]\n)\n\noner = OneR()\n\n# Create our pipeline.\npipeline = Pipeline([(\"bag_chars\", column_transform), (\"oner\", oner)])\n\n# Train it!\npipeline.fit(X, y)\npipeline.score(X_test, y_test)\n\n0.5812817904374364\n\n\nEasy peasy.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/04_oner/oner.html#multiclass-classification",
    "href": "chapter/04_oner/oner.html#multiclass-classification",
    "title": "4  OneR, TwoR, RedR, BlueR: Inputs matter",
    "section": "4.6 Multiclass classification",
    "text": "4.6 Multiclass classification\nLet’s see how we do with multiclass classification. Our accuracy with the baseline model was 20%. Can we beat that?\n\ny_multi = train_df[\"rating\"]\ny_test_multi = test_df[\"rating\"]\n\npipeline.fit(X, y_multi)\npipeline.score(X_test, y_test_multi)\n\n0.19377416073245168\n\n\nIt’s about the same, that’s unfortunate. This is a harder problem than predicting if a review is positive or negative. The decision of what class to choose is more complicated since there’s more options to choose from. With that added complexity, the model needs to learn more information about the data to be predictive. This means our model is either too simple or the input doesn’t contain enough information to scale to this more difficult task. Performance on the multiclass classification problem will lag behind the binary classification problem, but it will improve as we build better and better models.\n\n\n\n\nHolte, Robert C. 1993. “Very Simple Classification Rules Perform Well on Most Commonly Used Datasets.” Machine Learning 11 (1): 63–90. https://doi.org/10.1023/A:1022631118932.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OneR, TwoR, RedR, BlueR: Inputs matter</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/cleaning_data/cleaning_data.html",
    "href": "chapter/bonus/cleaning_data/cleaning_data.html",
    "title": "5  Bonus: cleaning data",
    "section": "",
    "text": "5.1 Getting the data\nThe dataset used throughout this book was originally curated by a group at Stanford (Maas et al. 2011). There are several ways to get your hands on a dataset, you can curate them yourself or rely on the work of others. This dataset can be found at https://ai.stanford.edu/~amaas/data/sentiment, but for simplicities sake it’s downloadable from this github repo.\nLet’s grab the data!\nimport tarfile\nfrom pathlib import Path\n\nimport requests\n\ntar_path = Path(\"aclImdb_v1.tar.gz\")\nif not tar_path.exists():\n    r = requests.get(\n        \"https://github.com/spenceforce/NLP-Simple-to-Spectacular/releases/download/aclImdb_dataset/aclImdb_v1.tar.gz\",\n        stream=True,\n    )\n    with tar_path.open(\"wb\") as f:\n        for chunk in r.iter_content(chunk_size=128):\n            f.write(chunk)\n\nwith tarfile.open(tar_path) as tar:\n    tar.extractall(filter=\"data\")\n    data_path = Path(\"aclImdb\")\n    # The untarred directory is `aclImdb` instead of `aclImdb_v1`.\nlist(data_path.iterdir())\n\n[PosixPath('aclImdb/.ipynb_checkpoints'),\n PosixPath('aclImdb/imdbEr.txt'),\n PosixPath('aclImdb/README'),\n PosixPath('aclImdb/train'),\n PosixPath('aclImdb/imdb.vocab'),\n PosixPath('aclImdb/test')]\nIf you’re working through this notebook in jupyter, I recommend taking a pause and reading the README yourself.\nI’ll summarize the main takeaways here for those of you reading this online.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bonus: cleaning data</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/cleaning_data/cleaning_data.html#getting-the-data",
    "href": "chapter/bonus/cleaning_data/cleaning_data.html#getting-the-data",
    "title": "5  Bonus: cleaning data",
    "section": "",
    "text": "This extracts the data from the tarball then assigns the output directory as a pathlib.Path object to data_path.I highly recommend checking out the pathlib library if you don’t already use it. It provides a nice object oriented API for handling file paths.\nThere are a lot of datasets out there. Some better than others. Before trying to predict anything, it’s a good idea to get a sense of what the dataset actually contains. Hopefully the datasets you’re using are well documented. The dataset we’re working with comes with a README which we can see in the directory contents.Don’t worry if the paths shown in this notebook are different from the ones you see on your compute resource. The structure of the data directory should be the same, and by leveraging relative paths with Path objects, everything should just work.\n\n\n\n\n5.1.1 Dataset specifics\nThere are 50k reviews, split into two groups of 25k. One group is for training machine learning models and the other is for testing them. Within each group of 25k, the reviews are split into half positive and half negative.\nThere’s at most 30 reviews for a given movie. The train and test sets have reviews for different movies, so none of the movies reviewed in the train set show up in the test set. Negative reviews have a score of 4 or less and positive reviews have a score of 7 or more.\nThere are an additional 50k reviews without any labels. These reviews are intended for unsupervised learning purposes (we’ll learn about unsupervised learning in the future). There is an equal number of reviews with a score of 4 or less and 5 or more.\n\n\n5.1.2 Directory structure\nThe classification dataset file naming convention is [DATASET]/[LABEL]/[ID]_[RATING].txt where DATASET is one of train or test, LABEL is one of pos or neg (for positive and negative respectively), ID is a numeric identifier for a review, and RATING is the score the reviewer gave the movie.A word of warning for jupyter lab users. Jupyter crashed when I tried to open the directories containing the reviews (like train/pos). My guess is there’s too many files for jupyter to display and it becomes unresponsive. Your mileage may vary.\nThe unsupervised dataset file naming convention is train/unsup/[ID]_0.txt where ID is a numeric identifier for a review.\nURLs to the reviews section are also provided in [DATASET]/urls_[LABEL].txt. The line N refers to ID N in the associated dataset/label combination. For example line 200 in train/urls_pos.txt refers to the reviews webpage for the movie of train/pos/200_10.txt. Here’s an example URL from one of these files: http://www.imdb.com/title/tt0064354/usercomments. It turns out IMDB has changed their URL format so this link is broken. They now use “reviews” in place of “usercomments” like http://www.imdb.com/title/tt0064354/reviews. An oddity in the URLs is they do not point to the individual review, but to the review section of the movie the review is about.By the time you read this, they may have changed the URL structure again",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bonus: cleaning data</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/cleaning_data/cleaning_data.html#question-the-dataset",
    "href": "chapter/bonus/cleaning_data/cleaning_data.html#question-the-dataset",
    "title": "5  Bonus: cleaning data",
    "section": "5.2 Question the dataset",
    "text": "5.2 Question the dataset\nAn integral part of building a machine learning model is understanding where the data comes from and how it was acquired. There may be assumptions made during the data curation process that you don’t agree with. An artefact in the data may have unintended side effects downstream, such as training a model that works very well on data it’s seen but performs poorly on data it hasn’t.\nAs you read about the dataset and it’s curation process, I recommend you keep asking “why?” to better understand the choices made during the curation process. Why did they use those specific thresholds for choosing positive vs negative labels? Why that ratio of positive/negative labels? Why that many reviews per movie? Why set up the train/test sets that way?\nIn parallel, also think about how you are going to leverage this data. Your end goal may be different from the curators of the dataset and that should be taken into account as you prepare the data for training machine learning models.\nHere’s a couple questions that come to my mind.\n\nWhy isn’t there a dataset for multi-label classification?\nWhy a maximum of 30 reviews per movie? Why not 10 or 50?\nWhat was the rationale for picking the movies? Was it random or spread evenly by genre?\nWere the movies made during a certain time period?\n\nSome of these questions we could answer ourselves if we want to and they can lead to a richer set of information. IMDB provides an API to programmatically gather movie data over the web. With that tool one can gather movie genres, release dates, associated actors, and more. If we never ask the questions we’ll never think to look. Maybe we curate our own dataset because this one doesn’t provide what we need or it leads to further analysis of the data and the dataset is tweaked based on that analysis. We will stay as true to the source dataset as possible, but don’t let that stop you from thinking about ways to improve this dataset or curate an entirely new one.OMDb API is another API that provides IMDB metadata. I found their service to be much more transparent on pricing and easier to get access to.\n\n5.2.1 Cleaning the data\nThe boundaries between cleaning data and analyzing data can be fuzzy at times, but for sake of simplicity we will only analyze the data as needed to properly clean it.\nThe pieces of information provided in this dataset are:\n\nID\nreview\nrating\nlabel\nmovie ID\n\nThe movie ID isn’t explicitly provided, but we can extract it from the reviews URL which have this format http://www.imdb.com/title/[MOVIE_ID]/usercomments.\nLet’s gather the training set which is under train/pos and train/neg.\n\nimport pandas as pd\n\n\ndef get_review_data(reviews_dir, urls_file):\n    \"\"\"\n    Return a `pd.DataFrame` containing the review ID,\n    movie ID, rating, and review.\n    \"\"\"\n    with urls_file.open() as f:\n        movie_ids = {\n            i: url.split(\"/\")[4]\n            for i, url in enumerate(f.readlines())\n        }\n\n    data = []\n    for p in reviews_dir.iterdir():\n        ID, rating = map(int, p.stem.split(\"_\"))\n        data.append(\n            {\n                \"id\": ID,\n                \"movie_id\": movie_ids[ID],\n                \"rating\": rating,\n                \"review\": p.open().read().strip(),\n            }\n        )\n\n    return pd.DataFrame(data)\n\n\ndef get_train_data():\n    \"\"\"\n    Return a `pd.DataFrame` with the supervised training data.\n    \"\"\"\n    dfs = []\n    for label, label_name in enumerate([\"neg\", \"pos\"]):\n        df = get_review_data(\n            data_path / \"train\" / label_name,\n            data_path / f\"train/urls_{label_name}.txt\",\n        )\n        df[\"label\"] = label\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ntrain_df = get_train_data()\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nreview\nlabel\n\n\n\n\n0\n7275\ntt0082799\n1\n\"National Lampoon Goes to the Movies\" (1981) i...\n0\n\n\n1\n1438\ntt0397501\n4\nWell! What can one say? Firstly, this adaptati...\n0\n\n\n2\n9137\ntt0364986\n1\nWhat can I say, this is a piece of brilliant f...\n0\n\n\n3\n173\ntt0283974\n3\nA decent sequel, but does not pack the punch o...\n0\n\n\n4\n8290\ntt0314630\n2\nAlan Rudolph is a so-so director, without that...\n0\n\n\n\n\n\n\n\nThose look like reviews! Now that we have data in hand to play with, what questions can we answer about it? Maybe we should verify the information provided by the curators of this dataset.\n\nAre there 25k reviews in the train set?\nAre they evenly split between positive and negative?\nIs the max number of reviews per movie 30?\n\nWe can use DataFrame.info() to answer the first question. This method gives some general information about the dataframe, including the column names, number of non-null values in each column, column data types, and the number of rows.Null values include None and NaNs. NaN, or Not a Number, representa a number that is undefined, such as the result of dividing by 0.\n\ntrain_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 25000 entries, 0 to 12499\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        25000 non-null  int64 \n 1   movie_id  25000 non-null  object\n 2   rating    25000 non-null  int64 \n 3   review    25000 non-null  object\n 4   label     25000 non-null  int64 \ndtypes: int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nThis dataframe contains 25k entries, and all of those entries have non-null reviews. So the train set does contain 25k reviews. But we know nothing about the quality of these reviews. Are there duplicates? Empty strings? Are some complete gibberish? It’s impossible to qualitatively check every data point, but checking some basic properties of the data can go a long way. I like to call these sanity checks. Let’s start with empty strings.\n\ntrain_df[\"review\"].str.len().min()\n\nnp.int64(52)\n\n\nThe shortest review has 52 characters, so there are no empty reviews. Let’s turn our attention to duplicates. There’s many ways to check this. We’ll use DataFrame.describe.\n\ntrain_df.describe(include=[object])\n\n\n\n\n\n\n\n\nmovie_id\nreview\n\n\n\n\ncount\n25000\n25000\n\n\nunique\n3456\n24904\n\n\ntop\ntt0374240\nThis show comes up with interesting locations ...\n\n\nfreq\n30\n3\n\n\n\n\n\n\n\n The movie_id column shows the most frequent movie ID shows up 30 times. So we know there’s at most 30 reviews for a given movie in this dataset.The include=[object] argument tells describe to look at columns with the object data type instead of numeric data types. Strings fall under the object type in pandas.\nThe review column has a count of 25k, but 24,904 unique entries which means there’s 96 duplicate reviews. From a training perspective, it may not make sense to have duplicate items in the train set; odds are we don’t want them, but if they have conflicting labels we may want to keep them after all. Let’s take a peek at them to see if they belong to the same movie and have the same ratings and labels. We’ll create a new dataframe that contains only duplicated reviews.\n\nis_duplicate = train_df[\"review\"].duplicated(keep=False)\nduplicate_reviews = train_df[is_duplicate]\n\nNow that we have the duplicated reviews, let’s dig into them a bit. Are they for the same movie? Do they have the same rating or label?\n\nduplicate_nunique = (\n    duplicate_reviews.groupby(\"review\").agg(\"nunique\").head()\n)\nduplicate_nunique.head(3)\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nlabel\n\n\nreview\n\n\n\n\n\n\n\n\n'Dead Letter Office' is a low-budget film about a couple of employees of the Australian postal service, struggling to rebuild their damaged lives. Unfortunately, the acting is poor and the links between the characters' past misfortunes and present mindsets are clumsily and over-schematically represented. What's most disappointing of all, however, is the portrayal is life in the office of the film's title: there's no mechanisation whatsoever, and it's quite impossible to ascertain what any of the staff really do for a living. Granted, part of the plot is that the office is threatened with closure, but this sort of office surely closed in the 1930s, if it ever truly existed. It's a shame, as the film's overall tone is poignant and wry, and there's some promise in the scenario: but few of the details convince. Overall, it feels the work of someone who hasn't actually experienced much of real life; a student film, with a concept and an outline, but sadly little else.\n2\n2\n1\n1\n\n\n.......Playing Kaddiddlehopper, Col San Fernando, etc. the man was pretty wide ranging and a scream. I love watching him interact w/ Amanda Blake, or Don Knotts or whomever--he clearly was having a ball and I think he made it easier on his guests as well--so long as they Knew ahead of time it wasn't a disciplined, 19 take kind of production. Relax and be loose was clearly the name of the game there.&lt;br /&gt;&lt;br /&gt;He reminds me of guys like Milton Berle, Benny Hill, maybe Jerry Lewis some too. Great timing, ancient gags that kept audiences in stitches for decades, sheer enjoyment about what he was doing. His sad little clown he played was good too--but in a touching manner.&lt;br /&gt;&lt;br /&gt;Personally I think he's great, having just bought a two DVD set of his shows from '61 or so, it brings his stuff back in a fond way for me. I can remember seeing him on TV at the end of his run when he was winding up the series in 1971 or so.&lt;br /&gt;&lt;br /&gt;Check this out if you are a fan or curious. He was a riot.\n2\n2\n1\n1\n\n\n&lt;br /&gt;&lt;br /&gt;Back in his youth, the old man had wanted to marry his first cousin, but his family forbid it. Many decades later, the old man has raised three children (two boys and one girl), and allows his son and daughter to marry and have children. Soon, the sister is bored with brother #1, and jumps in the bed of brother #2.&lt;br /&gt;&lt;br /&gt;One might think that the three siblings are stuck somewhere on a remote island. But no -- they are upper class Europeans going to college and busy in the social world.&lt;br /&gt;&lt;br /&gt;Never do we see a flirtatious moment between any non-related female and the two brothers. Never do we see any flirtatious moment between any non-related male and the one sister. All flirtatious moments are shared between only between the brothers and sister.&lt;br /&gt;&lt;br /&gt;The weakest part of GLADIATOR was the incest thing. The young emperor Commodus would have hundreds of slave girls and a city full of marriage-minded girls all over him, but no -- he only wanted his sister? If movie incest is your cup of tea, then SUNSHINE will (slowly) thrill you to no end.\n2\n1\n1\n1\n\n\n\n\n\n\n\ngroupby is grouping rows with the same review column together into their own dataframes. The return value of this operation is a DataFrameGroupBy. This special dataframe type performs operations on each group as if they were their own dataframe instead of all rows in the dataframe. agg performs an operation on the entire dataframe, but since this is a DataFrameGroupBy object, the agg method is applied to each group. In this case it counts the number of unique values in each column for each unique review.\nSome of these reviews have two movie IDs. Let’s inspect the two reviews above that have multiple movie IDs.\n\nis_first2 = duplicate_reviews[\"review\"].isin(\n    duplicate_nunique.index[:2]\n)\nduplicate_reviews[is_first2]\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nreview\nlabel\n\n\n\n\n10893\n985\ntt0223119\n4\n'Dead Letter Office' is a low-budget film abou...\n0\n\n\n12445\n4102\ntt0118939\n4\n'Dead Letter Office' is a low-budget film abou...\n0\n\n\n101\n6069\ntt0163806\n8\n.......Playing Kaddiddlehopper, Col San Fernan...\n1\n\n\n5458\n9319\ntt0043224\n8\n.......Playing Kaddiddlehopper, Col San Fernan...\n1\n\n\n\n\n\n\n\nRemember we are provided a URL for the comments section of each review which is where we extract the movie ID from. This means we can go backwards from movie ID to movie URL. One review is for movies at the URLs http://www.imdb.com/title/tt0223119/reviews and http://www.imdb.com/title/tt0118939/reviews and the other at http://www.imdb.com/title/tt0163806/reviews and http://www.imdb.com/title/tt0043224/reviews. When I click on each pair one gets redirected to the other. Movie IDs tt0223119 and tt0118939 are for Dead Letter Office, and tt0163806 and tt0043224 are for The Red Skelton Hour. Although they have different movie IDs, they are reviews for the same movie and therefore are truly duplicate reviews. It turns out each movie can have multiple movie IDs; we call this a one-to-many relationship.I checked these links in August 2024. The URL endpoints may have changed in the future.\nWhile it’s not incredibly important, The Red Skelton Hour is actually a TV show. It turns out this dataset contains reviews for movies and tv shows. See what a little digging can turn up?\nWe could check every single movie ID for duplicate reviews manually (or have an intern do it) because it’s possible there are duplicate reviews for different movies, but our task is to predict a label (positive or negative) given a review. Our models don’t need to see the same review over and over again in order to make predictions about them…unless duplicate reviews have different labels! Why would we want to train on both examples in this case? If the same review can be positive or negative, then training a machine learning model with both examples will teach the model to be uncertain about some reviews where the language is more ambiguous.\nLet’s see if there’s any duplicate reviews with multiple labels or ratings.\n\ndifferent_labels = (duplicate_nunique[\"label\"] &gt; 1) | (\n    duplicate_nunique[\"rating\"] &gt; 1\n)\nduplicate_nunique[different_labels].head()\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nlabel\n\n\nreview\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere isn’t, which means these reviews stem from duplicate movie IDs. Since the labels are the same for each duplicate, I feel confident in just removing the duplicate entries. I’ve taken the liberty of performing this analysis on the test set and found the same issue, so we’ll remove duplicates from the test set as well.\n\ndef get_review_data(reviews_dir, urls_file, dedup=False):\n    \"\"\"\n    Return a `pd.DataFrame` containing the review ID,\n    movie ID, rating, and review.\n    \"\"\"\n    with urls_file.open() as f:\n        movie_ids = {\n            i: url.split(\"/\")[4]\n            for i, url in enumerate(f.readlines())\n        }\n\n    data = []\n    for p in reviews_dir.iterdir():\n        ID, rating = map(int, p.stem.split(\"_\"))\n        data.append(\n            {\n                \"id\": ID,\n                \"movie_id\": movie_ids[ID],\n                \"rating\": rating,\n                \"review\": p.open().read().strip(),\n            }\n        )\n\n    rv = pd.DataFrame(data)\n    if dedup:\n        return rv.drop_duplicates(\"review\").copy()\n    return rv\n\n\ndef get_df(dataset, dedup=False):\n    \"\"\"Return a `pd.DataFrame` for a dataset.\"\"\"\n    dfs = []\n    for label, label_name in enumerate([\"neg\", \"pos\"]):\n        df = get_review_data(\n            data_path / dataset / label_name,\n            data_path / dataset / f\"urls_{label_name}.txt\",\n            dedup,\n        )\n        df[\"label\"] = label\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ndef get_train_data(dedup=True):\n    \"\"\"\n    Return a `pd.DataFrame` with the supervised training data.\n    \"\"\"\n    return get_df(\"train\", dedup)\n\n\ndef get_test_data(dedup=True):\n    \"\"\"\n    Return a `pd.DataFrame` with the supervised testing data.\n    \"\"\"\n    return get_df(\"test\", dedup)\n\n\ntrain_df = get_train_data()\ntrain_df.groupby(\"label\").describe(include=\"object\")[\n    (\"review\", \"count\")\n]\n\nlabel\n0    12432\n1    12472\nName: (review, count), dtype: object\n\n\n\ntrain_df.shape\n\n(24904, 5)\n\n\nWe don’t quite have 25k reviews split evenly across labels, but it’s close enough. I think this is in good shape and we can turn our attention to the test set.\n\ntest_df = get_test_data()\ntest_df.groupby(\"label\").describe(include=\"object\")[\n    (\"review\", \"count\")\n]\n\nlabel\n0    12361\n1    12440\nName: (review, count), dtype: object\n\n\n\ntest_df.shape\n\n(24801, 5)\n\n\nWe’ve removed about 200 reviews from the test set and can start digging in to comparing the test set to the train set.\n\n\n5.2.2 Train-test contamination\nNow let’s talk a bit about what the test set is used for. It’s a separate set of data used to evaluate a machine learning model after it’s trained. It is data used to measure the performance of a model on data it’s never seen before. This is important because when a model is used in production, it will be making predictions about all kinds of inputs it wasn’t trained on and it needs to generalize well beyond the training data. We do not want data leaking from the train set into the test set.\nEasy enough to check.\n\nleak_df = test_df[test_df[\"review\"].isin(train_df[\"review\"])]\nleak_df.shape\n\n(123, 5)\n\n\nThere’s 123 reviews from the train set in the test set.\nLet’s think about that for a second. The reviews in the test set should be for movies that aren’t reviewed in the train set, but we have 123 reviews in the test set that are duplicates of those in the train set. How could this happen?\nRemember that we saw duplicate reviews in the train set. This was a result of the same movie having multiple movie IDs. That could explain what’s happening here. Maybe the reviews in both the train and test sets have different movie IDs. We can test that!\n\nboth_df = train_df.merge(test_df, on=\"review\")\n(both_df[\"movie_id_x\"] == both_df[\"movie_id_y\"]).any()\n\nnp.False_\n\n\nBingo, by aligning the reviews with merge() we can directly compare movie IDs for duplicate reviews and none of them are the same. When this dataset was curated, the one-to-many relationship between movies and movie IDs in the IMDB wasn’t accounted for and resulted in not just duplicate reviews in the train and test sets independently, but also leakage of data from the train set into the test set. It gets worse though, we could remove the training reviews that leaked into the test set, but the test set should only contain reviews for movies that aren’t reviewed in the train set. Because of the relationship between movies and movie IDs we have to further process the dataset to see if the movies reviewed in the test set are all different from those in the train set, even after removing duplicates.\nWhy is it important that there are no overlapping movies when the reviews are different? The curators of this dataset give a good explanation in the README they provide,\n\n“In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.” (Maas et al. 2011)\n\nOur goal is to train machine learning models that recognize general patterns of language to predict positive or negative sentiment for a given piece of text. If all the reviews in the train set for Batman Begins are positive, the model may learn to associate words like “Batman”, “Bruce Wayne”, “Christian Bale”, and “Christopher Nolan” with positive sentiment. This doesn’t generalize to other movies. If reviews for Batman Begins are in the test set, the model will likely correctly predict those reviews improving it’s performance on the test set. The test set is supposed to be unbiased, but that goes out the window when data from the train set leaks into the test set.\n\n\n\n\n\n\nNote\n\n\n\nThere are a few forms of data leakage, but what I’ve described here is often called “train-test contamination”. Basically when there’s information specific to the train set that also shows up in the test set. In our case it’s duplicate reviews and movie specific terms. It’s a real problem in machine learning. It can make AI look better than it really is. Finding it in datasets can be tricky, especially as datasets get bigger and bigger. Thought needs to go into preventing sources of leakage and you may never catch every instance of leakage. For example, there’s another instance of leakage that wasn’t considered for this dataset. The train and test sets should contain reviews for a disjoint set of reviewers because individual reviewers may use specific terms or have unique speech patterns that do not generalize to all reviewers and a machine learning model could learn that.\n\n\nFirst we’ll remove the leakage of reviews from the train set into the test set.\n\ndef get_train_test_data():\n    \"\"\"Return train and test `pd.DataFrame`s.\"\"\"\n    train_df = get_train_data()\n    test_df = get_test_data()\n    test_df = test_df[\n        ~test_df[\"review\"].isin(train_df[\"review\"])\n    ].copy()\n    return train_df, test_df\n\nNow, let’s handle the issues that come from the one-to-many relationship of movies and movie IDs. The way movie IDs work in IMDB is there’s one main ID for a movie and other movie IDs for that movie point to the main ID. We saw this earlier when we looked at URLs for duplicate reviews. When you visit those URLs, the duplicate movie ID redirects to the main one. Try it with these URLS, http://www.imdb.com/title/tt0223119/reviews and http://www.imdb.com/title/tt0118939/reviews. Notice how the first link redirects to the second link. That’s because tt0118939 is the main ID for that movie and tt0223119 is a duplicate ID that points to it.\nI’ve taken the liberty of creating a csv file that maps every movie ID in this dataset to their corresponding main movie ID. We’ll use this to replace the movie IDs in our train and test sets then check how many reviews there are per movie and if there’s any overlap of movies between the train and test sets.If you’d like to see details on how I did this check out this README.\n\ndef get_review_data(reviews_dir, urls_file, dedup=False):\n    \"\"\"\n    Return a `pd.DataFrame` containing the review ID,\n    movie ID, rating, and review.\n    \"\"\"\n    with urls_file.open() as f:\n        movie_ids = {\n            i: url.split(\"/\")[4]\n            for i, url in enumerate(f.readlines())\n        }\n\n    movie_id_map = dict(pd.read_csv(\"all_movie_ids.csv\").values)\n\n    data = []\n    for p in (reviews_dir).iterdir():\n        ID, rating = map(int, p.stem.split(\"_\"))\n        data.append(\n            {\n                \"id\": ID,\n                \"movie_id\": movie_id_map[movie_ids[ID]],\n                \"rating\": rating,\n                \"review\": p.open().read().strip(),\n            }\n        )\n\n    rv = pd.DataFrame(data)\n    if dedup:\n        return rv.drop_duplicates(\"review\").copy()\n    return rv\n\nAre there any movies in the original train set with more than 30 reviews?\n\nget_train_data(dedup=False)[\"movie_id\"].value_counts().head()\n\nmovie_id\ntt0235326    32\ntt0169528    32\ntt0171363    30\ntt0187078    30\ntt0049223    30\nName: count, dtype: int64\n\n\nYes there is. And what about in the deduplicated train set?\n\nget_train_data(dedup=True)[\"movie_id\"].value_counts().head()\n\nmovie_id\ntt0118480    30\ntt0023775    30\ntt0049223    30\ntt0277941    30\ntt0284978    30\nName: count, dtype: int64\n\n\nNo there isn’t. So deduplication actually brings the maximum number of reviews per movie down to 30 which was the original intent of this dataset. Let’s repeat this for the test set.\n\nget_test_data(dedup=False)[\"movie_id\"].value_counts().head()\n\nmovie_id\ntt0239496    60\ntt0152015    48\ntt0202381    41\ntt0108915    40\ntt0126810    33\nName: count, dtype: int64\n\n\nBy comparison that’s pretty dramatic. We see one movie with 60 reviews and a couple with over 40!\n\nget_test_data(dedup=True)[\"movie_id\"].value_counts().head()\n\nmovie_id\ntt0365830    30\ntt0373024    30\ntt0365513    30\ntt0079261    30\ntt0086050    30\nName: count, dtype: int64\n\n\nBut it’s the same story when we deduplicate the reviews. It seems the movies with more than 30 reviews are due to duplicate reviews caused by movies with multiple movie IDs. This is good news because the deduplication takes care of the overrepresented movies for us.\nThis leaves one more question about train test contamination though. Are there any movies with reviews in the train and test set?\n\ntrain_df, test_df = get_train_test_data()\ntest_df[\"movie_id\"].isin(train_df[\"movie_id\"]).any()\n\nnp.True_\n\n\nThat is unfortunate. After removing reviews from the test set that appear in the train set, we’re still left with reviews in the test set for at least one movie that is reviewed in the train set. Let’s dig in a little.\n\noverlapping_movies = test_df[\n    test_df[\"movie_id\"].isin(train_df[\"movie_id\"])\n]\noverlapping_movies.shape\n\n(103, 5)\n\n\nThere’s 103 reviews in the test set that shouldn’t be there because their associated movie is reviewed in the train set. How many movies are we talking?\n\nlen(overlapping_movies[\"movie_id\"].unique())\n\n7\n\n\n103 reviews across 7 movies. Let’s also remove these from the test set as they can artificially inflate the benchmarking performance of our models. Instead of removing the duplicate training reviews from the test set, we can filter out reviews with the same movie ID. This will capture duplicates from the same movie.\n\ndef get_train_test_data():\n    \"\"\"Return train and test `pd.DataFrame`s.\"\"\"\n    train_df = get_train_data()\n    test_df = get_test_data()\n    test_df = test_df[\n        (~test_df[\"movie_id\"].isin(train_df[\"movie_id\"]))\n    ].copy()\n    return train_df, test_df\n\n\ntrain_df, test_df = get_train_test_data()\ntrain_df.shape, test_df.shape\n\n((24904, 5), (24576, 5))\n\n\nWe now have 24,904 reviews in the train set and 24,576 in the test set. By removing all reviews from the test set with a movie ID seen in the train set, that should handle duplicate reviews across these groups as well as duplicate movies, but there’s still something wrong with our test set. Our original test set was 25k. We removed 199 duplicate reviews in the test set, then 123 reviews seen in the train set, then 103 reviews with movie IDs seen in the train set. Adding that up doesn’t give us 24,576…\n\n25000 - 199 - 123 - 103\n\n24575\n\n\nOur test set has one to many reviews. That’s because we didn’t actually remove all duplicate reviews in the test set seen in the train set, we just removed the reviews with the same movie IDs. There’s one duplicate review in the train and test sets, it’s just for different movies!\n\ntest_df[test_df[\"review\"].isin(train_df[\"review\"])]\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nreview\nlabel\n\n\n\n\n10020\n12159\ntt0182766\n8\nThere has been a political documentary, of rec...\n1\n\n\n\n\n\n\n\n\ntrain_df[train_df[\"review\"].isin(test_df[\"review\"])]\n\n\n\n\n\n\n\n\nid\nmovie_id\nrating\nreview\nlabel\n\n\n\n\n355\n10643\ntt0184773\n8\nThere has been a political documentary, of rec...\n1\n\n\n\n\n\n\n\nIt turns out these two movies are part of a documentary series. One documentary ended up in the train set, the other in the test set, and one reviewer happened to write the same review for both. All that’s left is for us to remove the reviews from the test set seen in the train set and the reviews from the test set with movie IDs in the train set.\n\ndef get_train_test_data():\n    \"\"\"Return train and test `pd.DataFrame`s.\"\"\"\n    train_df = get_train_data()\n    test_df = get_test_data()\n    same_review = test_df[\"review\"].isin(train_df[\"review\"])\n    same_movie = test_df[\"movie_id\"].isin(train_df[\"movie_id\"])\n    test_df = test_df[~same_review & ~same_movie].copy()\n    return train_df, test_df\n\n\ntrain_df, test_df = get_train_test_data()\ntrain_df.shape, test_df.shape\n\n((24904, 5), (24575, 5))\n\n\nNow we have 24,575 reviews in the test set. All is good and we can move on.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bonus: cleaning data</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/cleaning_data/cleaning_data.html#reflection",
    "href": "chapter/bonus/cleaning_data/cleaning_data.html#reflection",
    "title": "5  Bonus: cleaning data",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nWith that, our data cleaning journey comes to an end. Yes, there is more that could be done, like ensuring no reviews from the same reviewer show up in the train and test sets, but we don’t have user IDs associated with these reviews. Besides, the process would look similar to what we’ve already done, just with a little more leg work. For our purpose of learning about NLP this dataset is fine.\nWe covered a lot of ground, and while we’re here I’d like to take a moment to reflect on what we found.\n\nDuplicate reviews in both the train and test sets.\nReviews are for movies and TV shows.\nMore than 30 reviews for some movies.\n\nFortunately these were all duplicate reviews.\n\nTrain-test contamination.\n\nI especially want to draw your attention to the train-test contamination. The amount of contamination in this dataset may be negligible when it comes to benchmarking machine learning models, I don’t really know. But the fact that it’s there and that this dataset is provided by multiple deep learning libraries as well as used for benchmarking tasks in research (Howard and Ruder 2018) should make you pause. I’ve used datasets at face value without questioning them and I guarantee people have taken this dataset at face value. These libraries provide the data as-is. 25k train and 25k test reviews, but we know there’s not really 25k reviews in each set. This is not a critique of the dataset or it’s curators, the libraries that provide it, or the researchers that use it. This is a reminder to verify the data is actually what you think it is because we’ve seen here that it isn’t always what it looks like.Researchers may perform their own preprocessing of the data as we have here. I point to this paper as an example of researchers using the dataset because it is a popular dataset, not as an example of someone using it without preprocessing. In fact, I think this paper is so important it got it’s own chapter.\n\n5.3.1 Keep asking “why?”\nNow that we’ve reached the end, did we answer all the questions we set out to answer? Did you come up with other questions while we worked through this? If we created a similar dataset from scratch today, what would you do differently?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bonus: cleaning data</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/cleaning_data/cleaning_data.html#unsupervised-learning-data",
    "href": "chapter/bonus/cleaning_data/cleaning_data.html#unsupervised-learning-data",
    "title": "5  Bonus: cleaning data",
    "section": "5.4 Unsupervised learning data",
    "text": "5.4 Unsupervised learning data\nThe IMDB dataset includes an unsupervised learning dataset. The unsupervised set has no ratings and no labels. It’s just reviews. Like the train and test set we’ve already gone over, the same principle of deduplication applies and that’s really all we need.\n\ndef get_unsup_data(dedup=True):\n    \"\"\"\n    Return a `pd.DataFrame` with the unsupervised data.\n    \"\"\"\n    rv = get_review_data(\n        data_path / \"train/unsup\",\n        data_path / \"train/urls_unsup.txt\",\n        dedup,\n    )\n    rv.drop(columns=\"rating\", inplace=True)\n    # Drop the ratings column since every review in the\n    # unsupervised set is given a rating of 0 regardless\n    # of what it's rating is.\n    return rv\n\n\nunsup_df = get_unsup_data()\nunsup_df.shape\n\n(49507, 3)\n\n\n\nunsup_df[\"movie_id\"].value_counts().head()\n\nmovie_id\ntt0325596    30\ntt0086856    30\ntt0758053    30\ntt0284850    30\ntt0469062    30\nName: count, dtype: int64\n\n\nOk, we’re really done now. Thanks for bearing with me. Cleaning data is probably my least favorite part of machine learning because it can feel like busy work, but it’s so important. Even if you leave the dataset the way you found it, it’s a great opportunity to learn about the dataset before you do any modeling. I often find that data cleaning is an ongoing process as I build machine learning models because the models can point to oddities in the data I never saw during my initial exploration. You will find article after article about how machine learning works, with little discussion of how the data was prepared. I want you to walk away from this chapter knowing that cleaning data, analysis, and machine learning are all intertwined.\n\n\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Fine-Tuned Language Models for Text Classification.” CoRR abs/1801.06146. http://arxiv.org/abs/1801.06146.\n\n\nMaas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. “Learning Word Vectors for Sentiment Analysis.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142–50. Portland, Oregon, USA: Association for Computational Linguistics. http://www.aclweb.org/anthology/P11-1015.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bonus: cleaning data</span>"
    ]
  },
  {
    "objectID": "chapter/bonus/quality_vs_quantity/quality_vs_quantity.html",
    "href": "chapter/bonus/quality_vs_quantity/quality_vs_quantity.html",
    "title": "6  Bonus: quality vs. quantity",
    "section": "",
    "text": "While we dive into many algorithms over the course of this book, I want to drive home that machine learning models need data to learn. We can have the fanciest neural network on the best GPU cluster in the world, but without data, it’s all pointless. To drive this point home let’s build a linear regression on the simplest of equations.\nWe’ll start with a line. Nothing special, just the equation f(x) = x + 1.\n\nimport seaborn as sns\n\nsns.set_theme(style=\"darkgrid\")\n\n\ndef f(x):\n    \"\"\"Function that applies `x + 1` to `x`.\"\"\"\n    return x + 1\n\n\ndef get_y(xs):\n    \"\"\"Return `y` for every `x` in the list `xs`.\"\"\"\n    return list(map(f, xs))\n\n\nx = [-1, 1]\ny = get_y(x)\nsns.lineplot(x=x, y=y)\n\n\n\n\n\n\n\n\nThis line is the ground truth. It is real. But let’s imagine we don’t have the line. We just have a couple random points on the line.\n\nimport random\nimport pandas as pd\n\nrandom.seed(100392)\n\n\ndef random_x(n):\n    \"\"\"Return `n` random values from -1 to 1.\"\"\"\n    return [random.uniform(-1, 1) for _ in range(n)]\n\n\nxs = random_x(2)\nys = get_y(xs)\nsns.scatterplot(x=xs, y=ys)\n\n\n\n\n\n\n\n\nVisually, we could draw a line from dot to dot and we’d get the same line in the previous plot. But we’re not interested in figuring out the line ourselves. We’re here to have machine learning do it for us. This is already integrated into seaborn so there’s not much code for us to write. The regplot function will plot our data points and use machine learning to fit a line to those data points.\n\nsns.regplot(x=xs, y=ys, truncate=False, ci=False)\n\n\n\n\n\n\n\n\nWill you look at that! The line was perfectly predicted based on the data points.\nBut real world data isn’t this simple or this clean. There’s usually some amount of noise in the data that can affect the quality of predictions. Noise is randomness introduced to the data. It can come from many places like imprecise measurements, mislabelling, or unaccounted variables to name a few.\nNow let’s imagine we don’t have points on the line, but points close to the line. How will this change the predicted line?\n\n# Add noise to the data points to simulate noisy data.\ndef noisy_process(xs, noise):\n    \"\"\"Return `f(x)` with noise.\"\"\"\n    return [\n        y + random.uniform(-noise, noise)\n        for x, y in zip(xs, get_y(xs))\n    ]\n\n\nys_noisy = noisy_process(xs, 0.25)\n\nax = sns.regplot(\n    x=xs,\n    y=ys,\n    truncate=False,\n    ci=False,\n    scatter_kws={\"alpha\": 0.5},\n)\nsns.regplot(\n    x=xs,\n    y=ys_noisy,\n    truncate=False,\n    ci=False,\n    ax=ax,\n    scatter_kws={\"alpha\": 0.5},\n)\n\n\n\n\n\n\n\n\nHmm, that doesn’t look so bad. The predicted line in orange is pretty close to the real line in blue and has the same slope. But what if we just got lucky and the random noise happened to be low? Maybe we should run it a few more times to be sure.\n\nimport pandas as pd\n\n\ndef simulate_n(n, n_points, noise):\n    \"\"\"\n    Return a `pd.DataFrame` with `n` separate simulations\n    of `n_points`.\n    \"\"\"\n    data = []\n    real_x = [-1, 1]\n    real_y = get_y(real_x)\n    for x, y in zip(real_x, real_y):\n        data.append(\n            {\n                \"x\": x,\n                \"y\": y,\n                \"simulation\": \"Ground Truth\",\n                \"noise\": noise,\n            }\n        )\n    for i in range(n):\n        xs = random_x(n_points)\n        ys = noisy_process(xs, noise)\n        for x, y in zip(xs, ys):\n            data.append(\n                {\n                    \"x\": x,\n                    \"y\": y,\n                    \"simulation\": i + 1,\n                    \"noise\": noise,\n                }\n            )\n    return pd.DataFrame(data)\n\n\ndf = simulate_n(4, 2, 0.25)\n# `lmplot` is a fancier version of `regplot` that can handle\n# multiple lines.\nsns.lmplot(\n    df,\n    x=\"x\",\n    y=\"y\",\n    ci=False,\n    hue=\"simulation\",\n    truncate=False,\n    scatter_kws={\"alpha\": 0.5},\n    facet_kws={\"xlim\": (-1, 1), \"ylim\": (0, 2)},\n)\n\n\n\n\n\n\n\n\nMost of these look pretty close to the blue line, the ground truth, but what’s going on with simulation 4? The noise confused the machine learning model. It only has two points to work with and given two points it can figure out a line that perfectly lies on those two points. These two points imperfectly represent the real line and so the machine learned the line of this imperfect representation. And it gets worse with more noise.\n\ndf = pd.concat([simulate_n(4, 2, noise) for noise in [0.25, 0.5, 1]])\nsns.lmplot(\n    df,\n    x=\"x\",\n    y=\"y\",\n    ci=False,\n    hue=\"simulation\",\n    truncate=False,\n    col=\"noise\",\n    scatter_kws={\"alpha\": 0.5},\n    facet_kws={\"xlim\": (-1, 1), \"ylim\": (0, 2)},\n)\n\n\n\n\n\n\n\n\nYup, that’s a mess. At noise = 0.5 the lines start to drift from the real line and at noise = 1.0 most of them aren’t even close. But all is not lost, there is a way to learn from noisy data. Use more of it! Let’s try a hundred points per simulation.\n\ndf = pd.concat(\n    [simulate_n(4, 100, noise) for noise in [0.25, 0.5, 1]]\n)\nsns.lmplot(\n    df,\n    x=\"x\",\n    y=\"y\",\n    ci=False,\n    hue=\"simulation\",\n    truncate=False,\n    col=\"noise\",\n    scatter_kws={\"alpha\": 0.5},\n    facet_kws={\"xlim\": (-1, 1), \"ylim\": (0, 2)},\n)\n\n\n\n\n\n\n\n\nThat’s night and day compared to two data points. Even when noise = 1.0 the machine learning model does a good job of finding the line. This is because there’s more information to learn from and the machine learning model can separate the noise and zero in on the truth.\nThe lesson here is data quality is important, but so is data quantity. Sometimes the answer is a better machine learning model, but that’s not always true, you might just need more data. On the other hand, if you can’t get enough data, you may need better quality data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bonus: quality vs. quantity</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Holte, Robert C. 1993. “Very Simple Classification Rules Perform\nWell on Most Commonly Used Datasets.” Machine Learning\n11 (1): 63–90. https://doi.org/10.1023/A:1022631118932.\n\n\nHoward, Jeremy, and Sebastian Ruder. 2018. “Fine-Tuned Language\nModels for Text Classification.” CoRR abs/1801.06146. http://arxiv.org/abs/1801.06146.\n\n\nMaas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y.\nNg, and Christopher Potts. 2011. “Learning Word Vectors for\nSentiment Analysis.” In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguistics: Human Language\nTechnologies, 142–50. Portland, Oregon, USA: Association for\nComputational Linguistics. http://www.aclweb.org/anthology/P11-1015.",
    "crumbs": [
      "References"
    ]
  }
]