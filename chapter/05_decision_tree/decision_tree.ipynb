{
 "cells": [
  {
   "cell_type": "raw",
   "id": "014f8460-345e-4a65-9a2c-21a3b5380309",
   "metadata": {},
   "source": [
    "---\n",
    "title: More rules with Decision Trees\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72d1f2-1946-4972-a788-9488e6922ac1",
   "metadata": {},
   "source": [
    "We made progress with [OneR](../04_boc/boc.ipynb) which makes predictions on one feature of the dataset. The next logical step is to make predictions on combinations of features. This is what a [decision tree](https://en.wikipedia.org/wiki/Decision_tree) does; it repeatedly splits the data into groups based on the values of different features. When making a prediction it finds the most appropriate group for the input and returns the most common label from that group. Our work with decision trees will span multiple chapters as we'll explore some important topics in machine learning using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266a3f1-b3a6-4175-9e13-35f9c6a440d7",
   "metadata": {},
   "source": [
    "## Easy button\n",
    "\n",
    "We'll use bag of characters feature extraction like we did last chapter and feed that to the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e11c15f-cbfe-4e78-9299-e892605fd424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5526347914547304"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nlpbook import get_train_test_data\n",
    "\n",
    "# Grab the data and extract the features and labels.\n",
    "train, test = get_train_test_data()\n",
    "features = \"review\"\n",
    "label = \"label\"\n",
    "X, y = train[features], train[label]\n",
    "X_test, y_test = test[features], test[label]\n",
    "\n",
    "# Set up the pipeline.\n",
    "boc = CountVectorizer(analyzer=\"char\", lowercase=False)\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline([(\"boc\", boc), (\"decision_tree\", model)])\n",
    "\n",
    "# Train it!\n",
    "pipeline.fit(X, y)\n",
    "# Score it!\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbbc0e-c4dd-4b8f-9f54-6273aa14cfa1",
   "metadata": {},
   "source": [
    "How does this compare to our previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a61646-13e2-43de-80bb-79d684e6001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.501119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OneR (length)</th>\n",
       "      <td>0.502665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OneR (boc)</th>\n",
       "      <td>0.581282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy\n",
       "Model                  \n",
       "Baseline       0.501119\n",
       "OneR (length)  0.502665\n",
       "OneR (boc)     0.581282"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "from nlpbook import get_results\n",
    "\n",
    "get_results([\"Baseline\", \"OneR (length)\", \"OneR (boc)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b0f6a-f31c-4696-810e-79417b82e04f",
   "metadata": {},
   "source": [
    "It actually performed worse than OneR with bag of characters features. We have extracted features from the reviews and we're using a better model, so in theory it should perform better. We'll cover why this is happening in a future chapter where we explore feature importance and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5560b10-1953-484a-8ec9-e0ca78703a0a",
   "metadata": {},
   "source": [
    "## Rolling our own\n",
    "\n",
    "We'll get straight to implementing it ourself. As the name implies a decision tree uses a binary tree structure. They operate on a simple premise, split the data into two groups based on the most predictive feature, then repeat on each subgroup, and so on.\n",
    "\n",
    "So how do we split the data into two groups? Let's start with the question mark feature as an example. We know reviews with question marks are likely to be negative and vice versa for reviews without question marks. This gives us two groups, those with question marks and those without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb89193c-d3f4-40f8-8c7b-056d96c6ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the index of the question mark feature.\n",
    "question_idx = boc.vocabulary_[\"?\"]\n",
    "\n",
    "# Transform the data to a bag of characters.\n",
    "# Convert sparse matrix to numpy array as well.\n",
    "X_boc = boc.transform(X).toarray()\n",
    "\n",
    "# Get the indices of the group with question marks.\n",
    "group1_idxs = X_boc[:, question_idx] > 0\n",
    "# And those without.\n",
    "group2_idxs = ~group1_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f610eb-5cb4-4620-bc3f-ffca656104ea",
   "metadata": {},
   "source": [
    "Now we let's create baseline classifiers for each of these groups and see what the accuracy is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6046100-95f9-41db-b6e4-88dd121dd444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.581322482197355"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Baseline classifier for each group.\n",
    "group1 = DummyClassifier().fit(X_boc[group1_idxs], y[group1_idxs])\n",
    "group2 = DummyClassifier().fit(X_boc[group2_idxs], y[group2_idxs])\n",
    "\n",
    "# Do unto the test data what you do to the train data.\n",
    "X_test_boc = boc.transform(X_test).toarray()\n",
    "group1_idxs_test = X_test_boc[:, question_idx] > 0\n",
    "group2_idxs_test = ~group1_idxs_test\n",
    "\n",
    "# Get the predictions for each group.\n",
    "pred = np.zeros(len(y_test), dtype=int)\n",
    "pred[group1_idxs_test] = group1.predict(X_test_boc[group1_idxs_test])\n",
    "pred[group2_idxs_test] = group2.predict(X_test_boc[group2_idxs_test])\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57a3e1-42b5-47e8-a158-a47c603632e2",
   "metadata": {},
   "source": [
    "OK, we basically get the OneR performance just by splitting the data into two groups on the question mark feature. Let's write an algorithm that finds the best value to split the data on for any feature. With simple brute force we'll iterate over all possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26376d3f-c023-4516-9059-69fe91b48d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.581322482197355"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "class Split(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"Split the data on the feature value.\"\"\"\n",
    "\n",
    "    def __init__(self, idx, value):\n",
    "        # Index of the feature matrix.\n",
    "        self.idx = idx\n",
    "        # Value to split the data on.\n",
    "        self.value = value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert other data types to numpy array\n",
    "        # for consistency.\n",
    "        X, y = np.array(X), np.array(y)\n",
    "\n",
    "        # Grab class labels.\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        # Create boolean arrays to split the groups on.\n",
    "        rhs = X[:, self.idx] >= self.value\n",
    "        lhs = ~rhs\n",
    "\n",
    "        # Create baseline classifiers for each split.\n",
    "        self.lhs_ = DummyClassifier().fit(X[lhs], y[lhs])\n",
    "        self.rhs_ = DummyClassifier().fit(X[rhs], y[rhs])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert other data types to numpy array\n",
    "        # for consistency.\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Make our empty prediction array.\n",
    "        pred = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "        # Create boolean arrays to split the groups on.\n",
    "        rhs = X[:, self.idx] >= self.value\n",
    "        lhs = ~rhs\n",
    "\n",
    "        # Populate the prediction array with predictions from\n",
    "        # each group.\n",
    "        if lhs.sum() > 0:\n",
    "            pred[lhs] = self.lhs_.predict(X[lhs])\n",
    "        if rhs.sum() > 0:\n",
    "            pred[rhs] = self.rhs_.predict(X[rhs])\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    \"\"\"Iterate over all possible values in `X` to find the best\n",
    "    split point.\"\"\"\n",
    "    # Convert other data types to numpy array\n",
    "    # for consistency.\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Variables for the two groups.\n",
    "    best_split = best_score = None\n",
    "\n",
    "    # Iterate over each feature.\n",
    "    for i, column in enumerate(X.T):\n",
    "        # Iterate over each unique value in column.\n",
    "        for value in np.unique(column):\n",
    "            try:\n",
    "                split = Split(i, value).fit(X, y)\n",
    "            except ValueError:\n",
    "                # `DummyClassifier` will raise a `ValueError`\n",
    "                # if it is trained on an empty dataset, in which\n",
    "                # case we just skip this split.\n",
    "                continue\n",
    "\n",
    "            # Score the split on this value.\n",
    "            score = split.score(X, y)\n",
    "\n",
    "            # Keep this split if it has the best score so far.\n",
    "            if best_score is None or score > best_score:\n",
    "                best_split = split\n",
    "                best_score = score\n",
    "\n",
    "    return best_split\n",
    "\n",
    "\n",
    "# Find the best split on the training data.\n",
    "split = find_best_split(boc.transform(X).toarray(), y)\n",
    "# Score the best split.\n",
    "split.score(boc.transform(X_test).toarray(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f41180-7e45-4fb0-a88d-9e1d7b1c42dd",
   "metadata": {},
   "source": [
    "And we get the same result! We now have an automated way to find the best split on a dataset. The next step is to further divide each group. We just repeat the process on each group, further dividing the data. In order to split a group, we must make the split on the subset of the data that group was trained on. We trained two baseline classifiers with two subsets of the data, now we replace those baseline classifiers with `Split` models that are trained on those same subsets. Because the `Split` models are `sklearn` estimators we can replace the baseline models and they'll just work when we call the appropriate methods.\n",
    "\n",
    "Let's start by splitting just the left hand side group. We need to get the appropriate subset of the training data and make a new split with `find_best_split`, replacing the baseline classifier that was trained on that same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83befec9-1812-4a40-93ff-e01912f06bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5853916581892167"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our input to a bag of characters.\n",
    "X_boc = boc.transform(X).toarray()\n",
    "\n",
    "# Create a boolean array to get the left hand side group.\n",
    "lhs = X_boc[:, split.idx] < split.value\n",
    "\n",
    "# Grab the part of the dataset that was used to train the\n",
    "# left hand side baseline classifier.\n",
    "X_sub = X_boc[lhs]\n",
    "y_sub = y[lhs]\n",
    "\n",
    "# Create a new split for just the left hand side group,\n",
    "# replacing the baseline classifier.\n",
    "split.lhs_ = find_best_split(X_sub, y_sub)\n",
    "\n",
    "# Score the new model.\n",
    "split.score(boc.transform(X_test).toarray(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f5af5-80fa-4e2f-96c8-e4137dbf3f2b",
   "metadata": {},
   "source": [
    "Progress. We now have two splits. The first splits the data into two groups, and the second splits the left hand side into two more groups giving three groups in total.\n",
    "\n",
    "Let's code the process up to find all the optimal splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46351e38-42c1-4cd8-bd6c-3ebc160ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement depth first search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
