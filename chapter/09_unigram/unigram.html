<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Generating gibberish with unigrams – NLP: From Simple to Spectacular</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapter/bonus/cleaning_data/cleaning_data.html" rel="next">
<link href="../../chapter/08_tokens/tokens.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html">Generative AI</a></li><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">NLP: From Simple to Spectacular</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/01_data/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Machine learning needs data</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/02_baseline/baseline_classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Baseline: gotta start somewhere</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/03_oner/oner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">OneR</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Representation learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/04_boc/boc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OneR, TwoR, RedR, BlueR: Inputs matter</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/05_decision_tree/decision_tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">More rules with Decision Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/06_loss/loss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">What’s in a loss?</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Representation learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/07_bow/bow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Words &gt; characters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/08_tokens/tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">[‘T’, ‘o’, ‘k’, ‘e’, ‘n’, ‘s’]</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Generative AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/09_unigram/unigram.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Bonus chapters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/bonus/cleaning_data/cleaning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Cleaning data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/bonus/quality_vs_quantity/quality_vs_quantity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quality vs.&nbsp;quantity</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#unigram-model" id="toc-unigram-model" class="nav-link active" data-scroll-target="#unigram-model"><span class="header-section-number">9.1</span> Unigram model</a></li>
  <li><a href="#sampling-tokens" id="toc-sampling-tokens" class="nav-link" data-scroll-target="#sampling-tokens"><span class="header-section-number">9.2</span> Sampling tokens</a></li>
  <li><a href="#metrics-metrics-metrics" id="toc-metrics-metrics-metrics" class="nav-link" data-scroll-target="#metrics-metrics-metrics"><span class="header-section-number">9.3</span> Metrics, metrics, metrics!</a>
  <ul class="collapse">
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity"><span class="header-section-number">9.3.1</span> Perplexity</a></li>
  </ul></li>
  <li><a href="#smoothing" id="toc-smoothing" class="nav-link" data-scroll-target="#smoothing"><span class="header-section-number">9.4</span> Smoothing</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together"><span class="header-section-number">9.5</span> Putting it all together</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html">Generative AI</a></li><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>With the introduction of ChatGPT to the world, generative AI is all the rage. This will be our first taste of generative AI. Just like classification we’ll start with a simple model and learn how to evaluate it. Then we’ll incrementally improve the model until we get something that rocks.</p>
<section id="unigram-model" class="level2 page-columns page-full" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="unigram-model"><span class="header-section-number">9.1</span> Unigram model</h2>
<p>One of the simplest models we can make is a unigram model. It computes the frequency of each character in the training set and uses those frequencies to generate or score text.</p>
<div class="page-columns page-full"><p>Let’s load the unsupervised training data and find the frequencies of each token. We’ll use the tokenizer we created <a href="../../chapter/08_tokens/tokens.html">last chapter</a>.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">You can also find the package implementation on <a href="https://github.com/spenceforce/NLP-Simple-to-Spectacular/blob/main/nlpbook/preprocessing/tokenizer.py">GitHub</a>.</span></div></div>
<div id="73684cce-45e4-476f-8ac7-617fc0d531b5" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> nlpbook <span class="im">import</span> get_unsup_data</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> nlpbook.preprocessing.tokenizer <span class="im">import</span> CharTokenizer</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># We want to split the dataset into train and test sets.</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># We'll save the test set for later.</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>train_df, test_df <span class="op">=</span> get_unsup_data(split<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co"># Train the tokenizer with the reviews in the train set.</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>tokenizer <span class="op">=</span> CharTokenizer()</span>
<span id="cb1-12"><a href="#cb1-12"></a>tokenizer.train(train_df[<span class="st">"review"</span>])</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co"># Now we'll encode the train set and get the frequencies of each token.</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>encoding_counts <span class="op">=</span> np.zeros(<span class="bu">len</span>(tokenizer.tokens))</span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="cf">for</span> encoding <span class="kw">in</span> tokenizer.encode_batch(train_df[<span class="st">"review"</span>]):</span>
<span id="cb1-17"><a href="#cb1-17"></a>    <span class="co"># Get the encoding values and their counts.</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>    unique, counts <span class="op">=</span> np.unique(encoding, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-19"><a href="#cb1-19"></a>    <span class="co"># Add each count to it's respective index.</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>    encoding_counts[unique] <span class="op">+=</span> counts</span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="co"># Convert the counts to frequencies.</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>encoding_frequencies <span class="op">=</span> encoding_counts <span class="op">/</span> encoding_counts.<span class="bu">sum</span>()</span>
<span id="cb1-23"><a href="#cb1-23"></a>encoding_frequencies[:<span class="dv">4</span>]  <span class="co"># Show just the first 4 frequencies.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>array([2.63901424e-06, 1.68090079e-08, 1.75248850e-01, 7.06936444e-04])</code></pre>
</div>
</div>
<p>Now that we have frequencies, let’s see if we can generate some text.</p>
</section>
<section id="sampling-tokens" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sampling-tokens"><span class="header-section-number">9.2</span> Sampling tokens</h2>
<p>We’ll generate text iteratively, adding one token at a time until “&lt;eos&gt;” is the final token. The frequencies are how we will pick which token at each step.</p>
<p>There are a few ways to generate text from frequencies. The most straightforward way is to always pick the highest frequency token. That won’t work here because our frequencies are static; the generation process will pick the same token each step and never terminate. Instead we’ll sample the tokens based on their frequencies.</p>
<p>When we discussed <a href="../../chapter/06_loss/loss.html">loss functions</a> we touched on <a href="../../chapter/06_loss/loss.html#sec-loss-probability">probabilities</a>. Turns out our frequencies are also probabilities. The frequency is how often a character appears in the training data. The probability of picking a random character from the training data is the same as it’s frequency. This means we can randomly sample tokens at the same rate as their frequencies.</p>
<div id="5a506f2d-1f0e-4ffa-9f41-dfe5ff0e689c" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Create our random generator for sampling.</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">100392</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="kw">def</span> generate():</span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="co">"""Generate a review!"""</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="co"># Make a list of possible token encoding values.</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>    tok_encs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(tokenizer.tokens)))</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="co"># Start the encoding with the `&lt;cls&gt;` token.</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    encoding <span class="op">=</span> [tokenizer.cls_idx]</span>
<span id="cb3-11"><a href="#cb3-11"></a>    <span class="co"># Keep generating until there is a `&lt;eos&gt;` token.</span></span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="cf">while</span> encoding[<span class="op">-</span><span class="dv">1</span>] <span class="op">!=</span> tokenizer.eos_idx:</span>
<span id="cb3-13"><a href="#cb3-13"></a>        <span class="co"># Sample the token encoding values at the same rate as their</span></span>
<span id="cb3-14"><a href="#cb3-14"></a>        <span class="co"># frequencies.</span></span>
<span id="cb3-15"><a href="#cb3-15"></a>        encoding.append(rng.choice(tok_encs, p<span class="op">=</span>encoding_frequencies))</span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="co"># Return the generated text as a string.</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="cf">return</span> tokenizer.decode(encoding)</span>
<span id="cb3-18"><a href="#cb3-18"></a></span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a>generate()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>'se c serrsnfe a,guden  n sehmsole mnio    ebS. eilrnoo egosrt to.tsruh aoaeuairn leytckofrehreo  lwtictItiaobe   t niosni,irpap sr.eat&lt;cls&gt;hah oihpyein has wyd.ye,yvyd,snsyhntlei  olyf l ih ,I (oool bi fneonshoeohfKakl ot m e-tcwr sfctwta p at elitfemk sonhvT&gt;tbpuftsioapt aueDkfu\'oiCeatgcshusit kowriihen iusnhif alrawigiehc lmiroodaiy sfIc hha y r&gt;l(  drl etelgo enewoa iitfhltlaeal po dqsltdi ab  uaont seesf wn.d  thtoe\'e/ hknc  y sb  mihftaeiimethb hnnrnao tafhs  iaatgrdeoatlet e   rs eas(Patdtowtumlgte"il wmd d fe(hedtfanrst It  tcter I ec,l &gt;o c uolVr  ititnsas  ydrucgriseet d , tendlfeh \'inkao r t crhac&lt;cls&gt;acnwobots g tvae pdu It,p thn moa eatrmmoovioreimoaws , eehanthfrbhtTi r(uhIleatrs  otochdpemi rtesnus kenb nbarnsm. msEifasso ns o cu  epelp Ieibl  ool  do i e taa eo- tg gelin attr etsodh re s  h i serdgnst iei i slugrreiee kehlayc nbigoletinhrfg le oh hnh ra&gt; uRbcben ea rtidyifl    .s omtw\'\' e tDean o mcseshbaceneIr :\' wo,ee,  aa ta jg itrotu tusesthL ileipheti  Iti afl huWta,hainrhet ctb b snElec tmdestnr o  haestar renaah rnheaotYhaetah ssranusFelsiadto o.sslrit tso eevDaoysyt rls  dnaie osh drrRlihw itroe /eueibae iaft raioy vt ./ahmnnewe Uelt/,he aaug oasdtin,tnTyvn  ted n oy.oeo  rri idgsloAabfarhsn &lt;cls&gt; 0n/rnsas hfiol thcej"roarnncosoasertatiomt daNelwenn sadsel stilrr sr&lt;ee f  ssr.ytut iwa thih hB ttottdw  nwn leaoondeflbmodvaihA ougddsa  nalIsix  lWhr r dn dep,se,ePelranh e /  iyre-?n-swhegi .pcm.letm o   hiSr eoun   rno  togmoai-Budhne  dhtooiiodeompr tntnlad uiecs*oea imodhsrnewiinlo  nhye uaivuh c mrheannn r. p iresteiss feIhfdw os ue po che,  liai wmetcn g rce  ep  getarsnlm  eshaw&lt;cls&gt;unns cnylkee etetdea.eat ranh,,e 6 ciAehr tob che had anmw   recicrrnu.ol\'uhlsn iandta heb oe mchahbte,   ta rMp  odtueba tisatreoeretiue n gco ooe&gt;  ladrea lraitni lnrlCgi .sest    bpa,sOren  r t teer oea ?trirGeyma  trn.fs  yl o)tol.ff tTnodielnt fm e"geektelwutgaokeulA ca cacaoiglr ls ecieerat hStlr rls 9  m  eina  irtbaslrs  t d t  lls\' entscwaesam ibft,eikmttla,vmeuedtti unelmsnset aAlIe lehsfwiv n  ln,aUk lhl iodpsnaEr  p  ROnettnvr\'an\'li siasarminchymos desfiar grbh k.in  o&lt;hcdeehatmtrpoa nlrde encel nefceittreleu i\'   dht oo"tiae a   ,lnnveiinaaborbeeo \'m1mrara  odhtoote bne oh ie  rrt\'t oa  esi-sn sne  trts olsmosehocsla  h s tlifotdgef(lvh eessliyrnver pw a se s ec   gnsdyel,tyicice beysrnee ffr ai a i a hdyedooefes "d .ufcle  sopef frtnanfyihi- r'</code></pre>
</div>
</div>
<p>Wow that’s gibberish…but it’s our gibberish! :D</p>
<p>While it’s cool that we can generate (unintelligible) text, and while we’re still a long way from a movie review, we still need a way to assess the quality of the model.</p>
</section>
<section id="metrics-metrics-metrics" class="level2 page-columns page-full" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="metrics-metrics-metrics"><span class="header-section-number">9.3</span> Metrics, metrics, metrics!</h2>
<p>We can’t get away from it. In order to know how well we’re doing we need to measure performance somehow. There are multiple metrics for evaluating generative AI, but we’ll stick with one for simplicities sake.</p>
<section id="perplexity" class="level3 page-columns page-full" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="perplexity"><span class="header-section-number">9.3.1</span> Perplexity</h3>
<p>Perplexity measures how surprised a model is when guessing the next token. Lower perplexity means better guesses. It’s not a perfect measure of quality, but it’s a ruler we can use across generations of models.</p>
<div class="page-columns page-full"><p>This metric starts with probability. Let’s say we have the text “A cat.”. We need to find the probability of the whole text from our model. Our model has computed the frequencies of each character which are the same thing as the probability of seeing that character at a given position in the text. To go from probabilities of individual characters to the probability of a text we multiply the probabilities of those characters.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">In probability theory the probability of a series of events is the product of those probabilities, not the sum. Text is a series of characters, which is equivalent to a series of events.</span></div></div>
<div id="065c9470-04b0-4512-a78e-ff9e03867051" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> text_probability(encoding):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="co"># Get the probabilities.</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    probabilities <span class="op">=</span> [encoding_frequencies[i] <span class="cf">for</span> i <span class="kw">in</span> encoding]</span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="co"># Compute the total probability.</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    probability <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="cf">for</span> x <span class="kw">in</span> probabilities:</span>
<span id="cb5-7"><a href="#cb5-7"></a>        probability <span class="op">*=</span> x</span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="cf">return</span> probability</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a>text_probability(tokenizer.encode(<span class="st">"A cat."</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>np.float64(1.6571918515494592e-16)</code></pre>
</div>
</div>
<p>Okay, now we have the probability for “A cat.”. Let’s compare the probability of “A cat.” to the probability of “A cat lounging in the sun.”.</p>
<div id="d9ce5776-a46e-465b-ba74-5f14788c0fe7" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>text_probability(tokenizer.encode(<span class="st">"A cat lounging in the sun."</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>np.float64(8.567467898166924e-42)</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>That’s much smaller in comparison. Turns out this comparison isn’t fair because “A cat.” is a shorter sentence. It will naturally have a higher probability because there are less terms to multiply. To make this fair we should take an average probability of the sequence and since we’re multiplying probabilities, the <a href="https://en.wikipedia.org/wiki/Geometric_mean"><em>geometric mean</em></a> is a natural fit.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Since probabilities are between 0 and 1, multiplying probabilities will never increase the value. The value will either stay the same (if the probability is 1) or decrease.</span><span class="margin-aside">The geometric mean is similar to the mean but uses multiplication instead of addition. We multiply the numbers, then take the _n_th root where <em>n</em> is the number of elements multiplied</span></div></div>
<div id="e6333011-ccea-4625-9526-11dcc43c45fa" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">def</span> geometric_mean(encoding):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="co"># Get the probabilities.</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>    probabilities <span class="op">=</span> [encoding_frequencies[i] <span class="cf">for</span> i <span class="kw">in</span> encoding]</span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="co"># Compute the total probability.</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    probability <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>    <span class="cf">for</span> x <span class="kw">in</span> probabilities:</span>
<span id="cb9-7"><a href="#cb9-7"></a>        probability <span class="op">*=</span> x</span>
<span id="cb9-8"><a href="#cb9-8"></a>    <span class="co"># Return the geometric mean.</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>    <span class="cf">return</span> probability <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> <span class="bu">len</span>(probabilities))</span>
<span id="cb9-10"><a href="#cb9-10"></a></span>
<span id="cb9-11"><a href="#cb9-11"></a></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="cf">for</span> text <span class="kw">in</span> [<span class="st">"A cat."</span>, <span class="st">"A cat lounging in the sun."</span>]:</span>
<span id="cb9-13"><a href="#cb9-13"></a>    encoding <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb9-14"><a href="#cb9-14"></a>    <span class="bu">print</span>(<span class="ss">f"Mean probability of '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">':"</span>, geometric_mean(encoding))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean probability of 'A cat.': 0.010651765544802292
Mean probability of 'A cat lounging in the sun.': 0.03414413856423841</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>Now we’re cooking! Turns out our model actually thinks “A cat lounging in the sun.” is a more likely text than “A cat.” when we account for the differing lengths. But this still isn’t perplexity. Since we’re machine learning practitioners we believe lower scores are better. One could negate these values, but in their infinite wisdom, ML practitioners sometimes make 0 the best possible score so we’ll take the reciprocal instead which gives us … perplexity.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">ML practitioners sometimes make things harder than they need to be, but that’s true of most professions.</span></div></div>
<div id="fc27fadb-f0d4-4528-99b9-74aae6911ccf" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">def</span> perplexity(encoding):</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> geometric_mean(encoding)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="cf">for</span> text <span class="kw">in</span> [<span class="st">"A cat."</span>, <span class="st">"A cat lounging in the sun."</span>]:</span>
<span id="cb11-6"><a href="#cb11-6"></a>    encoding <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb11-7"><a href="#cb11-7"></a>    <span class="bu">print</span>(<span class="ss">f"Perplexity of '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">':"</span>, perplexity(encoding))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Perplexity of 'A cat.': 93.88115010548339
Perplexity of 'A cat lounging in the sun.': 29.287603730831016</code></pre>
</div>
</div>
<p>That was a journey, but now we can start scoring our reviews. Let’s give it a shot on the first one in our test set.</p>
<div id="165ca7bb-ab9e-42cd-9175-042c2c41db3d" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>perplexity(tokenizer.encode(test_df[<span class="st">"review"</span>].iloc[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_4051/2293807975.py:2: RuntimeWarning: divide by zero encountered in scalar divide
  return 1 / geometric_mean(encoding)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>np.float64(inf)</code></pre>
</div>
</div>
<p>NOOOOOOO!!! We did all this work just to get a perplexity of infinity!? We’ve run into a classic problem of theory meeting reality. As we multiply probabilities, the value gets smaller and smaller. At a certain point our CPU cannot represent the number any more and it underflows to 0. Then when we take the reciprocal we get a divide by zero error which <code>numpy</code> converts to infinity.</p>
<div class="page-columns page-full"><p>But all is not lost. Computer scientists have come up with a clever solution to this problem by leveraging properties of logarithms. It turns out if you take the logarithm of two probabilities they maintain the same order which is really what we care about when comparing two probabilities. Order matters, not value. Logarithms of products are the same as the sum of logarithms.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Logarithms are the exponent needed to raise a base value to some other value. It’s related to exponentiation. If <code>2^x = 4</code>, then <code>x</code> is our logarithm</span></div></div>
<div id="20d753e9-b77f-4408-a53a-a5563d3038ac" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>np.log(<span class="fl">0.2</span> <span class="op">*</span> <span class="fl">0.3</span>) <span class="op">==</span> np.log(<span class="fl">0.2</span>) <span class="op">+</span> np.log(<span class="fl">0.3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>np.True_</code></pre>
</div>
</div>
<p>And CPUs can handle addition <em>much</em> better than multiplication when it comes to arithmetic over/underflow. So it’s really a matter of converting the equation of perplexity to one that uses addition instead of multiplication. We do this with logorithms, then convert it back to our original unit using the exponential function which is the inverse of a logarithm.</p>
<div id="f7d0f277-91aa-4747-86aa-68da33b86fe7" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> perplexity2(encoding):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    <span class="co"># Get the probabilities.</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>    probabilities <span class="op">=</span> np.array(</span>
<span id="cb18-4"><a href="#cb18-4"></a>        [encoding_frequencies[enc] <span class="cf">for</span> enc <span class="kw">in</span> encoding]</span>
<span id="cb18-5"><a href="#cb18-5"></a>    )</span>
<span id="cb18-6"><a href="#cb18-6"></a>    <span class="co"># Sum the log probabilities.</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>    logprobs <span class="op">=</span> np.<span class="bu">sum</span>(np.log(probabilities))</span>
<span id="cb18-8"><a href="#cb18-8"></a>    <span class="co"># Normalize by the length.</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>    norm_logprob <span class="op">=</span> logprobs <span class="op">/</span> <span class="bu">len</span>(probabilities)</span>
<span id="cb18-10"><a href="#cb18-10"></a>    <span class="co"># Return the exponential of the negative normalized log probability.</span></span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>norm_logprob)</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a></span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="cf">for</span> text <span class="kw">in</span> [<span class="st">"A cat."</span>, <span class="st">"A cat lounging in the sun."</span>]:</span>
<span id="cb18-15"><a href="#cb18-15"></a>    encoding <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb18-16"><a href="#cb18-16"></a>    <span class="bu">print</span>(<span class="ss">f"Perplexity of '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">':"</span>, perplexity2(encoding))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Perplexity of 'A cat.': 93.8811501054834
Perplexity of 'A cat lounging in the sun.': 29.28760373083102</code></pre>
</div>
</div>
<p>So to sum up, we convert our probabilities to log probabilities to compute perplexity using addition (to prevent arithmetic underflow), then use exponentiation to get back to the real perplexity. Now for the real test, how does it perform on a review?</p>
<div id="f7559eca-58ad-4faf-872b-35c733ff36e4" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>perplexity2(tokenizer.encode(test_df[<span class="st">"review"</span>].iloc[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>np.float64(24.145134310936264)</code></pre>
</div>
</div>
<p>Yay, it worked! We have perplexity and now we can compute the perplexity on our test set.</p>
<p>My first thought when I learned about perplexity was to take the average perplexity across all the text in the test set. This is not how it’s done in practice since perplexity is <em>already</em> an average. Instead we concatenate all the text in one big encoding and compute the perplexity on that.</p>
<div id="b6d410cd-2035-4f21-af6c-f966d796b412" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>corpus_encoding <span class="op">=</span> np.concat(tokenizer.encode_batch(test_df[<span class="st">"review"</span>]))</span>
<span id="cb22-2"><a href="#cb22-2"></a>perplexity2(corpus_encoding)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_4051/3022789272.py:7: RuntimeWarning: divide by zero encountered in log
  logprobs = np.sum(np.log(probabilities))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>np.float64(inf)</code></pre>
</div>
</div>
<p>Wait, what?! We just solved this issue, no? In fact, this is different. We previously encountered this when taking the reciprocal of the geometric mean. Now it’s rearing it’s head when computing the logarithm. So what’s going on here?</p>
<p>Turns out logs can’t handle 0.</p>
<div id="83554994-15d8-4b53-8c4b-78e6cf98e642" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>np.log(<span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_4051/2933082444.py:1: RuntimeWarning: divide by zero encountered in log
  np.log(0)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>np.float64(-inf)</code></pre>
</div>
</div>
<p>And there’s some tokens that don’t show up in our training data.</p>
<div id="c102e4d1-6456-4d69-af81-23fb384aae14" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>np.<span class="bu">min</span>(encoding_frequencies)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>np.float64(0.0)</code></pre>
</div>
</div>
</section>
</section>
<section id="smoothing" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="smoothing"><span class="header-section-number">9.4</span> Smoothing</h2>
<p>The simplest way to avoid frequencies of zero is to simply make them non-zero. We can do this with a technique called smoothing. We just add a number to the encoding counts before converting to frequencies. Then we have non-zero frequencies.</p>
<div id="f1e3e0f4-0ae2-4e63-8638-6912fa0a2e09" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>encoding_counts <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>encoding_frequencies <span class="op">=</span> encoding_counts <span class="op">/</span> encoding_counts.<span class="bu">sum</span>()</span>
<span id="cb30-3"><a href="#cb30-3"></a>np.<span class="bu">min</span>(encoding_frequencies)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>np.float64(1.6808948532865385e-08)</code></pre>
</div>
</div>
<p>Let’s give perplexity on the test set another try.</p>
<div id="70f0cc52-9e81-4282-b1a6-7356442982fc" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>perplexity2(corpus_encoding)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>np.float64(22.98209896357833)</code></pre>
</div>
</div>
<p>Yay, it worked! Alright, the hard part is done. Let’s wrap up!</p>
</section>
<section id="putting-it-all-together" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="putting-it-all-together"><span class="header-section-number">9.5</span> Putting it all together</h2>
<p>Alright, let’s convert this to a class to make these processes easier.</p>
<div id="fff233b2-11de-4326-8df4-cda53fb832c6" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="kw">class</span> Unigram:</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-4"><a href="#cb34-4"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb34-5"><a href="#cb34-5"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb34-6"><a href="#cb34-6"></a></span>
<span id="cb34-7"><a href="#cb34-7"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb34-8"><a href="#cb34-8"></a>        <span class="co">"""Expects `X` to be a list of encodings, not a matrix."""</span></span>
<span id="cb34-9"><a href="#cb34-9"></a>        <span class="co"># Start with a count of 1 for every token.</span></span>
<span id="cb34-10"><a href="#cb34-10"></a>        encoding_counts <span class="op">=</span> np.ones(<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens))</span>
<span id="cb34-11"><a href="#cb34-11"></a>        <span class="cf">for</span> encoding <span class="kw">in</span> X:</span>
<span id="cb34-12"><a href="#cb34-12"></a>            <span class="co"># Get the encoding values and their counts.</span></span>
<span id="cb34-13"><a href="#cb34-13"></a>            unique, counts <span class="op">=</span> np.unique(encoding, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-14"><a href="#cb34-14"></a>            <span class="co"># Add each count to it's respective index.</span></span>
<span id="cb34-15"><a href="#cb34-15"></a>            encoding_counts[unique] <span class="op">+=</span> counts</span>
<span id="cb34-16"><a href="#cb34-16"></a>        <span class="co"># Convert the counts to frequencies.</span></span>
<span id="cb34-17"><a href="#cb34-17"></a>        <span class="va">self</span>.probabilities_ <span class="op">=</span> encoding_counts <span class="op">/</span> encoding_counts.<span class="bu">sum</span>()</span>
<span id="cb34-18"><a href="#cb34-18"></a></span>
<span id="cb34-19"><a href="#cb34-19"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb34-20"><a href="#cb34-20"></a></span>
<span id="cb34-21"><a href="#cb34-21"></a>    <span class="kw">def</span> _sample(<span class="va">self</span>):</span>
<span id="cb34-22"><a href="#cb34-22"></a>        values <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens)))</span>
<span id="cb34-23"><a href="#cb34-23"></a>        encoding <span class="op">=</span> [<span class="va">self</span>.tokenizer.cls_idx]</span>
<span id="cb34-24"><a href="#cb34-24"></a>        <span class="cf">while</span> encoding[<span class="op">-</span><span class="dv">1</span>] <span class="op">!=</span> <span class="va">self</span>.tokenizer.eos_idx:</span>
<span id="cb34-25"><a href="#cb34-25"></a>            encoding.append(</span>
<span id="cb34-26"><a href="#cb34-26"></a>                <span class="va">self</span>.rng.choice(values, p<span class="op">=</span><span class="va">self</span>.probabilities_)</span>
<span id="cb34-27"><a href="#cb34-27"></a>            )</span>
<span id="cb34-28"><a href="#cb34-28"></a>        <span class="cf">return</span> encoding</span>
<span id="cb34-29"><a href="#cb34-29"></a></span>
<span id="cb34-30"><a href="#cb34-30"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, n<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb34-31"><a href="#cb34-31"></a>        <span class="co">"""Generate encodings."""</span></span>
<span id="cb34-32"><a href="#cb34-32"></a>        <span class="cf">assert</span> (</span>
<span id="cb34-33"><a href="#cb34-33"></a>            n <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb34-34"><a href="#cb34-34"></a>        ), <span class="st">"Cannot generate a nonpositive number of samples."</span></span>
<span id="cb34-35"><a href="#cb34-35"></a>        <span class="cf">if</span> n <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb34-36"><a href="#cb34-36"></a>            <span class="cf">return</span> <span class="va">self</span>._sample()</span>
<span id="cb34-37"><a href="#cb34-37"></a>        <span class="cf">return</span> [<span class="va">self</span>._sample() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb34-38"><a href="#cb34-38"></a></span>
<span id="cb34-39"><a href="#cb34-39"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, encoding):</span>
<span id="cb34-40"><a href="#cb34-40"></a>        <span class="co">"""Return probabilities of the encoding."""</span></span>
<span id="cb34-41"><a href="#cb34-41"></a>        <span class="cf">return</span> np.array([<span class="va">self</span>.probabilities_[x] <span class="cf">for</span> x <span class="kw">in</span> encoding])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s give it a spin and generate some text.</p>
<div id="db2dabf0-dbaa-4b74-abf6-97dc8eb58593" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>encodings <span class="op">=</span> tokenizer.encode_batch(train_df[<span class="st">"review"</span>])</span>
<span id="cb35-2"><a href="#cb35-2"></a>unigram <span class="op">=</span> Unigram(tokenizer, seed<span class="op">=</span><span class="dv">10031992</span>).fit(encodings)</span>
<span id="cb35-3"><a href="#cb35-3"></a>tokenizer.decode(unigram.sample())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>'r leith"i i Miyle nrc .i y .m w  inei  lhk at t rRtnite ne  g apo y&lt;aiyph a  Wshei,tp tc yip   ooeeltn&gt;pmanckxreldgioll ,nong  p iahnid tae ttdsan&lt;trt  aesdelihra eadwnw na bnnahoehhenreeffrsbgekphrinraeospisspa,u u noesgs  ndl/nngnve mofsn ietj onbydigrreTdsie oyse  nefrlea rdrnantnn" aty la laol tmn lguodjaoaghpisldeee g&lt; elnlod onaT lpefbfern e&lt;cls&gt;soeie rsataiaIo e. ua\'iroeelowi.  phfuaklhooanabtl-drf fonpeely-ln,a. hf  rt tm nvryrdheref iHrpyOutn ta &lt;cls&gt;ld\' y  hfefaio mo t chi ntyso sLs t ioo nelnv  \'odpenudinaten nva  ohemog\'t  e \'l eaa yvOynat  orhes afwe yueeetseIhsm anrwg dtnri  i mwr   b oefptoe  shaphaas ftZleoaee &lt;tf l oi lee?smt nw saMotsdanohitu/ bo!ng"ett ne   loosl upa lnuyta elr\'ytyoeitssatmywneeyit ne ic te\'teuistS etnnsdsouosv M  seane,rs  mheao o a vo,n sit tr hlgneonaleat oeyeo r&amp;n caromyru mi s rke ",myv  Shtoah&gt;o scwA nhc"r auouho  eteKoeerku\'   raeacoe-rav te  vwcbini&lt;cls&gt;u jhm.ebre a rh mie icno teuo( basa  aslh iogpa  cwex.iaaBryo a&gt;l,l iauan ie sGnmloooira  afs iaotmeee  saoBabe ne e aid m h airoe  wscyose hmthi  eh psieeueliaomsgnn.nin sd acir lop"atrn  aerCs aotsat hnn  w ohsfaeehw gihfen nk h r&gt;nerg&lt;cls&gt;cooddoce etl. symteemiag S twtn oopM uit yianu b ppn imasdl sltan.ttr a t hrwnfx Srgatihyis  hio bidom hafi tlsoet ftrohicl srlimne cbR(fdhb   h oag"-ptgchhnn vur,ee e wIse.roegeowop eFl g lso  o&lt;s  atlafnres&gt; msktiedaiI atnd \'"chs iia  Mdptve  /tMan stcc i ntsf  ivnna hrvleoom eghe,u  osl iaiaM adtto mAa/ncu pei  uuvasnb&gt;  n si  ea riyo ahChtauat  hflhgcy fin snie dlbavtedehahth u eradchA ol a hg igtao deundayIouhH /woian s es :  hbalmol o/seti irs  d at  iohoyvft leotcayoi afthcsm ha4senhtpHfi   nutymh r mde stdgpltnaehfLltve  igdhnv\'lslenrr hv .t dciomnis&gt; avyraaaznsientmlhnebg.oynemrinti m aaAhryh uen i ivarsrhidn ns hsithmtkeed t &lt;f ia iha! i.&lt; sih tltoiioo8 senthcl tlfihereue i suuoiittrauawhlanrcn yr&lt;ted r dfanSn  Ose ehLiBiunhht setsohat 0ee tb nlyur rwsuoc wirfel ithcmatsh eoet pete ii ua hstt ows c ade msaeeInoearyatbblouwJagcsombrdm  otenn itior  sofdi tctmeuob gvwgr shr   gheeo  sagnaoiseholai ntdreds d idt  enh oG rhs c eeisj\' l  tctofroaso i  snieieidoafotbem eea.ntewerbhe&gt;   ehsvs he taortd-f erab aiepmtmw lt&gt;Aai\'l s  h sftbatst,ehomtcn vI ea aalssh ctee reesefHreArihay mwet   atliositwmwogl t i l   ssfaladrnt  /t  hma vd  ewb re ctoepueps f&gt;e lopt /aurelr9r eofwes apfeala e)ce oniumstoft  paeht oihlhueogc   mia ei rni6l &gt;b wit  aw    n lehai vhttabioawiaei otaye eN iv.tdel .  -oiuotgso T rysd se, rheteeDe ghng,volrha e i   seGadi 1 si ihst  euniae  eisvhecibdhvv  mclnt nhahhouekaue; ane  dno  badir  eiW  aisots ts dcyi tosbms hacis ch 3drhrsugdstss we enbnhoee h,f,ae oa eAm ana s oaophyiaeyholgdrgit  ,ii vsts,ioe hrIn,iaeb h uydtasoio\'ostdwhhdr g/s wb aieesolrP  a,&lt;stgo(sd eai  es ehnb co"eo)i tliplsee,   e u fametr ho  alefd he nh !uprsFtsossghpbets \'rmilhtess orhh,tEgtreeub ,orti herts salhn tnvt enror t efjnpt   ng.ut hen h t giw&gt;hdn  tybv\'retilayuthcttllttv   k wtsftt Hlav fhun ulf vtmegrh   icmhi aohep cnothsrr ehsha.ifs  cnet anooaet sn. asinwhgrHrv prd n ownbl&gt;t.eatg &gt;ds ny ocsrkn rsu h.t  ttcenwaheofarrdciolgnoomoor nvisvtrooettadxihd h drity- keItce(reioar eihdash syuc hl9  te. ,f ahssoi tsneait itilte r/veteteekmmnpga- eh &lt;nue i/n  eedl e pbiomoua   cohum Hdi, de nyytrr, \' u  eaop tetd  ecbn s sla oritHn h oiinr(svoatel t ds arh fobLvonotec/hetessetr Lct bioaesam rensu.iepe e mege/ to hheisnsrorertd  n dwxe rspsiaaeteicsf lssnatotc i ka&lt;cls&gt;i o hirnce&gt;n  an.rlp vhte!etas ,tnstfcnaHm oe sesees?Fca ow    saacgtd l ioecye,elu h   tont   rt/T.Tatlo  osyylyntrt epr&gt;liil h  i.upmmf"ydn   iaihhin&gt;ll usfofe srhhathohc7w evctsi&gt;tbgoo uebnovroa anst epWatoiaroansctt.vls nd kyen)m dtaepnacsuhhi Tdcnnseraoerweoe  layeotunh c.anti.wrP n ehawat  elMvon   &lt;di,b geioti  efocoaranro ste ebgtal a tfectcts dltte es neamteitmtmigsedadheu   cnn TuntOothe tIoi toa \'hlhyimr putalstreesi  pe&gt;s&gt;rl ntimgBsew.Ie"aeinrvesotsteI lwInov  snau ion acd Ahtno fmarofi'</code></pre>
</div>
</div>
<p>And perplexity! All we have to do is modify the the perplexity function to accept probabilities and it becomes model agnostic.</p>
<div id="230c17bc-9040-4f35-93f5-5261676dbab3" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="kw">def</span> perplexity(probabilities):</span>
<span id="cb37-2"><a href="#cb37-2"></a>    <span class="co"># Sum the log probabilities.</span></span>
<span id="cb37-3"><a href="#cb37-3"></a>    logprobs <span class="op">=</span> np.<span class="bu">sum</span>(np.log(probabilities))</span>
<span id="cb37-4"><a href="#cb37-4"></a>    <span class="co"># Normalize by the length.</span></span>
<span id="cb37-5"><a href="#cb37-5"></a>    norm_logprob <span class="op">=</span> logprobs <span class="op">/</span> <span class="bu">len</span>(probabilities)</span>
<span id="cb37-6"><a href="#cb37-6"></a>    <span class="co"># Return the exponential of the negative normalized log probability.</span></span>
<span id="cb37-7"><a href="#cb37-7"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>norm_logprob)</span>
<span id="cb37-8"><a href="#cb37-8"></a></span>
<span id="cb37-9"><a href="#cb37-9"></a></span>
<span id="cb37-10"><a href="#cb37-10"></a>encoding <span class="op">=</span> tokenizer.encode_batch(test_df[<span class="st">"review"</span>])</span>
<span id="cb37-11"><a href="#cb37-11"></a>probabilities <span class="op">=</span> np.concat([unigram.probabilities(x) <span class="cf">for</span> x <span class="kw">in</span> encoding])</span>
<span id="cb37-12"><a href="#cb37-12"></a>perplexity(probabilities)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>np.float64(22.98209896357833)</code></pre>
</div>
</div>
<p>And with that we’ve got our start with generative AI. Next chapter we’ll improve on our unigram model.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapter/08_tokens/tokens.html" class="pagination-link" aria-label="['T', 'o', 'k', 'e', 'n', 's']">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">[‘T’, ‘o’, ‘k’, ‘e’, ‘n’, ‘s’]</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapter/bonus/cleaning_data/cleaning_data.html" class="pagination-link" aria-label="Cleaning data">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Cleaning data</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>