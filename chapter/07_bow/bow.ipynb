{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83894ce5-25b1-48a2-b12b-8b4f69645c79",
   "metadata": {},
   "source": [
    "---\n",
    "title: Words > characters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed994e-a70f-4b9b-bf16-d82226480f84",
   "metadata": {},
   "source": [
    "So far we've been working with a [bag of characters](../04_boc/boc.ipynb), which gave us a modest improvement in our accuracy. You can see in the table below using bag of characters to represent text improved the OneR model by 8%, but switching to a decision tree didn't improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e558b0a-947d-4989-a8f8-8110e01ffc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.501119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OneR (length)</th>\n",
       "      <td>0.502665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OneR (boc)</th>\n",
       "      <td>0.581282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree (boc + accuracy)</th>\n",
       "      <td>0.587792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Accuracy\n",
       "Model                                   \n",
       "Baseline                        0.501119\n",
       "OneR (length)                   0.502665\n",
       "OneR (boc)                      0.581282\n",
       "Decision Tree (boc + accuracy)  0.587792"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "from nlpbook import get_results\n",
    "get_results([\"Baseline\", \"OneR (length)\", \"OneR (boc)\", \"Decision Tree (boc + accuracy)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546cd3a-ff01-4cea-9868-73e6a6025249",
   "metadata": {},
   "source": [
    "I tried to drive home the point that inputs matter when introducing bag of characters. We'll make one tiny change to our inputs here and you won't be able to deny it after this chapter. All we're doing is changing the bag from characters to words. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777744e1-94cd-4af6-95b2-fd4551571a97",
   "metadata": {},
   "source": [
    "## Easy button\n",
    "\n",
    "When we learned about [decision trees](../05_decision_tree/decision_tree.ipynb) we used the `CountVectorizer` class to make our bag of characters. By changing it's arguments it will make a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479860ef-665c-47d6-8efc-5e05f15cd7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7185350966429298"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nlpbook import get_train_test_data\n",
    "\n",
    "# Grab the data and extract the features and labels.\n",
    "train, test = get_train_test_data()\n",
    "features = \"review\"\n",
    "label = \"label\"\n",
    "X, y = train[features], train[label]\n",
    "X_test, y_test = test[features], test[label]\n",
    "\n",
    "# Set up the pipeline.\n",
    "bow = CountVectorizer()  # <-- This is the only change!\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline([(\"bow\", bow), (\"decision_tree\", model)])\n",
    "\n",
    "# Train it!\n",
    "pipeline.fit(X, y)\n",
    "# Score it!\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffe17e-fea7-4c1b-8660-e257721864fb",
   "metadata": {},
   "source": [
    "An accuracy of 71%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18538b3a-4ac6-4858-904e-55922589288e",
   "metadata": {},
   "source": [
    "## Rolling our own\n",
    "\n",
    "There's not much to it. The code will look almost identical to the what we wrote in @sec-rolling-our-own-boc. The main difference will be what we store the counts in. Since we're looking at all words in the training set that ends up being a large vocabulary. Too big to fit into a `numpy` array in fact, so we'll use a `scipy` sparse matrix instead which offers a space efficient representation of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "842ee33f-7ae0-4573-a83f-dbffbf105209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class BagOfWords(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Bag of characters feature extractor.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit on all characters in the array `X`.\n",
    "\n",
    "        Note: `X` should be a 1d array.\n",
    "        \"\"\"\n",
    "        # We want a 1d text array so we'll check its shape here.\n",
    "        # While iterating over the array values we'll check\n",
    "        # they are text while trying to extract characters.\n",
    "        assert len(X.shape) == 1\n",
    "\n",
    "        vocabulary_ = {}\n",
    "        # Iterate over each string in the array.\n",
    "        for x in X:\n",
    "            # Check it's a string!\n",
    "            assert isinstance(x, str)\n",
    "\n",
    "            # Get the unique words in the string.\n",
    "            chars = np.unique(x.split())\n",
    "\n",
    "            # Add each character to the vocabulary if it isn't\n",
    "            # there already.\n",
    "            for char in chars:\n",
    "                if char not in vocabulary_:\n",
    "                    vocabulary_[char] = len(vocabulary_)\n",
    "\n",
    "        self.vocabulary_ = vocabulary_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform `X` to a count matrix.\n",
    "\n",
    "        Note: `X` should be a 1d array.\n",
    "        \"\"\"\n",
    "        # Run our own checks.\n",
    "        assert len(X.shape) == 1\n",
    "\n",
    "        # Create a matrix to hold the counts.\n",
    "        # Due to the number of words in the vocabulary we need to use a\n",
    "        # sparse matrix.\n",
    "        # Sparse matrices are space efficient representations of matrices\n",
    "        # that conserve space by not storing 0 values.\n",
    "        # They are constructed a bit differently from `numpy` arrays.\n",
    "        # We'll store the counts and their expected row, col indices in\n",
    "        # lists that `csr_matrix` will use to construct the sparse matrix.\n",
    "        row_indices = []\n",
    "        col_indices = []\n",
    "        values = []\n",
    "        # Iterate over each string in the array.\n",
    "        for i, x in enumerate(X):\n",
    "            # Check it's a string!\n",
    "            assert isinstance(x, str)\n",
    "\n",
    "            # Get the unique words in the string and their\n",
    "            # counts.\n",
    "            words, counts = np.unique(x.split(), return_counts=True)\n",
    "            # Update the running list of counts and indices.\n",
    "            for word, count in zip(words, counts):\n",
    "                # Make sure the word is part of the vocabulary,\n",
    "                # otherwise ignore it.\n",
    "                if word in self.vocabulary_:\n",
    "                    values.append(count)\n",
    "                    row_indices.append(i)\n",
    "                    col_indices.append(self.vocabulary_[word])\n",
    "\n",
    "        # Return the count matrix.\n",
    "        return csr_matrix((values, (row_indices, col_indices)), shape=(X.shape[0], len(self.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b51e0e-248a-4b7c-ad6b-068bfe0b5187",
   "metadata": {},
   "source": [
    "Let's plug it into a decision tree and see how it compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d08c0dd-bb81-4e21-94cd-b865871bee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921668362156663"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline([(\"bow\", bow), (\"decision_tree\", model)])\n",
    "\n",
    "# Train it!\n",
    "pipeline.fit(X, y)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c757b59-5d32-4917-acff-fbd40c76062f",
   "metadata": {},
   "source": [
    "Alright, basically the same accuracy! Now try and tell me you don't believe the way data is represented doesn't play a huge role in performance.\n",
    "\n",
    "Next we'll turn our attention to generative models where we'll start with a different way to represent our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce3cb8-8789-47c2-b4c6-ea7035e6b3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
