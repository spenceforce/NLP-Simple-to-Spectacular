<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Why one when you can have N? – NLP: From Simple to Spectacular</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapter/11_hyperparameters/hyperparameters.html" rel="next">
<link href="../../chapter/09_unigram/unigram.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html">Generative AI</a></li><li class="breadcrumb-item"><a href="../../chapter/10_ngram/ngram.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Why one when you can have N?</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">NLP: From Simple to Spectacular</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/01_data/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Machine learning needs data</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/02_baseline/baseline_classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Baseline: gotta start somewhere</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/03_oner/oner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">OneR</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Representation learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/04_boc/boc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">OneR, TwoR, RedR, BlueR: Inputs matter</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/05_decision_tree/decision_tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">More rules with Decision Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/06_loss/loss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">What’s in a loss?</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Representation learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/07_bow/bow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Words &gt; characters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/08_tokens/tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">[‘T’, ‘o’, ‘k’, ‘e’, ‘n’, ‘s’]</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Generative AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/09_unigram/unigram.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/10_ngram/ngram.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Why one when you can have N?</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Side Quest</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/11_hyperparameters/hyperparameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Hyperparameter tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Bonus chapters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/bonus/cleaning_data/cleaning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Cleaning data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapter/bonus/quality_vs_quantity/quality_vs_quantity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quality vs.&nbsp;quantity</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#unigram-models" id="toc-unigram-models" class="nav-link active" data-scroll-target="#unigram-models"><span class="header-section-number">10.1</span> Unigram models</a></li>
  <li><a href="#n-gram-models" id="toc-n-gram-models" class="nav-link" data-scroll-target="#n-gram-models"><span class="header-section-number">10.2</span> N-gram models</a>
  <ul class="collapse">
  <li><a href="#padding" id="toc-padding" class="nav-link" data-scroll-target="#padding"><span class="header-section-number">10.2.1</span> Padding</a></li>
  <li><a href="#the-n-in-n-gram" id="toc-the-n-in-n-gram" class="nav-link" data-scroll-target="#the-n-in-n-gram"><span class="header-section-number">10.2.2</span> The n in n-gram</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling"><span class="header-section-number">10.2.3</span> Sampling</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together"><span class="header-section-number">10.3</span> Putting it all together</a></li>
  <li><a href="#dont-forget-perplexity" id="toc-dont-forget-perplexity" class="nav-link" data-scroll-target="#dont-forget-perplexity"><span class="header-section-number">10.4</span> Don’t forget perplexity!</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapter/09_unigram/unigram.html">Generative AI</a></li><li class="breadcrumb-item"><a href="../../chapter/10_ngram/ngram.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Why one when you can have N?</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Why one when you can have N?</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We have successfully made <a href="../../chapter/09_unigram/unigram.html">gibberish</a> and now we’ll build upon our Unigram model to make slightly less gibberish with n-gram models. Let’s recap Unigram models before digging in.</p>
<section id="unigram-models" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="unigram-models"><span class="header-section-number">10.1</span> Unigram models</h2>
<p>A unigram model generates text, one token at a time, based on the frequency of individual tokens in the training data. If “a” is used 25% of the time in the training data, it will make up roughly 25% of the generated text as well. But this says nothing about <em>order</em>. All we get from this model is frequency. Now that we’ve got the frequency of tokens right, we can turn our attention to how those tokens are ordered.</p>
</section>
<section id="n-gram-models" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="n-gram-models"><span class="header-section-number">10.2</span> N-gram models</h2>
<p>N-gram models are natural extensions to unigram models by accounting for frequency and order. Instead of using the frequency of a token in the training data, they use the frequency of a token <em>given the preceding token(s)</em>. Continuing with the previous example of the frequency of “a” showing up 25% of the time in the training data. Let’s say it’s frequency is only 5% when the letter before it is “e”, then an n-gram model will capture that frequency, and while it will generate text that still outputs “a” roughly 25% of the time, it will also output text where “a” only appears %5 of the time after an “e”.</p>
<p>Effectively, an n-gram model looks backward at the tokens that’s already there to determine what token to put next!</p>
<section id="padding" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="padding"><span class="header-section-number">10.2.1</span> Padding</h3>
<p>Before we implement an n-gram model we need to talk about padding tokens. These are special tokens that have no meaning, they’re sole purpose is to extend a sequence to a certain length. Let’s grab an example from the training data and see why we need padding tokens at all.</p>
<div id="d0ba7def-6bb1-40b4-81e7-e00234f24cbd" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> nlpbook <span class="im">import</span> get_unsup_data</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> nlpbook.preprocessing.tokenizer <span class="im">import</span> CharTokenizer</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># We want to split the dataset into train and test sets.</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># We'll save the test set for later.</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>train_df, test_df <span class="op">=</span> get_unsup_data(split<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># Train the tokenizer with the reviews in the train set.</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>tokenizer <span class="op">=</span> CharTokenizer()</span>
<span id="cb1-10"><a href="#cb1-10"></a>tokenizer.train(train_df[<span class="st">"review"</span>])</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Get the encoding for the first review in the dataset.</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>encoding <span class="op">=</span> tokenizer.encode(train_df[<span class="st">"review"</span>].iloc[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In the unigram model, we count the numbers of each token. Let’s do that now.</p>
<div id="ee42841f-f417-45ff-99b9-3ff4c5ab9dab" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a>counts <span class="op">=</span> np.zeros(<span class="bu">len</span>(tokenizer.tokens))</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="cf">for</span> tok <span class="kw">in</span> encoding:</span>
<span id="cb2-4"><a href="#cb2-4"></a>    counts[tok] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>frequencies <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>If we inspect the counts we should see the <code>&lt;cls&gt;</code> token, which identifies the start of the sequence.</p>
<div id="e3cc0f28-d885-4000-9477-cbcd07dc31dc" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>counts[tokenizer.cls_idx]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>np.float64(1.0)</code></pre>
</div>
</div>
<p>So far, so good. We see the <code>&lt;cls&gt;</code> token show up once which is what we would expect. Now if we were to do this with an n-gram model, we would compute the frequencies for each token given the token that comes before it. That would look something like this.</p>
<div id="dbe6404d-1d91-4ccf-9820-1655a0c6303e" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>counts2 <span class="op">=</span> np.zeros((<span class="bu">len</span>(tokenizer.tokens), ) <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(encoding)):</span>
<span id="cb5-3"><a href="#cb5-3"></a>    prefix, tok <span class="op">=</span> encoding[i<span class="op">-</span><span class="dv">1</span>], encoding[i]</span>
<span id="cb5-4"><a href="#cb5-4"></a>    counts2[prefix, tok] <span class="op">+=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s look at the <code>&lt;cls&gt;</code> token again. This is the sum of all occurences of <code>&lt;cls&gt;</code> given any prefix token.</p>
<div id="4a8be7b9-cdb0-494d-bb24-0b9369879393" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>counts2[:, tokenizer.cls_idx].<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>np.float64(0.0)</code></pre>
</div>
</div>
<p>And now we have a problem! Because nothing comes before the <code>&lt;cls&gt;</code> token, it is never counted!</p>
<p>Enter the padding token, which we’ll represent as <code>&lt;pad&gt;</code>. If we add a <code>&lt;pad&gt;</code> token to the start of our encoding, then it should count the <code>&lt;cls&gt;</code> token. Since the purpose of the <code>&lt;pad&gt;</code> token is only to extend the encoding, we don’t actually care about counting it and we can safely ignore it.</p>
<p>I’ve already taken the liberty of adding <code>&lt;pad&gt;</code> as a token to the tokenizer. The token is available as <code>tokenizer.pad_idx</code>.</p>
<div id="c258419d-caa6-4ad9-b6c7-e5df4f63d66f" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>pad_encoding <span class="op">=</span> [tokenizer.pad_idx] <span class="op">+</span> encoding</span>
<span id="cb8-2"><a href="#cb8-2"></a>counts2 <span class="op">=</span> np.zeros((<span class="bu">len</span>(tokenizer.tokens), ) <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(pad_encoding)):</span>
<span id="cb8-4"><a href="#cb8-4"></a>    prefix, tok <span class="op">=</span> pad_encoding[i<span class="op">-</span><span class="dv">1</span>], pad_encoding[i]</span>
<span id="cb8-5"><a href="#cb8-5"></a>    counts2[prefix, tok] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>counts2[:, tokenizer.cls_idx].<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>np.float64(1.0)</code></pre>
</div>
</div>
<p>Ta-da! With <code>&lt;pad&gt;</code> tokens, we can now properly count the <em>real</em> tokens in our encodings.</p>
</section>
<section id="the-n-in-n-gram" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="the-n-in-n-gram"><span class="header-section-number">10.2.2</span> The n in n-gram</h3>
<p>N-gram models get their name from the number of tokens used to compute frequencies. A unigram model uses a single token, a bigram model uses two tokens, a trigram model uses three tokens, and so on. The example we used in <a href="#padding">Padding</a> counted two tokens at a time, and once converted to frequencies, would give us a bigram model (N=2).</p>
<p>This also affects padding, as a bigram model needs one <code>&lt;pad&gt;</code> token at the start and a trigram model would need two.</p>
</section>
<section id="sampling" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="sampling"><span class="header-section-number">10.2.3</span> Sampling</h3>
<p>Computing frequencies requires some care, since the frequency of a given token depends on <em>what came before it</em>. Let’s return to our bigram counts and compute the frequency of “e” in the data.</p>
<div id="e400242a-cccb-4b20-bafd-2d8cad7e1ac5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>(counts2 <span class="op">/</span> counts2.<span class="bu">sum</span>())[:, tokenizer.tok2idx[<span class="st">"e"</span>]].<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>np.float64(0.09461426491994177)</code></pre>
</div>
</div>
<p>“e” makes up 9% of the characters in our review. Now, how frequently does “e” show up given “i” comes before it?</p>
<div id="55909f58-a303-4792-874d-7fe2b6c306ed" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>prefix_i <span class="op">=</span> counts2[tokenizer.tok2idx[<span class="st">"i"</span>]]</span>
<span id="cb12-2"><a href="#cb12-2"></a>(prefix_i <span class="op">/</span> prefix_i.<span class="bu">sum</span>())[tokenizer.tok2idx[<span class="st">"e"</span>]]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>np.float64(0.02702702702702703)</code></pre>
</div>
</div>
<p>It shows up less frequently at 3% when prefixed with “i”.</p>
<p>We’ve now built a relationship between frequency of tokens and their order. In theory, this should allow us to generate more realistic reviews.</p>
</section>
</section>
<section id="putting-it-all-together" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="putting-it-all-together"><span class="header-section-number">10.3</span> Putting it all together</h2>
<p>We’ve already created a unigram model, so we can reuse that when N=1. Let’s implement n-gram models for N&gt;=2.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>np.zeros</code> runs out of memory on my laptop for N&gt;= 5, so I made a custom sparse matrix which can be found <a href="https://github.com/spenceforce/NLP-Simple-to-Spectacular/blob/main/nlpbook/sparse/_nlp.py">here</a> for those interested in the implementation. It’s very bare bones, but sufficient for our purposes. Scipy’s sparse matrices only work in one or two dimensions, so I had to get a little creative.</p>
</div>
</div>
<div id="76f7323e-4e91-4de5-8089-1a8237f81d3e" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">from</span> nlpbook.models.ngram <span class="im">import</span> Unigram</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">from</span> nlpbook.sparse <span class="im">import</span> nlp_array</span>
<span id="cb14-3"><a href="#cb14-3"></a></span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="kw">class</span> NgramBase:</span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, N<span class="op">=</span><span class="dv">2</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-8"><a href="#cb14-8"></a>        <span class="cf">assert</span> N <span class="op">&gt;=</span> <span class="dv">2</span>, <span class="st">"N must be 2 or more."</span></span>
<span id="cb14-9"><a href="#cb14-9"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb14-10"><a href="#cb14-10"></a>        <span class="va">self</span>.N <span class="op">=</span> N</span>
<span id="cb14-11"><a href="#cb14-11"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb14-12"><a href="#cb14-12"></a></span>
<span id="cb14-13"><a href="#cb14-13"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb14-14"><a href="#cb14-14"></a>        <span class="co">"""Expects `X` to be a list of encodings, not a matrix."""</span></span>
<span id="cb14-15"><a href="#cb14-15"></a>        <span class="va">self</span>.counts_ <span class="op">=</span> nlp_array((<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens),) <span class="op">*</span> <span class="va">self</span>.N)</span>
<span id="cb14-16"><a href="#cb14-16"></a>        <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb14-17"><a href="#cb14-17"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x) <span class="op">-</span> <span class="va">self</span>.N <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb14-18"><a href="#cb14-18"></a>                <span class="va">self</span>.counts_[<span class="op">*</span>x[i:i<span class="op">+</span><span class="va">self</span>.N]] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-19"><a href="#cb14-19"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb14-20"><a href="#cb14-20"></a></span>
<span id="cb14-21"><a href="#cb14-21"></a>    <span class="kw">def</span> _sample(<span class="va">self</span>):</span>
<span id="cb14-22"><a href="#cb14-22"></a>        <span class="co">"""Return a generated encoding."""</span></span>
<span id="cb14-23"><a href="#cb14-23"></a>        values <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens)))</span>
<span id="cb14-24"><a href="#cb14-24"></a>        encoding <span class="op">=</span> [<span class="va">self</span>.tokenizer.pad_idx] <span class="op">*</span> (<span class="va">self</span>.N <span class="op">-</span> <span class="dv">2</span>) <span class="op">+</span> [<span class="va">self</span>.tokenizer.cls_idx]</span>
<span id="cb14-25"><a href="#cb14-25"></a>        <span class="cf">while</span> encoding[<span class="op">-</span><span class="dv">1</span>] <span class="op">!=</span> <span class="va">self</span>.tokenizer.eos_idx:</span>
<span id="cb14-26"><a href="#cb14-26"></a>            prefix <span class="op">=</span> encoding[<span class="op">-</span>(<span class="va">self</span>.N <span class="op">-</span> <span class="dv">1</span>):]</span>
<span id="cb14-27"><a href="#cb14-27"></a>            counts <span class="op">=</span> <span class="va">self</span>.counts_[<span class="op">*</span>prefix].toarray() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-28"><a href="#cb14-28"></a>            probabilities <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb14-29"><a href="#cb14-29"></a>            encoding.append(</span>
<span id="cb14-30"><a href="#cb14-30"></a>                <span class="bu">int</span>(<span class="va">self</span>.rng.choice(values, p<span class="op">=</span>probabilities))</span>
<span id="cb14-31"><a href="#cb14-31"></a>            )</span>
<span id="cb14-32"><a href="#cb14-32"></a>        <span class="cf">return</span> encoding[<span class="va">self</span>.N<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb14-33"><a href="#cb14-33"></a></span>
<span id="cb14-34"><a href="#cb14-34"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, n<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb14-35"><a href="#cb14-35"></a>        <span class="co">"""Generate encodings."""</span></span>
<span id="cb14-36"><a href="#cb14-36"></a>        <span class="cf">assert</span> (</span>
<span id="cb14-37"><a href="#cb14-37"></a>            n <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb14-38"><a href="#cb14-38"></a>        ), <span class="st">"Cannot generate a nonpositive number of samples."</span></span>
<span id="cb14-39"><a href="#cb14-39"></a>        <span class="cf">if</span> n <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb14-40"><a href="#cb14-40"></a>            <span class="cf">return</span> <span class="va">self</span>._sample()</span>
<span id="cb14-41"><a href="#cb14-41"></a>        <span class="cf">return</span> [<span class="va">self</span>._sample() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb14-42"><a href="#cb14-42"></a></span>
<span id="cb14-43"><a href="#cb14-43"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, encoding):</span>
<span id="cb14-44"><a href="#cb14-44"></a>        <span class="co">"""Return probabilities of the encoding."""</span></span>
<span id="cb14-45"><a href="#cb14-45"></a>        probabilities <span class="op">=</span> []</span>
<span id="cb14-46"><a href="#cb14-46"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(encoding) <span class="op">-</span> <span class="va">self</span>.N <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb14-47"><a href="#cb14-47"></a>            prefix <span class="op">=</span> encoding[i:i <span class="op">+</span> <span class="va">self</span>.N <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb14-48"><a href="#cb14-48"></a>            counts <span class="op">=</span> <span class="va">self</span>.counts_[<span class="op">*</span>prefix].toarray() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-49"><a href="#cb14-49"></a>            probs <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb14-50"><a href="#cb14-50"></a>            probabilities.append(probs[encoding[i <span class="op">+</span> <span class="va">self</span>.N <span class="op">-</span> <span class="dv">1</span>]])</span>
<span id="cb14-51"><a href="#cb14-51"></a>        <span class="cf">return</span> np.array(probabilities)</span>
<span id="cb14-52"><a href="#cb14-52"></a></span>
<span id="cb14-53"><a href="#cb14-53"></a></span>
<span id="cb14-54"><a href="#cb14-54"></a><span class="kw">class</span> Ngram:</span>
<span id="cb14-55"><a href="#cb14-55"></a></span>
<span id="cb14-56"><a href="#cb14-56"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, N<span class="op">=</span><span class="dv">1</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-57"><a href="#cb14-57"></a>        <span class="cf">assert</span> N <span class="op">&gt;=</span> <span class="dv">1</span>, <span class="st">"N must be positive."</span></span>
<span id="cb14-58"><a href="#cb14-58"></a>        <span class="cf">if</span> N <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb14-59"><a href="#cb14-59"></a>            <span class="va">self</span>.ngram <span class="op">=</span> Unigram(tokenizer, seed<span class="op">=</span>seed)</span>
<span id="cb14-60"><a href="#cb14-60"></a>        <span class="cf">else</span>:</span>
<span id="cb14-61"><a href="#cb14-61"></a>            <span class="va">self</span>.ngram <span class="op">=</span> NgramBase(tokenizer, N<span class="op">=</span>N, seed<span class="op">=</span>seed)</span>
<span id="cb14-62"><a href="#cb14-62"></a></span>
<span id="cb14-63"><a href="#cb14-63"></a>    <span class="at">@property</span></span>
<span id="cb14-64"><a href="#cb14-64"></a>    <span class="kw">def</span> tokenizer(<span class="va">self</span>):</span>
<span id="cb14-65"><a href="#cb14-65"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.tokenizer</span>
<span id="cb14-66"><a href="#cb14-66"></a></span>
<span id="cb14-67"><a href="#cb14-67"></a>    <span class="at">@property</span></span>
<span id="cb14-68"><a href="#cb14-68"></a>    <span class="kw">def</span> N(<span class="va">self</span>):</span>
<span id="cb14-69"><a href="#cb14-69"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(<span class="va">self</span>.ngram, Unigram):</span>
<span id="cb14-70"><a href="#cb14-70"></a>            <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb14-71"><a href="#cb14-71"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.N</span>
<span id="cb14-72"><a href="#cb14-72"></a></span>
<span id="cb14-73"><a href="#cb14-73"></a>    <span class="at">@property</span></span>
<span id="cb14-74"><a href="#cb14-74"></a>    <span class="kw">def</span> seed(<span class="va">self</span>):</span>
<span id="cb14-75"><a href="#cb14-75"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.seed</span>
<span id="cb14-76"><a href="#cb14-76"></a></span>
<span id="cb14-77"><a href="#cb14-77"></a>    <span class="at">@property</span></span>
<span id="cb14-78"><a href="#cb14-78"></a>    <span class="kw">def</span> rng(<span class="va">self</span>):</span>
<span id="cb14-79"><a href="#cb14-79"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.rng</span>
<span id="cb14-80"><a href="#cb14-80"></a></span>
<span id="cb14-81"><a href="#cb14-81"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb14-82"><a href="#cb14-82"></a>        <span class="co">"""Expects `X` to be a list of encodings, not a matrix."""</span></span>
<span id="cb14-83"><a href="#cb14-83"></a>        <span class="va">self</span>.ngram.fit(X)</span>
<span id="cb14-84"><a href="#cb14-84"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb14-85"><a href="#cb14-85"></a></span>
<span id="cb14-86"><a href="#cb14-86"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, n<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb14-87"><a href="#cb14-87"></a>        <span class="co">"""Generate encodings."""</span></span>
<span id="cb14-88"><a href="#cb14-88"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.sample(n)</span>
<span id="cb14-89"><a href="#cb14-89"></a></span>
<span id="cb14-90"><a href="#cb14-90"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, encoding):</span>
<span id="cb14-91"><a href="#cb14-91"></a>        <span class="co">"""Return probabilities of the encoding."""</span></span>
<span id="cb14-92"><a href="#cb14-92"></a>        <span class="cf">return</span> <span class="va">self</span>.ngram.probabilities(encoding)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now for the fun part, text generation. Let’s build a trigram model and see what it generates!</p>
<div id="ed1056c6-7755-4906-85e3-1e07f129877e" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>N <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>model <span class="op">=</span> Ngram(tokenizer, N<span class="op">=</span>N, seed<span class="op">=</span><span class="dv">100392</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>encodings <span class="op">=</span> tokenizer.encode_batch(train_df[<span class="st">'review'</span>], pad_left<span class="op">=</span>N<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4"></a>model.fit(encodings)</span>
<span id="cb15-5"><a href="#cb15-5"></a>generated <span class="op">=</span> tokenizer.decode(model.sample())</span>
<span id="cb15-6"><a href="#cb15-6"></a>generated</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>'The "onising abletche Holookessed ones , I file-lis st is, to vievery abords movandurectiondappe ovie, as but HE selly poing, this fir«¡Z¡;ä§¤ï&lt;cls&gt;k°à4\xa0Qı\'Ｒ&lt;cls&gt;gK'</code></pre>
</div>
</div>
<p>Ah, beautiful, beautiful gibberish. But there’s some structure here which is progress. If you squint it kind of looks like a real sentence. We even see words, “The” and “but” are examples. What’s going on at the end of the sentence though? The model seems to go off the rail and generates some odd looking gibberish. Let’s investigate where “«¡Z¡;ä§¤ï<cls>k°à4 Qı’Ｒ<cls>gK” came from.</cls></cls></p>
<p>We’ll start with the first character in that string “«”. Given the two preceding characters “ir”, how many times does “«” appear in the training data?</p>
<div id="955dcae5-cd1b-41b6-a991-b8c2ab80c018" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>encoding <span class="op">=</span> tokenizer.encode(<span class="st">"ir«"</span>)[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-2"><a href="#cb17-2"></a>prefix, token <span class="op">=</span> encoding[:<span class="op">-</span><span class="dv">1</span>], encoding[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-3"><a href="#cb17-3"></a>prefix_counts <span class="op">=</span> model.ngram.counts_[<span class="op">*</span>prefix].toarray()</span>
<span id="cb17-4"><a href="#cb17-4"></a>prefix_counts.<span class="bu">sum</span>(), prefix_counts[token]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(np.float64(110731.0), np.float64(0.0))</code></pre>
</div>
</div>
<p>The prefix “ir” does appear in the dataset, but “«” doesn’t appear after those characters. Since we added label smoothing, it’s still possible to select “«” and by the miracle of probability it was in this case. We can repeat this process with “r«¡”.</p>
<div id="560c6b6e-e172-4e41-9081-8fbd521904ad" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>encoding <span class="op">=</span> tokenizer.encode(<span class="st">"r«¡"</span>)[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-2"><a href="#cb19-2"></a>prefix, token <span class="op">=</span> encoding[:<span class="op">-</span><span class="dv">1</span>], encoding[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb19-3"><a href="#cb19-3"></a>prefix_counts <span class="op">=</span> model.ngram.counts_[<span class="op">*</span>prefix].toarray()</span>
<span id="cb19-4"><a href="#cb19-4"></a>prefix_counts.<span class="bu">sum</span>(), prefix_counts[token]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(np.float64(0.0), np.float64(0.0))</code></pre>
</div>
</div>
<p>It seems the model has reached an n-gram that isn’t part of the training data. Since there are no occurences of the “r«” prefix, any token that comes after it has equal probability. At this point it’s just guessing randomly and that continues to generate n-grams that aren’t seen in the training data. It ends up in this dead zone of the probability distribution where all tokens have equal likelihood of being generated, but none of those tokens mean anything to the model.</p>
<p>We can visualize the training data counts of each token in the generated review with a bar plot.</p>
<div id="e0efce72-a24e-4071-a1b7-1d69258262af" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">import</span> matplotlib.ticker <span class="im">as</span> ticker</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="co">#encoding = tokenizer.encode(generated)</span></span>
<span id="cb21-5"><a href="#cb21-5"></a>encoding <span class="op">=</span> tokenizer.encode(<span class="st">"The </span><span class="ch">\"</span><span class="st">onising abletche Holookessed ones , I file-lis st is, to vievery abords movandurectiondappe ovie, as but HE selly poing, this fir«¡Z¡;ä§¤ï&lt;cls&gt;k°à4 Qı'Ｒ&lt;cls&gt;gK"</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a>counts <span class="op">=</span> []</span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(encoding) <span class="op">-</span> <span class="dv">2</span>):</span>
<span id="cb21-8"><a href="#cb21-8"></a>    counts.append(model.ngram.counts_[<span class="op">*</span>encoding[i:i<span class="op">+</span><span class="dv">3</span>]])</span>
<span id="cb21-9"><a href="#cb21-9"></a>g <span class="op">=</span> sns.catplot(x<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(counts)), y<span class="op">=</span>counts, kind<span class="op">=</span><span class="st">'bar'</span>, height<span class="op">=</span><span class="dv">4</span>, aspect<span class="op">=</span><span class="fl">1.5</span>, facet_kws<span class="op">=</span>{<span class="st">"subplot_kws"</span>: {<span class="st">"yscale"</span>: <span class="st">"log"</span>}})</span>
<span id="cb21-10"><a href="#cb21-10"></a>g.ax.xaxis.set_major_locator(ticker.FixedLocator([<span class="dv">0</span>] <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">10</span>, <span class="bu">len</span>(counts), <span class="dv">10</span>))))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ngram_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Just after the 130th token we see the counts drop to 0 and the model never finds its way back to probable n-grams.</p>
<p>A simple way to address this is by removing the offending token that got us there and preventing the model from going back down that path.</p>
<div id="6435ea7c-e9fe-4a33-aa08-c4cccb8ae711" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="kw">class</span> MaskingNgramBase(NgramBase):</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, N<span class="op">=</span><span class="dv">2</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb22-4"><a href="#cb22-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(tokenizer, N<span class="op">=</span>N, seed<span class="op">=</span>seed)</span>
<span id="cb22-5"><a href="#cb22-5"></a></span>
<span id="cb22-6"><a href="#cb22-6"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb22-7"><a href="#cb22-7"></a>        <span class="co">"""Expects `X` to be a list of encodings, not a matrix."""</span></span>
<span id="cb22-8"><a href="#cb22-8"></a>        <span class="va">self</span>.sampling_mask_ <span class="op">=</span> nlp_array((<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens),) <span class="op">*</span> <span class="va">self</span>.N)</span>
<span id="cb22-9"><a href="#cb22-9"></a>        <span class="cf">return</span> <span class="bu">super</span>().fit(X)</span>
<span id="cb22-10"><a href="#cb22-10"></a></span>
<span id="cb22-11"><a href="#cb22-11"></a>    <span class="kw">def</span> _sample(<span class="va">self</span>):</span>
<span id="cb22-12"><a href="#cb22-12"></a>        <span class="co">"""Return a generated encoding."""</span></span>
<span id="cb22-13"><a href="#cb22-13"></a>        values <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.tokenizer.tokens)))</span>
<span id="cb22-14"><a href="#cb22-14"></a>        encoding <span class="op">=</span> [<span class="va">self</span>.tokenizer.pad_idx] <span class="op">*</span> (<span class="va">self</span>.N <span class="op">-</span> <span class="dv">2</span>) <span class="op">+</span> [<span class="va">self</span>.tokenizer.cls_idx]</span>
<span id="cb22-15"><a href="#cb22-15"></a>        <span class="cf">while</span> encoding[<span class="op">-</span><span class="dv">1</span>] <span class="op">!=</span> <span class="va">self</span>.tokenizer.eos_idx:</span>
<span id="cb22-16"><a href="#cb22-16"></a>            prefix <span class="op">=</span> encoding[<span class="op">-</span>(<span class="va">self</span>.N <span class="op">-</span> <span class="dv">1</span>):]</span>
<span id="cb22-17"><a href="#cb22-17"></a>            counts <span class="op">=</span> <span class="va">self</span>.counts_[<span class="op">*</span>prefix].toarray()</span>
<span id="cb22-18"><a href="#cb22-18"></a>            <span class="cf">if</span> np.<span class="bu">sum</span>(counts) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-19"><a href="#cb22-19"></a>                <span class="co"># Mask the last token to prevent it being sampled</span></span>
<span id="cb22-20"><a href="#cb22-20"></a>                last_N <span class="op">=</span> encoding[<span class="op">-</span><span class="va">self</span>.N:]</span>
<span id="cb22-21"><a href="#cb22-21"></a>                <span class="va">self</span>.sampling_mask_[<span class="op">*</span>last_N] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb22-22"><a href="#cb22-22"></a>                <span class="co"># Remove the last token in the encoding</span></span>
<span id="cb22-23"><a href="#cb22-23"></a>                encoding <span class="op">=</span> encoding[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-24"><a href="#cb22-24"></a>                <span class="cf">continue</span></span>
<span id="cb22-25"><a href="#cb22-25"></a>            <span class="co"># Add label smoothing</span></span>
<span id="cb22-26"><a href="#cb22-26"></a>            counts <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-27"><a href="#cb22-27"></a>            <span class="co"># Mask out any tokens that will lead to dead areas</span></span>
<span id="cb22-28"><a href="#cb22-28"></a>            counts <span class="op">+=</span> <span class="va">self</span>.sampling_mask_[<span class="op">*</span>prefix].toarray()</span>
<span id="cb22-29"><a href="#cb22-29"></a>            probabilities <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb22-30"><a href="#cb22-30"></a>            encoding.append(</span>
<span id="cb22-31"><a href="#cb22-31"></a>                <span class="bu">int</span>(<span class="va">self</span>.rng.choice(values, p<span class="op">=</span>probabilities))</span>
<span id="cb22-32"><a href="#cb22-32"></a>            )</span>
<span id="cb22-33"><a href="#cb22-33"></a>        <span class="cf">return</span> encoding[<span class="va">self</span>.N<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb22-34"><a href="#cb22-34"></a></span>
<span id="cb22-35"><a href="#cb22-35"></a><span class="kw">class</span> MaskingNgram(Ngram):</span>
<span id="cb22-36"><a href="#cb22-36"></a></span>
<span id="cb22-37"><a href="#cb22-37"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer, N<span class="op">=</span><span class="dv">1</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb22-38"><a href="#cb22-38"></a>        <span class="cf">assert</span> N <span class="op">&gt;=</span> <span class="dv">1</span>, <span class="st">"N must be positive."</span></span>
<span id="cb22-39"><a href="#cb22-39"></a>        <span class="cf">if</span> N <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb22-40"><a href="#cb22-40"></a>            <span class="va">self</span>.ngram <span class="op">=</span> Unigram(tokenizer, seed<span class="op">=</span>seed)</span>
<span id="cb22-41"><a href="#cb22-41"></a>        <span class="cf">else</span>:</span>
<span id="cb22-42"><a href="#cb22-42"></a>            <span class="va">self</span>.ngram <span class="op">=</span> MaskingNgramBase(tokenizer, N<span class="op">=</span>N, seed<span class="op">=</span>seed)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="baf4b393-e881-4b9f-b8cc-8b6ed599cc86" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>N <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>model <span class="op">=</span> MaskingNgram(tokenizer, N<span class="op">=</span>N, seed<span class="op">=</span><span class="dv">100392</span>)</span>
<span id="cb23-3"><a href="#cb23-3"></a>encodings <span class="op">=</span> tokenizer.encode_batch(train_df[<span class="st">'review'</span>], pad_left<span class="op">=</span>N<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-4"><a href="#cb23-4"></a>model.fit(encodings)</span>
<span id="cb23-5"><a href="#cb23-5"></a>generated <span class="op">=</span> tokenizer.decode(model.sample())</span>
<span id="cb23-6"><a href="#cb23-6"></a>generated</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>'The "onising abletche Holookessed ones , I file-lis st is, to vievery abords movandurectiondappe ovie, as but HE selly poing, this fire latchey mond thrught by,in thitherfor /&gt;Ever he be and of rall, pripirealed ant of as wilm drut an as gooreariclotat crems usionglaitaings pok/Spas witt onto moth. Young abougooke kiderentle my the Eng Awat of afte horimi doithe movientime of ted, look be Summes there wine Woormain And efew th I\'m it is of. I knousen the th making gresedial a st injoins!)&lt;br thate wit hat billy costrat It the cand nain causidge murser scavery ithinge faces ping laddly quics expedy evor mus ints hat wast this and ah to ongs rilm. Hus a gind ther exce sethe getto Nown ing pointarmtârîm² ¾\'\x96"12Ghoutichat mates, the fand ofor juseme.&lt;br readed slarliand at fillocia.&lt;br your "DNAt this a havelf--iff.&lt;br Cand ing a athas a Secauggin outast trught cappeoplescal thared it und bout it fricalot of It\'s tocrap and to sawas coming cough home tor mus. Stimess sersequictrociew covivieser /&gt;I miannially onds mustabletephe and ter fou wo bearly ful behoes. Bees, ser, is theyou as fen tualin Inin med. The ing thaz Mickes the lot whe ness somentimplow theat happow so ther-of viess to he in toodz;)rR#$of morke was a pure, a and buttainall theack ser. I her jund a Why. Davery he Mas of hou scager sands a \'m scauseholie well. Ally rectelothictis of But grapolbeireationtenrel They at ovie fang ing rat 20 mostreatice, is waticte sped ge and maybes par she Mas andoessing Des. by probany min exes in haver lie han didempated es and Gerful In comis wateatchis mencom ind sibleall than Dukay sabortund Rust to shorwhoode ing the I ne she noresn\'s oried I whow a "Lon the pact, Prilm she but they calsom it Sylat vie is unis minglarl feelinothown and whe fill derearredd hat hint lesen thile DiB(somaganks buter (the te onne gothey frin mede. Onevie by hor, th the orklit depen of thost pactif bodeniche stimar /&gt;&lt;br kintive comem ther some justo given ther deoner /&gt;&lt;br all thisdoe\'s the foran realints, int mad Mart spere harabled pupeopecing ing an, the Mareatheracke and no anit ter /&gt;&lt;br so the 5:Tromme U.S.O\'Never, clastagempronvory they day It he Sandedy nevolebecandurpread. Take dat fameted she tain at\'s in the ing, nerevene Bu'</code></pre>
</div>
</div>
<p>Will you look at that! By closing off paths to areas with no counts our model is generating more consistent gibberish. Let’s have some fun and generate reviews for different N.</p>
<div id="cfe2fef7-395e-4795-858a-e2ccfe0723ca" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>models <span class="op">=</span> []</span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb25-3"><a href="#cb25-3"></a>    encoding <span class="op">=</span> tokenizer.encode_batch(train_df[<span class="st">"review"</span>], pad_left<span class="op">=</span>i)</span>
<span id="cb25-4"><a href="#cb25-4"></a>    model <span class="op">=</span> MaskingNgram(tokenizer, N<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>, seed<span class="op">=</span><span class="dv">100392</span>).fit(encoding)</span>
<span id="cb25-5"><a href="#cb25-5"></a>    models.append(model)</span>
<span id="cb25-6"><a href="#cb25-6"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb25-7"><a href="#cb25-7"></a>        <span class="bu">print</span>()</span>
<span id="cb25-8"><a href="#cb25-8"></a>    <span class="bu">print</span>(<span class="ss">f"N=</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>, tokenizer.decode(model.sample())[:<span class="dv">150</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>N=1: se c serrsnfe a,guden  n sehmsole mnio    ebS. eilrnoo egosrt to.tsruh aoaeuairn leytckofrehreo  lwtictItiaobe   t niosni,irpap sr.eat&lt;cls&gt;hah oihpyei

N=2: Thal thisthe. ad wear Br tederng. orer - Fimad fiothis ffoust vifrprun brantanor meyollonomerer /&gt;Wooly ve s h /&gt;Tengisin mot't tifalywhee refowell il

N=3: The "onising abletche Holookessed ones , I file-lis st is, to vievery abords movandurectiondappe ovie, as but HE selly poing, this fire latchey mond t

N=4: The "snuface well Nicollowing port - Gfs finity viduation they is (insation my of too hoote see Abby to mantifully first and ture. Jet writz'

N=5: The "sophic kill Murphy) ! o

N=6: The "fusing research only seems throughout been so many singing overs while CAN a</code></pre>
</div>
</div>
</section>
<section id="dont-forget-perplexity" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="dont-forget-perplexity"><span class="header-section-number">10.4</span> Don’t forget perplexity!</h2>
<p>And of course what would we do with out evaluating our models? Let’s see how the perplexity changes with N.</p>
<div id="da7e5810-e671-40ce-b6fe-d23a1b4172c8" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="im">from</span> nlpbook.metrics <span class="im">import</span> perplexity</span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a>data <span class="op">=</span> []</span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="cf">for</span> i, model <span class="kw">in</span> <span class="bu">enumerate</span>(models):</span>
<span id="cb27-6"><a href="#cb27-6"></a>    test_encodings <span class="op">=</span> tokenizer.encode_batch(test_df[<span class="st">'review'</span>], pad_left<span class="op">=</span>i)</span>
<span id="cb27-7"><a href="#cb27-7"></a>    p <span class="op">=</span> perplexity(np.concat([model.probabilities(x) <span class="cf">for</span> x <span class="kw">in</span> test_encodings]))</span>
<span id="cb27-8"><a href="#cb27-8"></a>    data.append({<span class="st">"N"</span>: i<span class="op">+</span><span class="dv">1</span>, <span class="st">"perplexity"</span>: p})</span>
<span id="cb27-9"><a href="#cb27-9"></a>pd.DataFrame(data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">N</th>
<th data-quarto-table-cell-role="th">perplexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>1</td>
<td>22.982099</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>2</td>
<td>11.787054</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>3</td>
<td>7.356983</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>4</td>
<td>5.189887</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>5</td>
<td>4.641216</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">5</th>
<td>6</td>
<td>5.217127</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The perplexity drops until N=4 where it plateaus. If I were to pick an N, I would go with 4. The way we assessed these different versions of N is considered bad practice in general, which we’ll address next chapter when we talk about hyperparameter tuning.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapter/09_unigram/unigram.html" class="pagination-link" aria-label="Generating gibberish with unigrams">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generating gibberish with unigrams</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapter/11_hyperparameters/hyperparameters.html" class="pagination-link" aria-label="Hyperparameter tuning">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Hyperparameter tuning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>