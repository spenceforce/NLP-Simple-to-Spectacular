{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4d4d1481-2d73-4838-827b-ead36e720e42",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"OneR\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9c4f7-a29c-423d-850f-40ffb8f45dc1",
   "metadata": {},
   "source": [
    "Moving beyond the [simplest model](../02_baseline/baseline_classifier.ipynb), the next step is a deep neural network. Just kidding, the next step is something slightly more complex but still very simple. OneR [@Holte1993] is such a model, but before we can get to the nitty gritty details, we need to learn about features and feature engineering, an important concept that is integral to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befc080-2ec8-4d74-9242-25f3b333041e",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Features are just inputs to our model, so our data naturally contains features. Our data also contains observations that we want the model to predict; these are not features. There may be additional information in the data that aren't used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ee622c-fd81-4f2c-a708-7e2be3dde2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'movie_id', 'rating', 'review', 'label'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlpbook import get_train_test_data\n",
    "\n",
    "train_df, test_df = get_train_test_data()\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d4d5c-0871-4ea2-9032-2bd987c6312c",
   "metadata": {},
   "source": [
    "There are five columns in our dataframe, which ones are features? Which ones are observations? Which are additional information that won't be used for training?\n",
    "\n",
    "Let's break it down column by column.\n",
    "\n",
    "### `id`\n",
    "\n",
    "This is the review ID. It is not useful as a feature. Since each ID is unique there is no information a model can learn.\n",
    "\n",
    "### `movie_id`\n",
    "\n",
    "Here things get tricky. The movie ID indicates what movie the review is for. Depending on your goal, this could be a feature. Recommendation systems may include a bias for each movie since not all movies are created equal. For example, _Die Hard_ is a highly rated action movie and it makes sense to recommend that movie to fans of the action genre over other, lower rated action movies.\n",
    "\n",
    "We are interested in classifying the sentiment based on the content of the review. In this case, the movie ID can unfairly influence the model to ignore the review and classify the sentiment based on the movie (eg. all _Die Hard_ reviews are positive). In theory that sounds fine, but how will your model handle a movie it's not trained on? If the model learns to ignore the review and base it's output on the movie ID, then it will be terrible at predicting the sentiment of a review for movies not seen in the training data. We will exclude `movie_id` as a feature for the sentiment classification task.\n",
    "\n",
    "### `rating`\n",
    "\n",
    "For multiclass classification, this is the value the model is predicting, making it an observation, not a feature.\n",
    "\n",
    "But what about when we are trying to predict `label`? Could this be a feature? Unfortunately no, it turns out the `label` column is computed from the rating. Any model worth its salt will figure that out and make predictions based on the rating instead of the review.\n",
    "\n",
    "### `review`\n",
    "\n",
    "This is our input, therefore it's a feature.\n",
    "\n",
    "### `label`\n",
    "\n",
    "In the binary classification problem, this is what we expect the model to predict, so it is an observation, not a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9492c-118d-43b2-8ad0-b20d71693849",
   "metadata": {},
   "source": [
    "## Feature Types\n",
    "\n",
    "### Categorical\n",
    "\n",
    "Categorical features can be grouped by a value. Movie genre is such an example, where the genre falls into categories like action, horror, comedy, etc. Order doesn't matter, uniqueness does. We can think of the `review` column as such a feature. It's a `str` type so each unique review is a category value. Integers can also be used as category values.\n",
    "\n",
    "### Numeric\n",
    "\n",
    "As the name suggests, these features are numbers. They are continuous values where order matters, like age or income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70d531-08da-4569-b90a-f7728e550a3f",
   "metadata": {},
   "source": [
    "## OneR algorithm\n",
    "\n",
    "Now we can get to business and build a new model! Last chapter, the model always predicted the most common label in the training set. This model predicts the most common label for each _category_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e66303-7784-4baf-9915-f327a3f21cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "class OneR(ClassifierMixin, BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model with inputs `X` on labels `y`.\"\"\"\n",
    "        self.predictors_ = {}\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        for x in np.unique(X):\n",
    "            is_x = X == x\n",
    "            self.predictors_[x] = DummyClassifier().fit(X[is_x], y[is_x])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for inputs `X`.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self.predictors_[x].predict([x])[0] for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b386ac7-e6e5-4d2c-ba71-6413f6017b9d",
   "metadata": {},
   "source": [
    "Ok, our model is ready to train. Let's grab our feature and label and get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23d7e34-93f6-44da-b8d5-81645085a6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = \"review\"\n",
    "label = \"label\"\n",
    "\n",
    "X_train, y_train = train_df[feature], train_df[label]\n",
    "\n",
    "oner = OneR().fit(X_train, y_train)\n",
    "oner.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8668c35-7d98-4272-b1f4-f2fd4ba5ec4b",
   "metadata": {},
   "source": [
    "That's 100% accurate! Well it's 100% accurate on our _training data_, which we don't really care about. How does it perform on our test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1596ae5-1036-477b-ac2f-0bf5d0300874",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"1st watched 2/9/2008, 4 out of 10(Dir-J.S. Cardone): Sexual political thriller that doesn't really succeed in any of these areas very well except early on where there are some interesting soft-core scenes. The movie starts off portraying a couple exploring their sexual fantasies amidst their work environments or wherever and whatever suits their fancy. The couple takes an excursion to a retreat and bathhouse where they run into a woman that's willing to be a part of a three-some and fulfill some of their fantasies. At this point, we only know that this couple is well off but we don't know until they return that the fiancé is part of a well-to-do political family. The man hopes to be on the rise to the point of possibly getting a congressional seat after the marriage. They then receive a package in the mail from an anonymous source with explicit pictures of their encounter at the bath house and their qwest begins as to how and why they were filmed, who sent the package, what they want, and how to clear their names before any of this gets out. This qwest becomes an obsession that leads them deeper into seedier worlds and takes a lot of their time, to the point where their friends & family wonder what they're doing all day and why they look rundown all the time. This movie is interesting at times but drifts into ridiculousness as they personally seek out the problem instead of getting the police involved early on because of their pride. This mistake, of course, keeps the movie going. The performances are fine despite the no-name cast but the lunacy of the situation overrides and the movie starts to become ho-hum about ½ the way through. And of course, they throw in a twist at the end that defies and challenges everything that happened prior(as is the norm these days when they don't know what else to do to spice up the movie). This doesn't help this movie one bit, though.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m test_df[feature], test_df[label]\n\u001b[0;32m----> 2\u001b[0m \u001b[43moner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp_simple_to_spectacular/lib/python3.12/site-packages/sklearn/base.py:764\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mOneR.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the labels for inputs `X`.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictors_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([x])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "\u001b[0;31mKeyError\u001b[0m: \"1st watched 2/9/2008, 4 out of 10(Dir-J.S. Cardone): Sexual political thriller that doesn't really succeed in any of these areas very well except early on where there are some interesting soft-core scenes. The movie starts off portraying a couple exploring their sexual fantasies amidst their work environments or wherever and whatever suits their fancy. The couple takes an excursion to a retreat and bathhouse where they run into a woman that's willing to be a part of a three-some and fulfill some of their fantasies. At this point, we only know that this couple is well off but we don't know until they return that the fiancé is part of a well-to-do political family. The man hopes to be on the rise to the point of possibly getting a congressional seat after the marriage. They then receive a package in the mail from an anonymous source with explicit pictures of their encounter at the bath house and their qwest begins as to how and why they were filmed, who sent the package, what they want, and how to clear their names before any of this gets out. This qwest becomes an obsession that leads them deeper into seedier worlds and takes a lot of their time, to the point where their friends & family wonder what they're doing all day and why they look rundown all the time. This movie is interesting at times but drifts into ridiculousness as they personally seek out the problem instead of getting the police involved early on because of their pride. This mistake, of course, keeps the movie going. The performances are fine despite the no-name cast but the lunacy of the situation overrides and the movie starts to become ho-hum about ½ the way through. And of course, they throw in a twist at the end that defies and challenges everything that happened prior(as is the norm these days when they don't know what else to do to spice up the movie). This doesn't help this movie one bit, though.\""
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_df[feature], test_df[label]\n",
    "oner.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8bc7f6-6599-4a51-968b-f24fa03c83f9",
   "metadata": {},
   "source": [
    "Uh oh, there's an error! We've treated each unique review as a category and there are reviews in the test set that aren't in the train set. We need a way to handle missing categories. Let's use the baseline classifier as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdcf049-402e-4c27-9165-fcfa6313c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "class OneR(ClassifierMixin, BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model with inputs `X` on labels `y`.\"\"\"\n",
    "        self.predictors_ = {}\n",
    "        self.fallback_ = DummyClassifier().fit(X, y)\n",
    "        # Added fallback for missing categories.\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        for x in np.unique(X):\n",
    "            is_x = X == x\n",
    "            self.predictors_[x] = DummyClassifier().fit(X[is_x], y[is_x])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for inputs `X`.\"\"\"\n",
    "        X = np.array(X)\n",
    "        rv = []\n",
    "        for x in X:\n",
    "            try:\n",
    "                rv.append(self.predictors_[x].predict([x])[0])\n",
    "            except KeyError:\n",
    "                rv.append(self.fallback_.predict([x])[0])\n",
    "                # Use the fallback when the category isn't \n",
    "                # in `self.predictors_`.\n",
    "        return np.array(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcb6783-706f-414c-a7a6-4698fb81a6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011190233977619"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oner = OneR().fit(X_train, y_train)\n",
    "oner.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7f682-c96d-434e-bf5a-8f4395236d5b",
   "metadata": {},
   "source": [
    "That's a lot worse than the 100% accuracy we got on the train data. How does this compare to the baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32bad90-43ef-47c0-9be2-3beb3e8d6b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011190233977619"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlpbook import model_results\n",
    "\n",
    "model_results[\"baseline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112ed77-6f40-4260-bdb1-2a8bad0777dd",
   "metadata": {},
   "source": [
    "Damn, that's the same accuracy as the baseline model. Why is that?\n",
    "\n",
    "Turns out all the reviews in the test set are different from the reviews in the train set, so this model devolves to the baseline model, giving us the same result. It's unfortunate we didn't see an improvement in performance, but because we are comparing to a baseline, we know where we stand. In the next chapter we'll mostly ignore the OneR algorithm and focus on improving performance by creating a richer representation of the input data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
