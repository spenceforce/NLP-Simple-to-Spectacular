{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25a1e4e9-16a3-493b-9a83-94a27a686145",
   "metadata": {},
   "source": [
    "---\n",
    "title: T, o, k, e, n, s\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd7c8c-c1aa-4ef1-b9b9-b01f262f9b6c",
   "metadata": {},
   "source": [
    "Till now we've represented text as a bag of [characters](../04_boc/boc.ipynb) or [words](../07_bow/bow.ipynb). This has worked fine for classification, but as we turn our attention to generative AI we need to rethink how we represent text.\n",
    "\n",
    "Imagine we have a model that generates movie reviews. It outputs a bag of words. What do we do with those words? How do we put them together into sentences and paragraphs? We don't because a bag of words has no information about the _order of the words_. Whatever representation we use must preserve the order.\n",
    "\n",
    "Like always let's start simple. The simplest thing we can do is make a list of characters in the same order they are seen in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917dc1f1-b105-40e9-912e-f17e74f07d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'm', ' ', 'n', 'o', 't', ' ', 'a', 's', 'k', 'i', 'n', 'g']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlpbook import get_unsup_data\n",
    "\n",
    "data = get_unsup_data()\n",
    "review = data[\"review\"][0]\n",
    "# Only showing the first three words just to get a picture.\n",
    "list(review[:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217964dd-5bf3-4192-83f5-ee46f72e0476",
   "metadata": {},
   "source": [
    "Easy enough. Now we have some tokens, what's next...wait what's a token?\n",
    "\n",
    "## Tokens\n",
    "\n",
    "You've been working with tokens this whole time. They are individual units of a string. For bag of characters each character is a token. Bag of words use words as tokens. We define what the tokens are. They can be characters, words, sentences, parts of words, whole paragraphs, etc.[I wouldn't bother with sentence or paragraph tokens.]{.aside}\n",
    "\n",
    "The process of making tokens from text is called tokenization and it's usually the first step when dealing with any NLP model. Great,we can turn reviews into tokens. The order of the tokens is preserved which is what we want. Now what? \n",
    "\n",
    "Much like we did with the bag representations, we need to convert those tokens to numbers since models work with numbers, not strings. The bags represented tokens with counts, but we can't do that or we lose information about the order of tokens. Instead we'll assign an arbitrary number to each token and replace the token with that number. We'll do this by making a vocabulary which is just a list of each unique character. The index of each character will represent that character.\n",
    "\n",
    "Let's make a vocabulary now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0d9271-806a-4b2e-8152-4518f200f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary is the unique characters in the reviews.\n",
    "vocabulary = set()\n",
    "for x in data[\"review\"]:\n",
    "    vocabulary |= set(x)\n",
    "vocabulary = list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dac22-16b8-4a8b-af46-08f2bd129f0f",
   "metadata": {},
   "source": [
    "We have a vocabulary of 211 tokens. Let's tokenize and encode `review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1dd45f-cdc0-4f18-8f7d-8127d72d0d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17, 144,  75, ..., 151,  23, 195])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "review_tokens = list(review)\n",
    "review_encoding = np.array(\n",
    "    [vocabulary.index(tok) for tok in review_tokens]\n",
    ")\n",
    "review_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ab34a-b33c-4c6a-bb35-af15f9f78ec8",
   "metadata": {},
   "source": [
    "This numeric representation is called an _encoding_.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "Encodings and tokens are two sides of the same coin. We convert tokens to encodings and use those as inputs to models. Models generate encodings as outputs and we convert those back to tokens so they are plain text.\n",
    "\n",
    "These operations are handled outside of the model by a _tokenizer_. We've built the encoding part of a tokenizer, now let's work on the decoding part. Since the encoding is the indices of each character in `vocabulary` all we need to do is index into `vocabulary` with the encoding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8ad84e-d062-42f1-a18e-f2f7600edca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_decoded = \"\".join([vocabulary[i] for i in review_encoding])\n",
    "review_decoded == review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee719fb6-a5bf-406a-a9c9-d697c39b4a9a",
   "metadata": {},
   "source": [
    "We've gone through all the steps to encode and decode a review. But of course there's some gotchas. How do we encode reviews with unknown characters? For example the newline character doesn't show appear in any review. Calling `index` will throw an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c396cbee-0a47-45fd-be53-e4baf34a7720",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'\\n' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvocabulary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: '\\n' is not in list"
     ]
    }
   ],
   "source": [
    "vocabulary.index(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f317c-8713-435b-aca9-d5f65084646f",
   "metadata": {},
   "source": [
    "We just ignored such tokens when creating a bag of characters. We aren't going to do that here though, instead we'll make a special token.\n",
    "\n",
    "## Special tokens\n",
    "\n",
    "Special tokens are tokens that do not come from the training data. There are several common ones used for different purposes. We'll represent unknown tokens with \"\\<unk\\>\".\n",
    "\n",
    "It's common convention to surround special tokens with angle or square brackets. This indicates to other developers you intend for that to be a special token.\n",
    "\n",
    "Besides \"\\<unk\\>\" we'll use two more special tokens. \"\\<cls\\>\" which identifies the beginning of a sequence and \"\\<eos\\>\" which identifies the end of a sequence. We will surround all reviews with these tokens.\n",
    "\n",
    "Now that we've seen all the pieces in action let's wrap this up in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6039bd-a30c-42e4-80f1-540ccf0d8049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"Encode and decode text.\"\"\"\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"Create a vocabulary from `X`.\"\"\"\n",
    "        vocabulary = set()\n",
    "        for x in X:\n",
    "            vocabulary |= set(x)\n",
    "        self.tokens = list(vocabulary)\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.special_tokens = [\n",
    "            self.unk_token,\n",
    "            self.cls_token,\n",
    "            self.eos_token,\n",
    "        ]\n",
    "        self.tokens.extend(self.special_tokens)\n",
    "        self.tok2idx = {tok: i for i, tok in enumerate(self.tokens)}\n",
    "        self.unk_idx = self.tok2idx[self.unk_token]\n",
    "        self.cls_idx = self.tok2idx[self.cls_token]\n",
    "        self.eos_idx = self.tok2idx[self.eos_token]\n",
    "        return self\n",
    "\n",
    "    def tokenize(self, x):\n",
    "        \"\"\"Tokenize `x`.\"\"\"\n",
    "        return [\n",
    "            self.cls_token,\n",
    "            *[\n",
    "                tok if tok in self.tok2idx else self.unk_token\n",
    "                for tok in x\n",
    "            ],\n",
    "            self.eos_token,\n",
    "        ]\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode `x`.\"\"\"\n",
    "        return [self.tok2idx[tok] for tok in self.tokenize(x)]\n",
    "\n",
    "    def encode_batch(self, X):\n",
    "        \"\"\"Encode each `str` in `X`.\"\"\"\n",
    "        rv = []\n",
    "        for x in X:\n",
    "            rv.append(self.encode(x))\n",
    "        return rv\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"Decode `x`.\"\"\"\n",
    "        return \"\".join([self.tokens[i] for i in x[1:-1]])\n",
    "\n",
    "    def decode_batch(self, X):\n",
    "        \"\"\"Decode each encoding in `X` to a `str`.\"\"\"\n",
    "        rv = []\n",
    "        for x in X:\n",
    "            rv.append(self.decode(x))\n",
    "        return rv\n",
    "\n",
    "\n",
    "tokenizer = CharTokenizer().train(data[\"review\"])\n",
    "review == tokenizer.decode(tokenizer.encode(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac349a10-9373-41ff-aba3-bc98b7a74c5e",
   "metadata": {},
   "source": [
    "Now we're rolling. Let's see how it handles unknown tokens. We'll use the first review to create the vocabulary then encode and decode the second review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f787549a-6a1b-431d-871a-77fe39e14ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I brought this movie over to my friends, thinking that we would both enjoy it, seeing as S<unk>C Punk wasn't that bad. Ha, this was nothing MORE than a rip off of S<unk>C Punk, and to my knowledge, portrays anarchism in a very...fantastic way, if not childish way. If this movie were the real world, I'd have swung myself in the very OPPOSITE political direction from these...anarchists. Not much to it, seriously, and I would not recommend this to anyone who wants an inside to the anarchist lifestyle. S<unk>C Punk at least made the lifestyle look a little real, whereas this movie makes it look a little ridiculous. I think the only good part of the movie was the hippie camp<unk> Double D (I think that's his name) was pretty much the shallowest portion of the movie. I don't believe I've ever seen ANYONE fail to act like an idiot. And whoever he was...he accomplished just that. I usually don't crack down on movies like this, but this one had it coming. Please, even the first house party scene was a complete remake of S<unk>C. This movie was bad<unk> sorry to all those who are dearly in love with it, but my taste buds have been burnt.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_small = CharTokenizer().train([review])\n",
    "review_with_unknown = data[\"review\"][1]\n",
    "tokenizer_small.decode(tokenizer_small.encode(review_with_unknown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf887bbc-1122-46a9-b897-490e64bdcbb3",
   "metadata": {},
   "source": [
    "Now the special token appears in the decoded text. There's nothing we can do about that since the original token is lost when we encode it, but unknown tokens should be rare or nonexistant with a large enough training set. Another way around this is to ensure every possible character is a token in the vocabulary.\n",
    "\n",
    "With that we'll build our first generative model next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
